{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science / Machine Learning Meetup #1 Deep Learning Hands-on\n",
    "# オルタナティブ・データと自然言語処理\n",
    "\n",
    "## はじめに\n",
    "\n",
    "演習の概略は以下の通りです。\n",
    "1. [環境準備](#環境準備)\n",
    "1. [WEBスクレイピング](#WEBスクレイピング)\n",
    "1. [感情分析](#感情分析)\n",
    "    1. 前処理\n",
    "    1. ニューラル・ネットワーク構築\n",
    "    1. トレーニング\n",
    "    1. 予測\n",
    "\n",
    "以下の点にご注意ください。\n",
    "- 実行するコードの中に、ご利用中のユーザー名に合わせて、変更していただく部分があります。\n",
    "\n",
    "## 環境準備\n",
    "\n",
    "### パッケージのインストールとインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install ipython-sql==0.3.9\n",
    "!pip3 install PyHive==0.6.1\n",
    "!pip3 install SQLAlchemy==1.3.13\n",
    "!pip3 install thrift==0.13.0\n",
    "!pip3 install sasl==0.2.1\n",
    "!pip3 install thrift_sasl==0.3.0\n",
    "\n",
    "\n",
    "!pip3 install nltk==3.4.5\n",
    "!pip3 install torch==1.4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記でインストールしたPyHiveは、Pythonコードの中でimportして使われるのではなく、Hiveへの接続の際の接続文字列：`sqlalchemy.create_engine('hive://<host>:<port>')`の中でdialectsとして指定された際に必要になります。そのため、インストール後に利用するためには、新しくプロセスを始める必要があります。**インストールした後に一度、KernelをRestartしてください。**インストールしたプロセスでは、接続時に下記のようなエラーが発生します。\n",
    "`NoSuchModuleError: Can't load plugin: sqlalchemy.dialects:hive`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import subprocess\n",
    "import glob\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "\n",
    "from pyhive import hive\n",
    "import sqlalchemy\n",
    "\n",
    "import sys\n",
    "#from random import random\n",
    "from operator import add\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import torch\n",
    "import nltk\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WEBスクレイピング\n",
    "\n",
    "無償で利用できるAPIを用いて演習を行います。そのため、利用に一定の制限が課せられることにご留意ください。\n",
    "例えば、ご利用状況に応じて、下記のようなエラーメッセージを受け取ることがあります。\n",
    "\n",
    "```\n",
    "{\"response\":{\"status\":429},\"errors\":[{\"message\":\"Rate limit exceeded. Client may not make more than 200 requests an hour.\"}]}\n",
    "```\n",
    "まず、APIで取得したデータをCDSWプロジェクト内のファイルとして保存します。\n",
    "\n",
    "取得する銘柄の候補が、`ticker.txt`に定義されています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_file = open(\"ticker.txt\")\n",
    "data = ticker_file.readlines()\n",
    "ticker_file.close()\n",
    "\n",
    "ticker_list = [i.rstrip('\\n') for i in data]\n",
    "\n",
    "print(len(ticker_list))\n",
    "print(ticker_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols = ['BBRY', 'AAPL', 'AMZN', 'BABA', 'YHOO', 'FB', 'GOOG', 'BBBY', 'JNUG', 'SBUX', 'MU']\n",
    "\n",
    "NUM_REQUEST = 200 - len(symbols)\n",
    "\n",
    "random.seed(12345)\n",
    "symbols.extend(random.sample(ticker_list, NUM_REQUEST))\n",
    "\n",
    "args = ['curl', '-X', 'GET', '']\n",
    "URL = \"https://api.stocktwits.com/api/2/streams/symbol/\"\n",
    "\n",
    "FILE_PATH = \"./data/\"\n",
    "\n",
    "start_datetime = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "for symbol in symbols:\n",
    "    try:\n",
    "        args[3] = URL + symbol + \".json\"\n",
    "        print(args[3])\n",
    "        proc = subprocess.run(args,stdout = subprocess.PIPE, stderr = subprocess.PIPE)\n",
    "\n",
    "        path = FILE_PATH + symbol + \"_\" + start_datetime + \".json\"\n",
    "        print(path)\n",
    "        with open(path, mode='w') as f:\n",
    "            f.write(proc.stdout.decode(\"utf8\"))\n",
    "    except:\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正常なレスポンス・ステータスを持っていないファイルを取り除きます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!grep -rlv '{\"response\":{\"status\":200}' data\n",
    "!grep -rlv '{\"response\":{\"status\":200}' data | xargs rm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、保存したファイルを、分散処理環境（クラスター）を使って加工するためにHDFSへコピーします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export HADOOP_CONF_DIR=/etc/hadoop/conf; hdfs dfs -mkdir ./twits/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export HADOOP_CONF_DIR=/etc/hadoop/conf; hdfs dfs -put ./data/* ./twits/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export HADOOP_CONF_DIR=/etc/hadoop/conf; hdfs dfs -ls ./twits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データ変換\n",
    "\n",
    "クラスターでデータを変換します。CDSW上では、ユーザーごとに別のプロジェクトを使っていましたが、クラスター環境では、自分が利用しているユーザーとデータを意識して取り扱う必要があります。\n",
    "\n",
    "\n",
    "あなたの（HADOOPクラスターへアクセスする）ユーザ名は以下で確認できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $HADOOP_USER_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データベースの準備\n",
    "\n",
    "\n",
    "\n",
    "**下記のセルの中を適切なユーザ名とURL（Hiveサーバー）に置換してください。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**下記のセルの中を適切なユーザ名とURL（Hiveサーバー）に置換してください。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql hive://<YOUR_USER_NAME>@master.ykono.work:10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**あなたのユーザ名でデータベースを作成・利用してください**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql CREATE DATABASE <YOUR_USER_NAME>\n",
    "%sql USE <YOUR_USER_NAME>\n",
    "%sql SHOW TABLES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ライブラリファイルのコピー・登録\n",
    "\n",
    "Hiveクエリの中でjsonファイルを扱えるようにするためのライブラリを登録します。\n",
    "ライブラリファイルはGithubリポジトリに含まれています（ライブラリの詳細は`/lib/README.jar`を参照ください）。\n",
    "はじめにCDSWからHDFSにコピーし、HDFS上のファイルをHiveへ登録します。\n",
    "\n",
    "コンパイル済みのライブラリファイルをリポジトリに含めています。\n",
    "- json-1.3.7.3.jar\n",
    "- json-serde-cdh5-shim-1.3.7.3.jar\n",
    "- json-serde-1.3.7.3.jar'\n",
    "\n",
    "- brickhouse-0.7.1-SNAPSHOT.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export HADOOP_CONF_DIR=/etc/hadoop/conf; hdfs dfs -put `ls -1 ./lib/*.jar` .; hdfs dfs -ls ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**下記のパスを適切なユーザ名で置換してください。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql add jar hdfs:/user/<YOUR_USER_NAME>/json-1.3.7.3.jar\n",
    "%sql add jar hdfs:/user/<YOUR_USER_NAME>/json-serde-1.3.7.3.jar\n",
    "%sql add jar hdfs:/user/<YOUR_USER_NAME>/json-serde-cdh5-shim-1.3.7.3.jar\n",
    "%sql add jar hdfs:/user/<YOUR_USER_NAME>/brickhouse-0.7.1-SNAPSHOT.jar\n",
    "%sql CREATE TEMPORARY FUNCTION to_json AS 'brickhouse.udf.json.ToJsonUDF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql DROP TABLE IF EXISTS twits\n",
    "%sql DROP TABLE IF EXISTS message_extracted\n",
    "%sql DROP TABLE IF EXISTS message_filtered\n",
    "%sql DROP TABLE IF EXISTS message_exploded\n",
    "%sql DROP TABLE IF EXISTS sentiment_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SNSメッセージファイルを格納した場所を指定して、テーブルを作成します。\n",
    "\n",
    "**`LOCATION`指定にあなたがファイルをアップロードしたパスを指定してください**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE EXTERNAL TABLE twits (\n",
    "\tmessages \n",
    "\tARRAY<\n",
    "\t    STRUCT<body: STRING,\n",
    "\t        symbols:ARRAY<STRUCT<symbol:STRING>>,\n",
    "\t        entities:STRUCT<sentiment:STRUCT<basic:STRING>>\n",
    "\t    >\n",
    "\t>\n",
    ")\n",
    "ROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe' \n",
    "STORED AS TEXTFILE\n",
    "LOCATION '/user/<YOUR_USER_NAME>/twits'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "select count(*) from twits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "select * from twits limit 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データ変換のためのテーブルを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql create table message_extracted (symbols array<struct<symbol:string>>, sentiment STRING, body STRING) STORED AS TEXTFILE\n",
    "%sql create table message_filtered (symbols array<struct<symbol:string>>, sentiment STRING, body STRING) STORED AS TEXTFILE\n",
    "%sql create table message_exploded (symbol string, sentiment STRING, body STRING) STORED AS TEXTFILE\n",
    "%sql create table sentiment_data (sentiment int, body STRING) STORED AS TEXTFILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "元のデータから必要なデータのみを抽出します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "insert overwrite table message_extracted \n",
    "select message.symbols, message.entities.sentiment, message.body from twits \n",
    "lateral view explode(messages) messages as message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "select * from message_extracted limit 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "select count(*) from message_extracted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データから、メッセージ・ボディが含まれているデータのみを取り出します。同時に、銘柄に対するセンチメントを文字列からを数値に置換します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "insert overwrite table message_filtered \n",
    "select symbols, \n",
    "    case sentiment when 'Bearish' then -2 when 'Bullish' then 2 ELSE 0 END as sentiment, \n",
    "    body from message_extracted \n",
    "    where body is not null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "select * from message_filtered limit 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一つのメッセージに複数の銘柄が紐づけられています。データ正規化のため、データ１行につき、一つの銘柄を持つようにデータを変換します（同じメッセージを持つ行が複数作られます）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "insert overwrite table message_exploded \n",
    "select symbol.symbol, sentiment, body from message_filtered lateral view explode(symbols) symbols as symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "select * from message_exploded limit 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここまでの操作で、元の複雑な構造のデータから、１レコードにつき、銘柄、センチメント、メッセージ本文を持つフォーマットに変換されました。\n",
    "銘柄毎のセンチメントの件数などの分析を行うには、このテーブルを利用します。\n",
    "\n",
    "この後の感情分析では、メッセージ本文の文字列から、センチメントを判定する予測モデルを構築します。そのため銘柄情報は利用しないため、センチメントとメッセージ本文のみを取り出します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "insert overwrite table sentiment_data \n",
    "select sentiment, body from message_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "select * from sentiment_data limit 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSONファイルの作成\n",
    "\n",
    "加工したデータをJSONファイルとして出力します。\n",
    "\n",
    "感情分析を担当するデータサイエンティスト・機械学習エンジニアは、このJSONファイルを使います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql DROP TABLE IF EXISTS json_message\n",
    "%sql create table json_message (message STRING) STORED AS TEXTFILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "insert overwrite table json_message\n",
    "select to_json(named_struct('message_body', body, 'sentiment', sentiment)) from sentiment_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "select * from json_message limit 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`HQL_SELECT_MESSAGE`をあなたが作成したデータベースを指定してください**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from __future__ import print_function\n",
    "\n",
    "HQL_SELECT_MESSAGE = \"select * from <YOUR_USER_NAME>.json_message\"\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"JsonGen\")\\\n",
    "    .getOrCreate()\n",
    "    \n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "json_list = spark.sql(HQL_SELECT_MESSAGE)\n",
    "\n",
    "path = \"./output.json\"\n",
    "\n",
    "with open(path, mode='w') as f:\n",
    "    f.write('{\"data\":[')\n",
    "    bool_first_line = True\n",
    "    for row in json_list.rdd.collect():\n",
    "        if bool_first_line:\n",
    "            bool_first_line = False\n",
    "            f.write(row.message)\n",
    "        else:\n",
    "            # あまりスマートではありませんが、ある程度の量のデータを使ったDeep Learning処理をシミュレーションするため、\n",
    "            # 同じ情報を使って、データを嵩増ししています。\n",
    "            # API利用の制約や、演習時間の制約がなければ、\n",
    "            # 上記のWebスクレイピングで、大量の訓練データを取得することが可能です。\n",
    "            for i in range(100): \n",
    "                f.write(\",\\n\")\n",
    "                f.write(row.message)\n",
    "    \n",
    "    f.write(\"]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l | grep output.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head output.json\n",
    "!tail output.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 感情分析\n",
    "\n",
    "投資判断のために、企業の価値を考慮する際のアプローチとして、従来の枠組みにとらわれない様々な情報（オルタナティブ・データ）を用いることを考えます。\n",
    "\n",
    "投資家の判断を左右し得る様々な情報を入力とし、投資判断のための定量的なシグナルに変換する予測モデルを構築します。\n",
    "入力となるデータには様々なものがあります。以下はその例です。\n",
    "\n",
    "- ニュース（製品のリコール、自然災害など）\n",
    "\n",
    "ニューラルネットワークを使ったDeep Learningによって、入力データの形式を問わず、予測モデルを構築することができます。\n",
    "\n",
    "ここでは、ソーシャルメディアサイトStockTwitsの投稿を使用します。\n",
    "StockTwitsのコミュニティは、投資家、トレーダー、起業家により利用されています。\n",
    "\n",
    "感情のスコアを生成するこれらのtwitを中心にモデルを構築します。\n",
    "\n",
    "モデルの訓練のためには、入力に対応するラベルが必要になります。ラベルの精度は、モデルの訓練に当たって大変重要な要素です。\n",
    "\n",
    "センチメントの度合いを把握するために、非常にネガティブ、ネガティブ、ニュートラル、ポジティブ、非常にポジティブという5段階のスケールを使用します。それぞれ、-2から2までの数値に対応しています。\n",
    "\n",
    "このラベル付きデータによって訓練されたモデルを使用して、自然言語を入力として、その文章の背後にある感情を予測するモデルを構築します。\n",
    "\n",
    "\n",
    "### データの確認\n",
    "データがどのように見えるかを確認します。\n",
    "\n",
    "各フィールドの意味:\n",
    "\n",
    "* `'message_body'`: メッセージ本文テキスト\n",
    "* `'sentiment'`: センチメントスコア。-2から2までの５段階。0は中立。\n",
    "\n",
    "下記のような内容になっているはずです。\n",
    "```\n",
    "{'data':\n",
    "  {'message_body': '............................',\n",
    "   'sentiment': 2},\n",
    "  {'message_body': '............................',\n",
    "   'sentiment': -2},\n",
    "   ...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データを読み込みます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./output.json', 'r') as f:\n",
    "    twits = json.load(f)\n",
    "\n",
    "print(twits['data'][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データ件数の確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(twits['data']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データの前処理\n",
    "\n",
    "テキストを前処理します。\n",
    "\n",
    "本文に含まれるティッカーシンボル（「$シンボル」で示される）はセンチメントに関する情報を提供しないため削除します。\n",
    "また、「@ユーザー名」で、ユーザに関する情報が記載されていますが、これもまたセンチメント情報を提供しないため、削除します。\n",
    "URLも削除します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### メッセージ本文とセンチメント・ラベルのリスト化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [twit['message_body'] for twit in twits['data']]\n",
    "# Since the sentiment scores are discrete, we'll scale the sentiments to 0 to 4 for use in our network\n",
    "sentiments = [twit['sentiment'] + 2 for twit in twits['data']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### プリプロセス関数の定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "\n",
    "def preprocess(message):\n",
    "    \"\"\"\n",
    "    入力として文字列を受け取り、次の操作を実行する: \n",
    "        - 全てのアルファベットを小文字に変換\n",
    "        - URLを削除\n",
    "        - ティッカーシンボルを削除 \n",
    "        - 句読点を削除\n",
    "        - 文字列をスペースで分割しトークン化する\n",
    "        - シングル・キャラクターのトークンを削除\n",
    "    \n",
    "    パラメータ\n",
    "    ----------\n",
    "        message : 前処理の対象テキストメッセージ\n",
    "        \n",
    "    戻り値\n",
    "    -------\n",
    "        tokens: 前処理後のトークン配列\n",
    "    \"\"\" \n",
    "    #TODO: Implement \n",
    "    \n",
    "    # Lowercase the twit message\n",
    "    text = message.lower()\n",
    "    \n",
    "    # Replace URLs with a space in the message\n",
    "    text = re.sub(\"http(s)?://([\\w\\-]+\\.)+[\\w-]+(/[\\w\\- ./?%&=]*)?\",' ', text)\n",
    "    \n",
    "    # Replace ticker symbols with a space. The ticker symbols are any stock symbol that starts with $.\n",
    "    text = re.sub(\"\\$[^ \\t\\n\\r\\f]+\", ' ', text)\n",
    "    \n",
    "    # Replace StockTwits usernames with a space. The usernames are any word that starts with @.\n",
    "    text = re.sub(\"@[^ \\t\\n\\r\\f]+\", ' ', text)\n",
    "\n",
    "    # Replace everything not a letter with a space\n",
    "    text = re.sub(\"[^a-z]\", ' ', text)\n",
    "    \n",
    "    \n",
    "    # Tokenize by splitting the string on whitespace into a list of words\n",
    "    tokens = text.split()\n",
    "\n",
    "    # Lemmatize words using the WordNetLemmatizer. You can ignore any word that is not longer than one character.\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    tokens = [wnl.lemmatize(w, pos='v') for w in tokens if len(w) > 1]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitsメッセージ前処理\n",
    "上記で定義した`preprocess`関数を全てStockTwitメッセージ・データに適用します。\n",
    "\n",
    "※この処理には、データのサイズに応じて多少時間がかかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = list(map(preprocess, messages))\n",
    "\n",
    "print(tokenized[:3])\n",
    "print(len(tokenized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words\n",
    "\n",
    "すべてのメッセージがトークン化されたので、ボキャブラリ（語彙）データを作成します。\n",
    "その際に、コーパス全体で各単語が出現する頻度をカウントします\n",
    "（[`Counter`](https://docs.python.org/3.1/library/collections.html#collections.Counter)関数を利用）。\n",
    "\n",
    "※この処理には、データのサイズに応じて多少時間がかかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "#words = []\n",
    "#for tokens in tokenized:\n",
    "#    for token in tokens:\n",
    "#        words.append(token)\n",
    "out_list = tokenized\n",
    "words = [element for in_list in out_list for element in in_list]\n",
    "\n",
    "print(words[:13])\n",
    "print(len(words))\n",
    "\n",
    "\"\"\"\n",
    "Create a vocabulary by using Bag of words\n",
    "\"\"\"\n",
    "\n",
    "word_counts = Counter(words)\n",
    "sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "int_to_vocab = {ii: word for ii, word in enumerate(sorted_vocab)}\n",
    "vocab_to_int = {word:ii for ii, word in int_to_vocab.items()}\n",
    "\n",
    "bow = []\n",
    "for tokens in tokenized:\n",
    "    bow.append([vocab_to_int[token] for token in tokens])\n",
    "\n",
    "print(len(bow))\n",
    "print(bow[:3])\n",
    "\n",
    "# This BOW will not be used because it is not filtered to eliminate common words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 単語の重要性（メッセージに現れる頻度）に応じた調整\n",
    "\n",
    "ボキャブラリーを使用して、「the」、「and」、「it」などの最も一般的な単語の一部を削除します。\n",
    "これらの単語は非常に一般的であるため、センチメントを特定する目的に寄与せず、ニューラルネットワークへの入力のノイズとなります。これらを除外することで、ネットワークの学習時間を短縮することができます。\n",
    "\n",
    "また、非常に稀にしか用いられない単語も削除します。\n",
    "ここでは、各単語のカウントをメッセージの数で除算する必要があります。\n",
    "\n",
    "次に、メッセージのごく一部にしか表示されない単語を削除します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Set the following variables:\n",
    "    freqs\n",
    "    low_cutoff\n",
    "    high_cutoff\n",
    "    K_most_common\n",
    "\"\"\"\n",
    "\n",
    "print(\"len(sorted_vocab):\",len(sorted_vocab))\n",
    "print(\"sorted_vocab - top:\", sorted_vocab[:3])\n",
    "print(\"sorted_vocab - least:\", sorted_vocab[-15:])\n",
    "\n",
    "# Dictionart that contains the Frequency of words appearing in messages.\n",
    "# The key is the token and the value is the frequency of that word in the corpus.\n",
    "total_count = len(words)\n",
    "freqs = {word: count/total_count for word, count in word_counts.items()}\n",
    "\n",
    "#print(\"freqs[supplication]:\",freqs[\"supplication\"] )\n",
    "print(\"freqs[the]:\",freqs[\"the\"] )\n",
    "\n",
    "\"\"\"\n",
    "This was the post by Ricardo:\n",
    "\n",
    "there's no exact value for low_cutoff and high_cutoff, \n",
    "however I'd recommend you to use \n",
    "a low_cutoff that's around 0.000002 and 0.000007 \n",
    "(This depends on the values you get from your freqs calculations) and \n",
    "a high_cutofffrom 5 to 20 (this depends on the most_common values from the bow).\n",
    "\"\"\"\n",
    "\n",
    "# Float that is the frequency cutoff. Drop words with a frequency that is lower or equal to this number.\n",
    "low_cutoff = 0.000002\n",
    "\n",
    "# Integer that is the cut off for most common words. Drop words that are the `high_cutoff` most common words.\n",
    "\"\"\"\n",
    "example_count = []\n",
    "example_count.append(sorted_vocab.index(\"the\"))\n",
    "example_count.append(sorted_vocab.index(\"for\"))\n",
    "example_count.append(sorted_vocab.index(\"of\"))\n",
    "print(example_count)\n",
    "high_cutoff = min(example_count)\n",
    "\"\"\"\n",
    "high_cutoff = 20\n",
    "print(\"high_cutoff:\",high_cutoff)\n",
    "print(\"low_cutoff:\",low_cutoff)\n",
    "\n",
    "# The k most common words in the corpus. Use `high_cutoff` as the k.\n",
    "#K_most_common = [word for word in sorted_vocab[:high_cutoff]]\n",
    "K_most_common = sorted_vocab[:high_cutoff]\n",
    "\n",
    "print(\"K_most_common:\",K_most_common)\n",
    "\n",
    "\n",
    "filtered_words = [word for word in freqs if (freqs[word] > low_cutoff and word not in K_most_common)]\n",
    "\n",
    "print(\"len(filtered_words):\",len(filtered_words)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### フィルターされた単語を削除して語彙を更新\n",
    "ボキャブラリーに役立つ3つの変数を作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Set the following variables:\n",
    "    vocab\n",
    "    id2vocab\n",
    "    filtered\n",
    "\"\"\"\n",
    "\n",
    "# A dictionary for the `filtered_words`. The key is the word and value is an id that represents the word. \n",
    "vocab =  {word:ii for ii, word in enumerate(filtered_words)}\n",
    "# Reverse of the `vocab` dictionary. The key is word id and value is the word. \n",
    "id2vocab = {ii:word for word, ii in vocab.items()}\n",
    "# tokenized with the words not in `filtered_words` removed.\n",
    "\n",
    "print(\"len(tokenized):\", len(tokenized))\n",
    "\n",
    "filtered = [[token for token in tokens if token in vocab] for tokens in tokenized]\n",
    "print(\"len(filtered):\", len(filtered))\n",
    "print(\"tokenized[:1]\", tokenized[:1])\n",
    "print(\"filtered[:1]\",filtered[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分類クラス間のバランス\n",
    "\n",
    "訓練データのラベルには、一般に偏りがあることがよく見受けられます（例外的なデータは少ない）。\n",
    "例えば、データの50％がニュートラルであること場合、毎回0（ニュートラル）を予測するだけで、ネットワークの精度が50％になることを意味します。\n",
    "\n",
    "ネットワークが適切に学習できるように、クラスのバランスを取る必要があります。つまり、それぞれのセンチメントスコアがデータにほぼ同じ頻度で表含まれていることが望ましいと言えます。\n",
    "\n",
    "ここでは、中立的な感情を持つデータを全体の20%になるように、ランダムにドロップします。\n",
    "\n",
    "データに含まれるニュートラルデータのパーセンテージと、データ削除により期待されるパーセンテージの値を使って、\n",
    "データをドロップする確率を求めます。\n",
    "\n",
    "同時に、長さが0のメッセージを削除します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced = {'messages': [], 'sentiments':[]}\n",
    "\n",
    "n_neutral = sum(1 for each in sentiments if each == 2)\n",
    "N_examples = len(sentiments)\n",
    "\n",
    "keep_prob = (N_examples - n_neutral)/4/n_neutral\n",
    "\n",
    "print(\"keep prob:\", keep_prob)\n",
    "\n",
    "for idx, sentiment in enumerate(sentiments):\n",
    "    message = filtered[idx]\n",
    "    if len(message) == 0:\n",
    "        # skip this message because it has length zero\n",
    "        continue\n",
    "    elif sentiment != 2 or random.random() < keep_prob:\n",
    "        balanced['messages'].append(message)\n",
    "        balanced['sentiments'].append(sentiment) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "バランスされたデータ中、センチメントが「ニュートラル」であるデータの割合を確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neutral = sum(1 for each in balanced['sentiments'] if each == 2)\n",
    "N_examples = len(balanced['sentiments'])\n",
    "n_neutral/N_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's convert our tokens into integer ids which we can pass to the network.\n",
    "\n",
    "メッセージをID（数値）に変換します。この処理は、ニューラルネットワークの入力として用いるために必要です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = [[vocab[word] for word in message] for message in balanced['messages']]\n",
    "sentiments = balanced['sentiments']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ボキャブラリ・ファイルを保存します。このファイルは、予測の際に、入力を変換するために必要になります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('vocab.pickle', 'wb') as f:\n",
    "    pickle.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ニューラルネットワーク\n",
    "これでボキャブラリーができたので、トークンをIDに変換し、それをネットワークに渡すことができます。ネットワークを定義します\n",
    "\n",
    "下記は、ネットワークの概要です：\n",
    "\n",
    "#### Embed -> RNN -> Dense -> Softmax\n",
    "\n",
    "### SentimentClassifier (感情分類器)実装\n",
    "\n",
    "クラスは、3つの主要な部分で構成されています：: \n",
    "\n",
    "1. init function `__init__` \n",
    "2. forward pass `forward`  \n",
    "3. hidden state `init_hidden`. \n",
    "\n",
    "出力層では、softmaxを使用します。出力フォーマットによって出力層を選択します。\n",
    "\n",
    "（例えば、出力が２値/バイナリであれば、シグモイド関数）\n",
    "\n",
    "このネットワークでは、センチメントスコアには5つのクラスがあるためsoftmaxが適しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, lstm_size, output_size, lstm_layers=1, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "            vocab_size : The vocabulary size.\n",
    "            embed_size : The embedding layer size.\n",
    "            lstm_size : The LSTM layer size.\n",
    "            output_size : The output size.\n",
    "            lstm_layers : The number of LSTM layers.\n",
    "            dropout : The dropout probability.\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.lstm_size = lstm_size\n",
    "        self.output_size = output_size\n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embed_size)\n",
    "        self.lstm = nn.LSTM(self.embed_size, self.lstm_size, self.lstm_layers, dropout=self.dropout)\n",
    "        \n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(lstm_size, output_size)\n",
    "        \n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\" \n",
    "        Initializes hidden state\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "            batch_size : The size of batches.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "            hidden_state\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # 隠れ層として、n_layers x batch_size x hidden_dimの構造を持つテンソルを二つ作成し、ゼロで初期化\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        \n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        hidden = (weight.new(self.lstm_layers, batch_size,self.lstm_size).zero_(),\n",
    "                         weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_())\n",
    "        return hidden\n",
    "\n",
    "\n",
    "    def forward(self, nn_input, hidden_state):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on nn_input.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "            nn_input : The batch of input to the NN.\n",
    "            hidden_state : The LSTM hidden state.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            logps: log softmax output\n",
    "            hidden_state: The new hidden state.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = nn_input.size(0)\n",
    "        \n",
    "        # embed\n",
    "        embeds = self.embedding(nn_input)\n",
    "        \n",
    "        # LSTM\n",
    "        lstm_out, hidden_state = self.lstm(embeds, hidden_state)\n",
    "        \n",
    "        \"\"\"\n",
    "        remember here you do not have batch_first=True, \n",
    "        so accordingly shape your input. \n",
    "        Moreover, since now input is seq_length x batch you just need to transform lstm_out = lstm_out[-1,:,:].\n",
    "        you don't have to use batch_first=True in this case, \n",
    "        nor reshape the outputs with .view just transform your lstm_out as advised and you should be good to go.\n",
    "        \"\"\"\n",
    "        #lstm_out = lstm_out.contiguous().view(-1, self.lstm_size)    \n",
    "        lstm_out = lstm_out[-1,:,:]\n",
    "        \n",
    "        # dropout\n",
    "        out = self.dropout(lstm_out)\n",
    "        \n",
    "        # Dense Layer (nn.Linear) RNNの隠れ層から値を予測\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        # Softmax関数\n",
    "        logps = self.softmax(out)\n",
    "        \n",
    "        \n",
    "        return logps, hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルの確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentimentClassifier(len(vocab), 10, 6, 5, dropout=0.1, lstm_layers=2)\n",
    "model.embedding.weight.data.uniform_(-1, 1)\n",
    "input = torch.randint(0, 1000, (5, 4), dtype=torch.int64)\n",
    "batch_size = 4\n",
    "hidden = model.init_hidden(4)\n",
    "\n",
    "logps, _ = model.forward(input, hidden)\n",
    "print(logps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### トレーニング\n",
    "### DataLoaderとバッチ処理\n",
    "ここで、データをループするために使用できるジェネレーターを構築します。\n",
    "\n",
    "効率化のため、シーケンスをバッチとして渡します。\n",
    "\n",
    "入力テンソルは次のような形になります：(sequence_length, batch_size)\n",
    "\n",
    "したがって、シーケンスが40トークンで、25シーケンスを渡す場合、入力サイズは(40, 25)になります。\n",
    "\n",
    "シーケンスの長さを40に設定した場合、40トークンより多いまたは少ないメッセージは、以下のように処理します。\n",
    "- 40トークン未満のメッセージの場合、空のスポットにゼロを埋め込む。\n",
    "   - データを処理する前にRNNが何も開始しないように、必ずパッドを残しておく必要がある。\n",
    "   - メッセージに20個のトークンがある場合、最初の20個のスポットは0になる。\n",
    "- メッセージに40個を超えるトークンがある場合、最初の40個のトークンを保持。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def dataloader(messages, labels, sequence_length=30, batch_size=32, shuffle=False):\n",
    "def dataloader(messages, labels, sequence_length=20, batch_size=32, shuffle=False):\n",
    "    \"\"\" \n",
    "    Build a dataloader.\n",
    "    \"\"\"\n",
    "    if shuffle:\n",
    "        indices = list(range(len(messages)))\n",
    "        random.shuffle(indices)\n",
    "        messages = [messages[idx] for idx in indices]\n",
    "        labels = [labels[idx] for idx in indices]\n",
    "\n",
    "    total_sequences = len(messages)\n",
    "\n",
    "    for ii in range(0, total_sequences, batch_size):\n",
    "        batch_messages = messages[ii: ii+batch_size]\n",
    "        \n",
    "        # First initialize a tensor of all zeros\n",
    "        batch = torch.zeros((sequence_length, len(batch_messages)), dtype=torch.int64)\n",
    "        for batch_num, tokens in enumerate(batch_messages):\n",
    "            token_tensor = torch.tensor(tokens)\n",
    "            # Left pad!\n",
    "            start_idx = max(sequence_length - len(token_tensor), 0)\n",
    "            batch[start_idx:, batch_num] = token_tensor[:sequence_length]\n",
    "        \n",
    "        label_tensor = torch.tensor(labels[ii: ii+len(batch_messages)])\n",
    "        \n",
    "        yield batch, label_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  データの分割（訓練用と検証用）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Split data into training and validation datasets. Use an appropriate split size.\n",
    "The features are the `token_ids` and the labels are the `sentiments`.\n",
    "\"\"\"   \n",
    "\n",
    "split_frac = 0.98 # for small data\n",
    "#split_frac = 0.8 # for big data\n",
    "\n",
    "## split data into training, validation, and test data (features and labels, x and y)\n",
    "\n",
    "split_idx = int(len(token_ids)*split_frac)\n",
    "train_features, remaining_features = token_ids[:split_idx], token_ids[split_idx:]\n",
    "train_labels, remaining_labels = sentiments[:split_idx], sentiments[split_idx:]\n",
    "\n",
    "test_idx = int(len(remaining_features)*0.5)\n",
    "valid_features, test_features = remaining_features[:test_idx], remaining_features[test_idx:]\n",
    "valid_labels, test_labels = remaining_labels[:test_idx], remaining_labels[test_idx:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### トレーニング準備\n",
    "\n",
    "利用可能なデバイス(CUDA/GPUまたはGPU)を確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = SentimentClassifier(len(vocab)+1, 200, 128, 5, dropout=0.)\n",
    "model = SentimentClassifier(len(vocab)+1, 1024, 512, 5, lstm_layers=2, dropout=0.2)\n",
    "model.embedding.weight.data.uniform_(-1, 1)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### トレーニング実施\n",
    "\n",
    "トレーニングを実行します。モデルの訓練の進行度合を確認するために、定期的にLossを出力します。\n",
    "\n",
    "※この処理には、データのサイズに応じて、十分な時間が必要です。\n",
    "\n",
    "GPUを備えた環境で実行する場合、ターミナルで以下のコマンドを実行することで、GPUが利用されていることを確認することができます（ GPU実行中、コマンド実行により表示されるテーブルの右上のVolatile GPU-Utilのパーセンテージ値が増えます）\n",
    "```\n",
    "$ watch nvidia-smi\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "epochs = 5\n",
    "batch_size =  64\n",
    "batch_size =  512\n",
    "learning_rate = 0.001\n",
    "\n",
    "print_every = 100\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "model.train()\n",
    "\n",
    "#val_losses = []\n",
    "total_losses = []\n",
    "#accuracy = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Starting epoch {}'.format(epoch + 1))\n",
    "    \n",
    "    steps = 0\n",
    "    for text_batch, labels in dataloader(\n",
    "            train_features, train_labels, batch_size=batch_size, sequence_length=20, shuffle=True):\n",
    "        steps += 1\n",
    "        hidden = model.init_hidden(labels.shape[0]) \n",
    "        \n",
    "        # デバイス(CPU, GPU)の設定\n",
    "        text_batch, labels = text_batch.to(device), labels.to(device)\n",
    "        for each in hidden:\n",
    "            each.to(device)\n",
    "        \n",
    "        # モデルのトレーニング\n",
    "        hidden = tuple([each.data for each in hidden])\n",
    "        model.zero_grad()\n",
    "        output, hidden = model(text_batch, hidden)\n",
    "        loss = criterion(output.squeeze(), labels)\n",
    "        loss.backward()\n",
    "        clip = 5\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate loss\n",
    "        #val_losses.append(loss.item())\n",
    "        total_losses.append(loss.item())\n",
    "        \n",
    "        correct_count = 0.0\n",
    "        if steps % print_every == 0:\n",
    "            model.eval()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            ps = torch.exp(output)\n",
    "            top_p, top_class = ps.topk(1, dim=1)\n",
    "            #?top_class = top_class.to(device)\n",
    "            #?labels = labels.to(device)\n",
    "\n",
    "            correct_count += torch.sum(top_class.squeeze()== labels)\n",
    "            #accuracy.append(100*correct_count/len(labels))\n",
    "            \n",
    "            # Print metrics\n",
    "            print(\"Epoch: {}/{}...\".format(epoch+1, epochs),\n",
    "                 \"Step: {}...\".format(steps),\n",
    "                 \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                 \"Total Loss: {:.6f}\".format(np.mean(total_losses)),\n",
    "                 #\"Collect Count: {}\".format(correct_count),\n",
    "                 #\"Accuracy: {:.2f}\".format((100*correct_count/len(labels))),\n",
    "                 # AttributeError: 'torch.dtype' object has no attribute 'type'\n",
    "                 #\"Accuracy Avg: {:.2f}\".format(np.mean(accuracy))\n",
    "                 )\n",
    "            \n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'state_dict': model.state_dict()}, 'checkpoint.pth.tar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 予測（Prediction）関数の作成\n",
    "\n",
    "訓練されたモデルを使って、入力されたテキストから予測結果を生成するpredict関数を実装します。\n",
    "\n",
    "テキストは、ネットワークに渡される前に前処理される必要があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pickle\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "cur_dir = os.path.dirname(os.path.abspath('__file__'))\n",
    "print(cur_dir)\n",
    "sys.path.append(cur_dir)\n",
    "\n",
    "vocab_filename = 'vocab.pickle'\n",
    "vocab_path = cur_dir + \"/\" + vocab_filename\n",
    "vocab_l = pickle.load(open(vocab_path, 'rb'))\n",
    "\n",
    "#model_path = cur_dir + \"/\" + \"model.torch\"\n",
    "#model_l = torch.load(model_path, map_location='cpu')\n",
    "\n",
    "model_l = SentimentClassifier(len(vocab_l)+1, 1024, 512, 5, lstm_layers=2, dropout=0.2)\n",
    "checkpoint = torch.load('./checkpoint.pth.tar')\n",
    "model_l.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "class UnknownWordsError(Exception):\n",
    "  \"Only unknown words are included in text\"\n",
    "\n",
    "\n",
    "def predict_func(text, model, vocab):\n",
    "    \"\"\" \n",
    "    Make a prediction on a single sentence.\n",
    "    Parameters\n",
    "    ----------\n",
    "        text : The string to make a prediction on.\n",
    "        model : The model to use for making the prediction.\n",
    "        vocab : Dictionary for word to word ids. The key is the word and the value is the word id.\n",
    "    Returns\n",
    "    -------\n",
    "        pred : 予測値（numpyベクトル）\n",
    "    \"\"\"\n",
    "\n",
    "    tokens = preprocess(text)    \n",
    "\n",
    "    # Filter non-vocab words\n",
    "    tokens = [token for token in tokens if token in vocab] #pass\n",
    "    # Convert words to ids\n",
    "    tokens = [vocab[token] for token in tokens] #pass\n",
    "\n",
    "    if len(tokens) == 0:\n",
    "        raise UnknownWordsError\n",
    "\n",
    "    # Adding a batch dimension\n",
    "    text_input = torch.from_numpy(np.asarray(torch.LongTensor(tokens).view(-1, 1)))\n",
    "\n",
    "    # Get the NN output       \n",
    "    batch_size = 1\n",
    "    hidden = model.init_hidden(batch_size) #pass\n",
    "    \n",
    "    logps, _ = model(text_input, hidden) #pass\n",
    "    # Take the exponent of the NN output to get a range of 0 to 1 for each label.\n",
    "    pred = torch.round(logps.squeeze())#pass\n",
    "    pred = torch.exp(logps) \n",
    "    \n",
    "    return pred\n",
    "\n",
    "\n",
    "def predict_api(args):\n",
    "    \"\"\" \n",
    "    Make a prediction on a single sentence.\n",
    "    Parameters\n",
    "    ----------\n",
    "        args : 入力（Pythonディクショナリ）\n",
    "    Returns\n",
    "    -------\n",
    "        pred : 予測値（Python配列）\n",
    "    \"\"\"\n",
    "    text = args.get('text')\n",
    "    try:\n",
    "        result = predict_func(text, model_l, vocab_l)\n",
    "        return result.detach().numpy()[0]\n",
    "    except UnknownWordsError:\n",
    "        return [0,0,1,0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ポジティブなセンチメントを連想させる文章を入力として予測（適宜、文章を変更して実行してみることができます）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\"text\": \"I'm bullish on $goog\"}\n",
    "result = predict_api(args)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ネガティブなセンチメントを連想させる文章を入力として予測（適宜、文章を変更して実行してみることができます）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\"text\": \"I'm bearish on $goog\"}\n",
    "result = predict_api(args)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ボキャブラリ辞書に存在しない単語のみの文章を入力として予測（適宜、文章を変更して実行してみることができます）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\"text\": \"kono yoshiyuki\"}\n",
    "result = predict_api(args)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最後に\n",
    "\n",
    "データベースを削除する場合は、**データベース名を適切に変更した後で**下記を実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql DROP DATABASE IF EXISTS <YOUR_USER_NAME> CASCADE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
