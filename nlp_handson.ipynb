{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science / Machine Learning Meetup #1 Deep Learning Hands-on\n",
    "# オルタナティブ・データと自然言語処理\n",
    "\n",
    "## 演習概略\n",
    "\n",
    "1. [前処理](#前処理)\n",
    "1. [モデル構築](#モデル構築)\n",
    "1. [予測](#予測)\n",
    "\n",
    "\n",
    "## 環境準備\n",
    "\n",
    "### パッケージのインストールとインポート\n",
    "\n",
    "パッケージをインストールします（この処理は、プロジェクトで一度行えばよく、後から再度このファイルを使って分析する際にはスキップできます）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install nltk==3.4.5\n",
    "!pip3 install torch==1.4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上でインストールしたパッケージを含め、必要なモジュールをインポートします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import subprocess\n",
    "import glob\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "\n",
    "import sys\n",
    "\n",
    "from operator import add\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import torch\n",
    "import nltk\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テーブルまたはリストのデータを表形式で表示するための、関数を定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_plot(list, head_count=10):\n",
    "\n",
    "    df_s = pd.DataFrame(list)\n",
    "    df_s = df_s.head(head_count)\n",
    "\n",
    "    fig,ax = plt.subplots(figsize=((len(df_s.columns)+1)*1.2, (len(df_s)+1)*0.4))\n",
    "    ax.axis('off')\n",
    "    tbl = ax.table(cellText=df_s.values,\n",
    "                   bbox=[0,0,1,1],\n",
    "                   colLabels=df_s.columns,\n",
    "                   rowLabels=df_s.index)\n",
    "    # plt.savefig('table.png') #PNG画像出力\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ファイルの確認\n",
    "\n",
    "本ワークショップでは、事前に準備してあるトレーニング・データを用いて、感情分析のためのモデル構築を実施します。\n",
    "\n",
    "クラスターのtmpディレクトリにあるファイルを自分のプロジェクト環境にコピーします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv output.json output.json.pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export HADOOP_CONF_DIR=/etc/hadoop/conf; hdfs dfs -get /tmp/output.json ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l | grep output.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head output.json\n",
    "!tail output.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前処理\n",
    "\n",
    "\n",
    "### データの確認\n",
    "データがどのように見えるかを確認します。\n",
    "\n",
    "下記のような内容になっているはずです。\n",
    "```\n",
    "[{'message_body': '............................', 'sentiment': 2}, {'message_body': '............................', 'sentiment': -2}, ...}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./output.json', 'r') as f:\n",
    "    twits = json.load(f)\n",
    "\n",
    "print(twits['data'][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データ件数の確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(twits['data']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_messages = [twit['message_body'] for twit in twits['data'] if twit['sentiment'] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(positive_messages[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_messages = [twit['message_body'] for twit in twits['data'] if twit['sentiment'] == -2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(negative_messages[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### メッセージ本文とセンチメント・ラベルのリスト化\n",
    "ニューラルネットワークで利用するために、-2 ~ 2 を取るセンチメント・スコアを 0 ~ 4 に変換します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [twit['message_body'] for twit in twits['data']]\n",
    "sentiments = [twit['sentiment'] + 2 for twit in twits['data']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データの前処理\n",
    "\n",
    "テキストを前処理します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### プリプロセス関数の定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "\n",
    "def preprocess(message):\n",
    "    \"\"\"\n",
    "    入力として文字列を受け取り、次の操作を実行する: \n",
    "        - 全てのアルファベットを小文字に変換\n",
    "        - URLを削除\n",
    "        - ティッカーシンボル($WWW)を削除\n",
    "        - ユーザ名(@WWW)を削除 \n",
    "        - 句読点等、アルファベット以外を削除\n",
    "        - 文字列をスペースで分割しトークン化\n",
    "        - 動詞の時制や、名詞の複数形など文法的な情報（構文中での語の使い方）を抽象し、単語を一般化\n",
    "        - シングル・キャラクターのトークンを削除\n",
    "    \n",
    "    パラメータ\n",
    "    ----------\n",
    "        message : 前処理の対象テキストメッセージ\n",
    "        \n",
    "    戻り値\n",
    "    -------\n",
    "        tokens: 前処理後のトークン配列\n",
    "    \"\"\" \n",
    "    \n",
    "    text = message.lower()\n",
    "    \n",
    "    text = re.sub(\"http(s)?://([\\w\\-]+\\.)+[\\w-]+(/[\\w\\- ./?%&=]*)?\",' ', text)\n",
    "    \n",
    "    text = re.sub(\"\\$[^ \\t\\n\\r\\f]+\", ' ', text)\n",
    "    \n",
    "    text = re.sub(\"@[^ \\t\\n\\r\\f]+\", ' ', text)\n",
    "\n",
    "    text = re.sub(\"[^a-z]\", ' ', text)\n",
    "    \n",
    "    tokens = text.split()\n",
    "\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    tokens = [wnl.lemmatize(w, pos='v') for w in tokens if len(w) > 1]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### メッセージ前処理\n",
    "上記で定義した`preprocess`関数を全てStockTwitメッセージ・データに適用します。\n",
    "\n",
    "※この処理には、データのサイズに応じて多少時間がかかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = list(map(preprocess, messages))\n",
    "\n",
    "print(tokenized[:3])\n",
    "print(len(tokenized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words\n",
    "\n",
    "すべてのメッセージがトークン化されたので、ボキャブラリ（語彙）データを作成します。\n",
    "その際に、コーパス全体で各単語が出現する頻度をカウントします\n",
    "（[`Counter`](https://docs.python.org/3.1/library/collections.html#collections.Counter)関数を利用）。\n",
    "\n",
    "※この処理には、データのサイズに応じて多少時間がかかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "out_list = tokenized\n",
    "words = [element for in_list in out_list for element in in_list]\n",
    "\n",
    "word_counts = Counter(words)\n",
    "sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "int_to_vocab = {ii: word for ii, word in enumerate(sorted_vocab)}\n",
    "vocab_to_int = {word:ii for ii, word in int_to_vocab.items()}\n",
    "\n",
    "bow = []\n",
    "for tokens in tokenized:\n",
    "    bow.append([vocab_to_int[token] for token in tokens])\n",
    "\n",
    "print(len(bow))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 単語の重要性（メッセージに現れる頻度）に応じた調整\n",
    "\n",
    "ボキャブラリーを使用して、「the」、「and」、「it」などの最も一般的な単語の一部を削除します。\n",
    "これらの単語は非常に一般的であるため、センチメントを特定する目的に寄与せず、ニューラルネットワークへの入力のノイズとなります。これらを除外することで、ネットワークの学習時間を短縮することができます。\n",
    "\n",
    "また、非常に稀にしか用いられない単語も削除します。\n",
    "ここでは、各単語のカウントをメッセージの数で除算する必要があります。\n",
    "\n",
    "次に、メッセージのごく一部にしか表示されない単語を削除します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"len(sorted_vocab):\",len(sorted_vocab))\n",
    "print(\"sorted_vocab - top:\", sorted_vocab[:3])\n",
    "print(\"sorted_vocab - least:\", sorted_vocab[-15:])\n",
    "\n",
    "total_count = len(words)\n",
    "freqs = {word: count/total_count for word, count in word_counts.items()}\n",
    "\n",
    "print(\"freqs[the]:\",freqs[\"the\"] )\n",
    "\n",
    "low_cutoff = 0.000002\n",
    "high_cutoff = 20\n",
    "print(\"high_cutoff:\",high_cutoff)\n",
    "print(\"low_cutoff:\",low_cutoff)\n",
    "\n",
    "K_most_common = sorted_vocab[:high_cutoff]\n",
    "\n",
    "print(\"K_most_common:\",K_most_common)\n",
    "\n",
    "filtered_words = [word for word in freqs if (freqs[word] > low_cutoff and word not in K_most_common)]\n",
    "\n",
    "print(\"len(filtered_words):\",len(filtered_words)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  データ探索\n",
    "\n",
    "### 一般的な単語"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_plot(K_most_common, head_count=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### センチメント毎の頻出単語"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_sorted_vocab(messages):\n",
    "    words = [word for message in messages for word in message]\n",
    "\n",
    "    word_counts = Counter(words)\n",
    "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "\n",
    "    return sorted_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_words(sorted_vocab, freqs):\n",
    "\n",
    "    low_cutoff = 0.000002\n",
    "\n",
    "    high_cutoff = 20\n",
    "\n",
    "    K_most_common = sorted_vocab[:high_cutoff]\n",
    "\n",
    "    filtered_words = [word for word in freqs if (freqs[word] > low_cutoff and word not in K_most_common)]\n",
    "    \n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_messages = [twit['message_body'] for twit in twits['data'] if twit['sentiment'] == -2]\n",
    "negative_tokenized = list(map(preprocess, negative_messages))\n",
    "negative_sorted_vocab = convert_to_sorted_vocab(negative_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_plot(negative_sorted_vocab[20:], head_count=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_messages = [twit['message_body'] for twit in twits['data'] if twit['sentiment'] == 2]\n",
    "positive_tokenized = list(map(preprocess, positive_messages))\n",
    "positive_sorted_vocab = convert_to_sorted_vocab(positive_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_plot(positive_sorted_vocab[20:], head_count=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### フィルターされた単語を削除して語彙辞書を更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab =  {word:ii for ii, word in enumerate(filtered_words)}\n",
    "\n",
    "id2vocab = {ii:word for word, ii in vocab.items()}\n",
    "\n",
    "print(\"len(tokenized):\", len(tokenized))\n",
    "\n",
    "filtered = [[token for token in tokens if token in vocab] for tokens in tokenized]\n",
    "\n",
    "print(\"len(filtered):\", len(filtered))\n",
    "print(\"tokenized[:1]\", tokenized[:1])\n",
    "print(\"filtered[:1]\",filtered[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分類クラス間のバランス\n",
    "\n",
    "訓練データのラベルには、一般に偏りがあることがよく見受けられます（例外的なデータは少ない）。\n",
    "例えば、データの50％がニュートラルであること場合、毎回0（ニュートラル）を予測するだけで、ネットワークの精度が50％になることを意味します。\n",
    "\n",
    "ネットワークが適切に学習できるように、クラスのバランスを取る必要があります。つまり、それぞれのセンチメントスコアがデータにほぼ同じ頻度で表含まれていることが望ましいと言えます。\n",
    "\n",
    "ここでは、中立的な感情を持つデータを全体の20%になるように、ランダムにドロップします。\n",
    "\n",
    "データに含まれるニュートラルデータのパーセンテージと、データ削除により期待されるパーセンテージの値を使って、\n",
    "データをドロップする確率を求めます。\n",
    "\n",
    "同時に、長さが0のメッセージを削除します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced = {'messages': [], 'sentiments':[]}\n",
    "\n",
    "n_neutral = sum(1 for each in sentiments if each == 2)\n",
    "N_examples = len(sentiments)\n",
    "\n",
    "keep_prob = (N_examples - n_neutral)/4/n_neutral\n",
    "\n",
    "print(\"keep prob:\", keep_prob)\n",
    "\n",
    "for idx, sentiment in enumerate(sentiments):\n",
    "    message = filtered[idx]\n",
    "    if len(message) == 0:\n",
    "        # skip this message because it has length zero\n",
    "        continue\n",
    "    elif sentiment != 2 or random.random() < keep_prob:\n",
    "        balanced['messages'].append(message)\n",
    "        balanced['sentiments'].append(sentiment) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "バランスされたデータ中、センチメントが「ニュートラル」であるデータの割合を確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neutral = sum(1 for each in balanced['sentiments'] if each == 2)\n",
    "N_examples = len(balanced['sentiments'])\n",
    "n_neutral/N_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ボキャブラリ・ファイルを保存します。このファイルは、予測の際に、入力を変換するために必要になります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('vocab.pickle', 'wb') as f:\n",
    "    pickle.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの構築\n",
    "\n",
    "これでボキャブラリーができたので、トークンをIDに変換し、それをネットワークに渡すことができます。ネットワークを定義します\n",
    "\n",
    "\n",
    "### SentimentClassifier (感情分類器)実装\n",
    "\n",
    "クラスは、3つの主要な部分で構成されています：: \n",
    "\n",
    "1. init function `__init__` \n",
    "2. forward pass `forward`  \n",
    "3. hidden state `init_hidden`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, lstm_size, output_size, lstm_layers=1, dropout=0.1):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.lstm_size = lstm_size\n",
    "        self.output_size = output_size\n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embed_size)\n",
    "        self.lstm = nn.LSTM(self.embed_size, self.lstm_size, self.lstm_layers, dropout=self.dropout)\n",
    "        \n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(lstm_size, output_size)\n",
    "        \n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        \n",
    "        # 隠れ層として、n_layers x batch_size x hidden_dimの構造を持つテンソルを二つ作成し、ゼロで初期化        \n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        hidden = (weight.new(self.lstm_layers, batch_size,self.lstm_size).zero_(),\n",
    "                         weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_())\n",
    "        return hidden\n",
    "\n",
    "\n",
    "    def forward(self, nn_input, hidden_state):\n",
    "        \n",
    "        batch_size = nn_input.size(0)\n",
    "        \n",
    "        # embed\n",
    "        embeds = self.embedding(nn_input)\n",
    "        \n",
    "        # LSTM\n",
    "        lstm_out, hidden_state = self.lstm(embeds, hidden_state)\n",
    "\n",
    "        lstm_out = lstm_out[-1,:,:]\n",
    "        \n",
    "        # dropout\n",
    "        out = self.dropout(lstm_out)\n",
    "        \n",
    "        # Dense Layer (nn.Linear) RNNの隠れ層から値を予測\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        # Softmax関数\n",
    "        logps = self.softmax(out)\n",
    "        \n",
    "        \n",
    "        return logps, hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルの確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentimentClassifier(len(vocab), 10, 6, 5, dropout=0.1, lstm_layers=2)\n",
    "model.embedding.weight.data.uniform_(-1, 1)\n",
    "input = torch.randint(0, 1000, (5, 4), dtype=torch.int64)\n",
    "batch_size = 4\n",
    "hidden = model.init_hidden(4)\n",
    "\n",
    "logps, _ = model.forward(input, hidden)\n",
    "print(logps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader定義\n",
    "\n",
    "データをバッチとして渡す関数を定義します。\n",
    "\n",
    "ネットワークへの入力は次のような形になります：(sequence_length, batch_size)\n",
    "\n",
    "したがって、シーケンス（メッセージデータ）が40トークンで、25シーケンスを渡す場合、入力サイズは(40, 25)になります。\n",
    "\n",
    "シーケンスの長さを40に設定した場合、40トークンより多いまたは少ないメッセージは、以下のように処理します。\n",
    "- 40トークン未満のメッセージの場合、足りない部分は先頭からゼロを埋め込む。\n",
    "   - データを処理する前にRNNが何も開始しないように、先頭にを空欄を配置します。\n",
    "   - メッセージに20個のトークンがある場合、最初の20個のスポットは0になる。\n",
    "- メッセージに40個を超えるトークンがある場合、最初の40個のトークンを保持。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(messages, labels, sequence_length=20, batch_size=32, shuffle=False):\n",
    "    if shuffle:\n",
    "        indices = list(range(len(messages)))\n",
    "        random.shuffle(indices)\n",
    "        messages = [messages[idx] for idx in indices]\n",
    "        labels = [labels[idx] for idx in indices]\n",
    "\n",
    "    total_sequences = len(messages)\n",
    "\n",
    "    for ii in range(0, total_sequences, batch_size):\n",
    "        batch_messages = messages[ii: ii+batch_size]\n",
    "        \n",
    "        # ゼロで初期化\n",
    "        batch = torch.zeros((sequence_length, len(batch_messages)), dtype=torch.int64)\n",
    "        for batch_num, tokens in enumerate(batch_messages):\n",
    "            token_tensor = torch.tensor(tokens)\n",
    "            # レフトパッド\n",
    "            start_idx = max(sequence_length - len(token_tensor), 0)\n",
    "            batch[start_idx:, batch_num] = token_tensor[:sequence_length]\n",
    "        \n",
    "        label_tensor = torch.tensor(labels[ii: ii+len(batch_messages)])\n",
    "        \n",
    "        yield batch, label_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### トークンIDの作成\n",
    "\n",
    "トレーニングに用いるメッセージに含まれる単語を、辞書を使ってID（数値）に変換します。この処理は、ニューラルネットワークの入力として用いるために必要です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = [[vocab[word] for word in message] for message in balanced['messages']]\n",
    "sentiments = balanced['sentiments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(token_ids[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  データの分割（訓練用と検証用）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_frac = 0.98 # for small data\n",
    "#split_frac = 0.8 # for big data\n",
    "\n",
    "split_idx = int(len(token_ids)*split_frac)\n",
    "train_features, remaining_features = token_ids[:split_idx], token_ids[split_idx:]\n",
    "train_labels, remaining_labels = sentiments[:split_idx], sentiments[split_idx:]\n",
    "\n",
    "test_idx = int(len(remaining_features)*0.5)\n",
    "valid_features, test_features = remaining_features[:test_idx], remaining_features[test_idx:]\n",
    "valid_labels, test_labels = remaining_labels[:test_idx], remaining_labels[test_idx:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### トレーニング\n",
    "\n",
    "#### トレーニング準備\n",
    "\n",
    "利用可能なデバイス(CUDA/GPUまたはGPU)を確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = SentimentClassifier(len(vocab)+1, 200, 128, 5, dropout=0.)\n",
    "model = SentimentClassifier(len(vocab)+1, 1024, 512, 5, lstm_layers=2, dropout=0.2)\n",
    "model.embedding.weight.data.uniform_(-1, 1)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### トレーニング実施\n",
    "\n",
    "トレーニングを実行します。モデルの訓練の進行度合を確認するために、定期的にLossを出力します。\n",
    "\n",
    "※この処理には、データのサイズに応じて、十分な時間が必要です。\n",
    "\n",
    "GPUを備えた環境で実行する場合、ターミナルで以下のコマンドを実行することで、GPUが利用されていることを確認することができます（ GPU実行中、コマンド実行により表示されるテーブルの右上のVolatile GPU-Utilのパーセンテージ値が増えます）\n",
    "```\n",
    "$ watch nvidia-smi\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "epochs = 5\n",
    "batch_size =  64\n",
    "batch_size =  512\n",
    "learning_rate = 0.001\n",
    "\n",
    "print_every = 100\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "model.train()\n",
    "\n",
    "#val_losses = []\n",
    "total_losses = []\n",
    "#accuracy = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Starting epoch {}'.format(epoch + 1))\n",
    "    \n",
    "    steps = 0\n",
    "    for text_batch, labels in dataloader(\n",
    "            train_features, train_labels, batch_size=batch_size, sequence_length=20, shuffle=True):\n",
    "        steps += 1\n",
    "        hidden = model.init_hidden(labels.shape[0]) \n",
    "        \n",
    "        # デバイス(CPU, GPU)の設定\n",
    "        text_batch, labels = text_batch.to(device), labels.to(device)\n",
    "        for each in hidden:\n",
    "            each.to(device)\n",
    "        \n",
    "        # モデルのトレーニング\n",
    "        hidden = tuple([each.data for each in hidden])\n",
    "        model.zero_grad()\n",
    "        output, hidden = model(text_batch, hidden)\n",
    "        loss = criterion(output.squeeze(), labels)\n",
    "        loss.backward()\n",
    "        clip = 5\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate loss\n",
    "        #val_losses.append(loss.item())\n",
    "        total_losses.append(loss.item())\n",
    "        \n",
    "        correct_count = 0.0\n",
    "        if steps % print_every == 0:\n",
    "            model.eval()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            ps = torch.exp(output)\n",
    "            top_p, top_class = ps.topk(1, dim=1)\n",
    "            #?top_class = top_class.to(device)\n",
    "            #?labels = labels.to(device)\n",
    "\n",
    "            correct_count += torch.sum(top_class.squeeze()== labels)\n",
    "            #accuracy.append(100*correct_count/len(labels))\n",
    "            \n",
    "            # Print metrics\n",
    "            print(\"Epoch: {}/{}...\".format(epoch+1, epochs),\n",
    "                 \"Step: {}...\".format(steps),\n",
    "                 \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                 \"Total Loss: {:.6f}\".format(np.mean(total_losses)),\n",
    "                 #\"Collect Count: {}\".format(correct_count),\n",
    "                 #\"Accuracy: {:.2f}\".format((100*correct_count/len(labels))),\n",
    "                 # AttributeError: 'torch.dtype' object has no attribute 'type'\n",
    "                 #\"Accuracy Avg: {:.2f}\".format(np.mean(accuracy))\n",
    "                 )\n",
    "            \n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'state_dict': model.state_dict()}, 'checkpoint.pth.tar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 予測\n",
    "\n",
    "### Prediction関数の作成\n",
    "\n",
    "訓練されたモデルを使って、入力されたテキストから予測結果を生成するpredict関数を実装します。\n",
    "\n",
    "テキストは、ネットワークに渡される前に前処理される必要があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pickle\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "cur_dir = os.path.dirname(os.path.abspath('__file__'))\n",
    "print(cur_dir)\n",
    "sys.path.append(cur_dir)\n",
    "\n",
    "vocab_filename = 'vocab.pickle'\n",
    "vocab_path = cur_dir + \"/\" + vocab_filename\n",
    "vocab_l = pickle.load(open(vocab_path, 'rb'))\n",
    "\n",
    "#model_path = cur_dir + \"/\" + \"model.torch\"\n",
    "#model_l = torch.load(model_path, map_location='cpu')\n",
    "\n",
    "model_l = SentimentClassifier(len(vocab_l)+1, 1024, 512, 5, lstm_layers=2, dropout=0.2)\n",
    "checkpoint = torch.load('./checkpoint.pth.tar')\n",
    "model_l.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "class UnknownWordsError(Exception):\n",
    "    \"Only unknown words are included in text\"\n",
    "\n",
    "\n",
    "def predict_func(text, model, vocab):\n",
    "    \"\"\" \n",
    "    Make a prediction on a single sentence.\n",
    "    Parameters\n",
    "    ----------\n",
    "        text : The string to make a prediction on.\n",
    "        model : The model to use for making the prediction.\n",
    "        vocab : Dictionary for word to word ids. The key is the word and the value is the word id.\n",
    "    Returns\n",
    "    -------\n",
    "        pred : 予測値（numpyベクトル）\n",
    "    \"\"\"\n",
    "\n",
    "    tokens = preprocess(text)    \n",
    "\n",
    "    # Filter non-vocab words\n",
    "    tokens = [token for token in tokens if token in vocab] #pass\n",
    "    # Convert words to ids\n",
    "    tokens = [vocab[token] for token in tokens] #pass\n",
    "\n",
    "    if len(tokens) == 0:\n",
    "        raise UnknownWordsError\n",
    "\n",
    "    # Adding a batch dimension\n",
    "    text_input = torch.from_numpy(np.asarray(torch.LongTensor(tokens).view(-1, 1)))\n",
    "\n",
    "    # Get the NN output       \n",
    "    batch_size = 1\n",
    "    hidden = model.init_hidden(batch_size) #pass\n",
    "    \n",
    "    logps, _ = model(text_input, hidden) #pass\n",
    "    # Take the exponent of the NN output to get a range of 0 to 1 for each label.\n",
    "    pred = torch.round(logps.squeeze())#pass\n",
    "    pred = torch.exp(logps) \n",
    "    \n",
    "    return pred\n",
    "\n",
    "\n",
    "def predict_api(args):\n",
    "    \"\"\" \n",
    "    Make a prediction on a single sentence.\n",
    "    Parameters\n",
    "    ----------\n",
    "        args : 入力（Pythonディクショナリ）\n",
    "    Returns\n",
    "    -------\n",
    "        pred : 予測値（Python配列）\n",
    "    \"\"\"\n",
    "    text = args.get('text')\n",
    "    try:\n",
    "        result = predict_func(text, model_l, vocab_l)\n",
    "        return result.detach().numpy()[0]\n",
    "    except UnknownWordsError:\n",
    "        return [0,0,1,0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ポジティブなセンチメントを連想させる文章を入力として予測（適宜、文章を変更して実行してみることができます）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\"text\": \"I'm bullish on $goog\"}\n",
    "result = predict_api(args)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ネガティブなセンチメントを連想させる文章を入力として予測（適宜、文章を変更して実行してみることができます）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\"text\": \"I'm bearish on $goog\"}\n",
    "result = predict_api(args)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ボキャブラリ辞書に存在しない単語のみの文章を入力として予測（適宜、文章を変更して実行してみることができます）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\"text\": \"kono yoshiyuki\"}\n",
    "result = predict_api(args)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
