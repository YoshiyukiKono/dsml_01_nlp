{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science / Machine Learning Meetup #1 Deep Learning Hands-on\n",
    "# オルタナティブ・データと自然言語処理\n",
    "\n",
    "## はじめに\n",
    "\n",
    "演習の概略は以下の通りです。\n",
    "1. [前処理](#前処理)\n",
    "1. [モデル構築](#モデル構築)\n",
    "1. [予測](#予測)\n",
    "\n",
    "\n",
    "### 投資判断への自然言語処理応用の一事例としての感情分析\n",
    "投資判断のために、企業の価値を考慮する際のアプローチとして、従来の枠組みにとらわれない様々な情報（オルタナティブ・データ）を用いることを考えます。\n",
    "\n",
    "投資家の判断を左右し得る様々な情報を入力とし、投資判断のための定量的なシグナルに変換する予測モデルを構築します。\n",
    "入力となるデータには様々なものがあります。以下はその例です。\n",
    "\n",
    "- ニュース（製品のリコール、自然災害など）\n",
    "- \n",
    "\n",
    "Deep Learningによって、入力データの形式を問わず、予測モデルを構築することができます。\n",
    "\n",
    "ここでは、ソーシャルメディアサイトStockTwitsの投稿を使用します。StockTwitsのコミュニティは、投資家、トレーダー、起業家により利用されています。\n",
    "\n",
    "これらのtwitをトレーニング・データとして感情のスコアを生成するモデルを構築します。\n",
    "\n",
    "モデルの訓練のためには、入力に対応するラベルが必要になります。ラベルの精度は、モデルの訓練に当たって大変重要な要素です。\n",
    "\n",
    "センチメントの度合いを把握するために、非常にネガティブ、ネガティブ、ニュートラル、ポジティブ、非常にポジティブという5段階のスケールを使用します。それぞれ、-2から2までの数値に対応しています。\n",
    "\n",
    "このラベル付きデータによって訓練されたモデルを使用して、自然言語を入力として、その文章の背後にある感情を予測するモデルを構築します。\n",
    "\n",
    "構築された予測モデルは、投資判断の１要素として、銘柄に対するコミュニティのセンチメントを判定するために用いられます。\n",
    "\n",
    "ここでは、（主にラベルのついたデータの準備という課題をスキップするため）ソーシャルメディアの投稿を用いましたが、自然言語の解釈としては、四半期報告など、企業が発表する文章から、専門家の労力を用いずに、定量的な情報に変換するための利用などが考えられます。\n",
    "\n",
    "\n",
    "## 環境準備\n",
    "\n",
    "### パッケージのインストールとインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipython-sql==0.3.9\n",
      "  Downloading https://files.pythonhosted.org/packages/ab/df/427e7cf05ffc67e78672ad57dce2436c1e825129033effe6fcaf804d0c60/ipython_sql-0.3.9-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from ipython-sql==0.3.9) (1.12.0)\n",
      "Requirement already satisfied: ipython>=1.0 in /usr/local/lib/python3.6/dist-packages (from ipython-sql==0.3.9) (5.1.0)\n",
      "Requirement already satisfied: ipython-genutils>=0.1.0 in /usr/local/lib/python3.6/dist-packages (from ipython-sql==0.3.9) (0.2.0)\n",
      "Collecting prettytable (from ipython-sql==0.3.9)\n",
      "  Downloading https://files.pythonhosted.org/packages/ef/30/4b0746848746ed5941f052479e7c23d2b56d174b82f4fd34a25e389831f5/prettytable-0.7.2.tar.bz2\n",
      "Collecting sqlalchemy>=0.6.7 (from ipython-sql==0.3.9)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/47/35edeb0f86c0b44934c05d961c893e223ef27e79e1f53b5e6f14820ff553/SQLAlchemy-1.3.13.tar.gz (6.0MB)\n",
      "\u001b[K     |████████████████████████████████| 6.0MB 9.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sqlparse (from ipython-sql==0.3.9)\n",
      "  Downloading https://files.pythonhosted.org/packages/ef/53/900f7d2a54557c6a37886585a91336520e5539e3ae2423ff1102daf4f3a7/sqlparse-0.3.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=1.0->ipython-sql==0.3.9) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.3 in /usr/local/lib/python3.6/dist-packages (from ipython>=1.0->ipython-sql==0.3.9) (1.0.15)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython>=1.0->ipython-sql==0.3.9) (41.0.1)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython>=1.0->ipython-sql==0.3.9) (2.4.2)\n",
      "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython>=1.0->ipython-sql==0.3.9) (4.3.2)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=1.0->ipython-sql==0.3.9) (4.7.0)\n",
      "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=1.0->ipython-sql==0.3.9) (0.8.1)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython>=1.0->ipython-sql==0.3.9) (4.4.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.3->ipython>=1.0->ipython-sql==0.3.9) (0.1.7)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=1.0->ipython-sql==0.3.9) (0.6.0)\n",
      "Building wheels for collected packages: prettytable, sqlalchemy\n",
      "  Building wheel for prettytable (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/cdsw/.cache/pip/wheels/80/34/1c/3967380d9676d162cb59513bd9dc862d0584e045a162095606\n",
      "  Building wheel for sqlalchemy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/cdsw/.cache/pip/wheels/b3/35/98/4c9cb3fd63d21d5606b972dd70643769745adf60e622467b71\n",
      "Successfully built prettytable sqlalchemy\n",
      "Installing collected packages: prettytable, sqlalchemy, sqlparse, ipython-sql\n",
      "Successfully installed ipython-sql-0.3.9 prettytable-0.7.2 sqlalchemy-1.3.13 sqlparse-0.3.0\n",
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting PyHive==0.6.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4e/26/de91125c0d9e8947d48f387f4d1f2e7a22aa92a30771ad02f63a5653361b/PyHive-0.6.1.tar.gz (41kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 2.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting future (from PyHive==0.6.1)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
      "\u001b[K     |████████████████████████████████| 829kB 5.7MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from PyHive==0.6.1) (2.8.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil->PyHive==0.6.1) (1.12.0)\n",
      "Building wheels for collected packages: PyHive, future\n",
      "  Building wheel for PyHive (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/cdsw/.cache/pip/wheels/00/61/fb/77a0e77deb4c900276f689e62628a5ca7ba9df600f9ad7ba6a\n",
      "  Building wheel for future (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/cdsw/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
      "Successfully built PyHive future\n",
      "Installing collected packages: future, PyHive\n",
      "Successfully installed PyHive-0.6.1 future-0.18.2\n",
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: SQLAlchemy==1.3.13 in ./.local/lib/python3.6/site-packages (1.3.13)\n",
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting thrift==0.13.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/97/1e/3284d19d7be99305eda145b8aa46b0c33244e4a496ec66440dac19f8274d/thrift-0.13.0.tar.gz (59kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 2.7MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.7.2 in /usr/local/lib/python3.6/dist-packages (from thrift==0.13.0) (1.12.0)\n",
      "Building wheels for collected packages: thrift\n",
      "  Building wheel for thrift (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/cdsw/.cache/pip/wheels/02/a2/46/689ccfcf40155c23edc7cdbd9de488611c8fdf49ff34b1706e\n",
      "Successfully built thrift\n",
      "Installing collected packages: thrift\n",
      "Successfully installed thrift-0.13.0\n",
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting sasl==0.2.1\n",
      "  Downloading https://files.pythonhosted.org/packages/8e/2c/45dae93d666aea8492678499e0999269b4e55f1829b1e4de5b8204706ad9/sasl-0.2.1.tar.gz\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sasl==0.2.1) (1.12.0)\n",
      "Building wheels for collected packages: sasl\n",
      "  Building wheel for sasl (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/cdsw/.cache/pip/wheels/56/20/21/ff481fd0f4ae09d5d94c76d089f550204580b1703e44f27dd5\n",
      "Successfully built sasl\n",
      "Installing collected packages: sasl\n",
      "Successfully installed sasl-0.2.1\n",
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting thrift_sasl==0.3.0\n",
      "  Downloading https://files.pythonhosted.org/packages/50/fe/89cbc910809e3757c762f56ee190ca39e0f28b7ea451835232c0c988d706/thrift_sasl-0.3.0.tar.gz\n",
      "Requirement already satisfied: thrift>=0.10.0 in ./.local/lib/python3.6/site-packages (from thrift_sasl==0.3.0) (0.13.0)\n",
      "Requirement already satisfied: sasl>=0.2.1 in ./.local/lib/python3.6/site-packages (from thrift_sasl==0.3.0) (0.2.1)\n",
      "Requirement already satisfied: six>=1.7.2 in /usr/local/lib/python3.6/dist-packages (from thrift>=0.10.0->thrift_sasl==0.3.0) (1.12.0)\n",
      "Building wheels for collected packages: thrift-sasl\n",
      "  Building wheel for thrift-sasl (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/cdsw/.cache/pip/wheels/c8/3a/34/1d82df3d652788fc211c245d51dde857a58e603695ea41d93d\n",
      "Successfully built thrift-sasl\n",
      "Installing collected packages: thrift-sasl\n",
      "Successfully installed thrift-sasl-0.3.0\n",
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting nltk==3.4.5\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5MB 4.0MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk==3.4.5) (1.12.0)\n",
      "Building wheels for collected packages: nltk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/cdsw/.cache/pip/wheels/96/86/f6/68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n",
      "Successfully built nltk\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.4.5\n",
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting torch==1.4.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/19/4804aea17cd136f1705a5e98a00618cb8f6ccc375ad8bfa437408e09d058/torch-1.4.0-cp36-cp36m-manylinux1_x86_64.whl (753.4MB)\n",
      "\u001b[K     |████████████████████████████████| 753.4MB 41kB/s s eta 0:00:01     |█▌                              | 35.2MB 5.4MB/s eta 0:02:13[K     |█▊                              | 39.4MB 5.4MB/s eta 0:02:12     |██                              | 47.3MB 5.4MB/s eta 0:02:11MB/s eta 0:00:097MB 69.9MB/s eta 0:00:09| 191.8MB 69.9MB/s eta 0:00:091MB 56.1MB/s eta 0:00:10  | 276.2MB 72.9MB/s eta 0:00:07MB/s eta 0:00:05███████████████▋          | 508.0MB 69.2MB/s eta 0:00:04:00:03��        | 545.9MB 71.9MB/s eta 0:00:03█████████████▊        | 559.8MB 71.9MB/s eta 0:00:03MB/s eta 0:00:02MB/s eta 0:00:02��██████████████▍   | 669.5MB 69.7MB/s eta 0:00:02ta 0:00:01ta 0:00:01�███| 751.5MB 69.5MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: torch\n",
      "Successfully installed torch-1.4.0\n",
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!pip3 install ipython-sql==0.3.9\n",
    "#!pip3 install PyHive==0.6.1\n",
    "#!pip3 install SQLAlchemy==1.3.13\n",
    "#!pip3 install thrift==0.13.0\n",
    "#!pip3 install sasl==0.2.1\n",
    "#!pip3 install thrift_sasl==0.3.0\n",
    "\n",
    "\n",
    "!pip3 install nltk==3.4.5\n",
    "!pip3 install torch==1.4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記でインストールしたPyHiveは、Pythonコードの中でimportして使われるのではなく、Hiveへの接続の際の接続文字列：`sqlalchemy.create_engine('hive://<host>:<port>')`の中でdialectsとして指定された際に必要になります。そのため、インストール後に利用するためには、新しくプロセスを始める必要があります。**インストールした後に一度、KernelをRestartしてください。**インストールしたプロセスでは、接続時に下記のようなエラーが発生します。\n",
    "`NoSuchModuleError: Can't load plugin: sqlalchemy.dialects:hive`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-cd7a09c89091>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import subprocess\n",
    "import glob\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "\n",
    "#from pyhive import hive\n",
    "#import sqlalchemy\n",
    "\n",
    "import sys\n",
    "#from random import random\n",
    "from operator import add\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import torch\n",
    "import nltk\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ファイルの確認\n",
    "\n",
    "本ワークショップでは、事前に準備してあるトレーニング・データを用いて、感情分析のためのモデル構築を実施します。\n",
    "\n",
    "クラスターのtmpディレクトリにあるファイルを自分のプロジェクト環境にコピーします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export HADOOP_CONF_DIR=/etc/hadoop/conf; hdfs dfs -get /tmp/output.json ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 cdsw cdsw 96174003 Feb  7 05:39 output.json\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l | grep output.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"data\":[{\"message_body\":\"$AAPL had approximately 2395M USD go to the short side at 52 pct short  Bears and Bulls are fighting close  https://www.algowins.com/?wdt_column_filter[1]=AAPL\",\"sentiment\":0},\n",
      "{\"message_body\":\"$AAPL nice squeeze on the daily. If jobs report goes well 330 tomorrow\",\"sentiment\":2},\n",
      "{\"message_body\":\"$AAPL nice squeeze on the daily. If jobs report goes well 330 tomorrow\",\"sentiment\":2},\n",
      "{\"message_body\":\"$AAPL nice squeeze on the daily. If jobs report goes well 330 tomorrow\",\"sentiment\":2},\n",
      "{\"message_body\":\"$AAPL nice squeeze on the daily. If jobs report goes well 330 tomorrow\",\"sentiment\":2},\n",
      "{\"message_body\":\"$AAPL nice squeeze on the daily. If jobs report goes well 330 tomorrow\",\"sentiment\":2},\n",
      "{\"message_body\":\"$AAPL nice squeeze on the daily. If jobs report goes well 330 tomorrow\",\"sentiment\":2},\n",
      "{\"message_body\":\"$AAPL nice squeeze on the daily. If jobs report goes well 330 tomorrow\",\"sentiment\":2},\n",
      "{\"message_body\":\"$AAPL nice squeeze on the daily. If jobs report goes well 330 tomorrow\",\"sentiment\":2},\n",
      "{\"message_body\":\"$AAPL nice squeeze on the daily. If jobs report goes well 330 tomorrow\",\"sentiment\":2},\n",
      "{\"message_body\":\"$YUMA watching. On the verge of a breakout.\",\"sentiment\":2},\n",
      "{\"message_body\":\"$YUMA watching. On the verge of a breakout.\",\"sentiment\":2},\n",
      "{\"message_body\":\"$YUMA watching. On the verge of a breakout.\",\"sentiment\":2},\n",
      "{\"message_body\":\"$YUMA watching. On the verge of a breakout.\",\"sentiment\":2},\n",
      "{\"message_body\":\"$YUMA watching. On the verge of a breakout.\",\"sentiment\":2},\n",
      "{\"message_body\":\"$YUMA watching. On the verge of a breakout.\",\"sentiment\":2},\n",
      "{\"message_body\":\"$YUMA watching. On the verge of a breakout.\",\"sentiment\":2},\n",
      "{\"message_body\":\"$YUMA watching. On the verge of a breakout.\",\"sentiment\":2},\n",
      "{\"message_body\":\"$YUMA watching. On the verge of a breakout.\",\"sentiment\":2},\n",
      "{\"message_body\":\"$YUMA watching. On the verge of a breakout.\",\"sentiment\":2}]}"
     ]
    }
   ],
   "source": [
    "!head output.json\n",
    "!tail output.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前処理\n",
    "\n",
    "\n",
    "### データの確認\n",
    "データがどのように見えるかを確認します。\n",
    "\n",
    "各フィールドの意味:\n",
    "\n",
    "* `'message_body'`: メッセージ本文テキスト\n",
    "* `'sentiment'`: センチメントスコア。-2から2までの５段階。0は中立。\n",
    "\n",
    "下記のような内容になっているはずです。\n",
    "```\n",
    "{'data':\n",
    "  {'message_body': '............................',\n",
    "   'sentiment': 2},\n",
    "  {'message_body': '............................',\n",
    "   'sentiment': -2},\n",
    "   ...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データを読み込みます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'message_body': '$AAPL had approximately 2395M USD go to the short side at 52 pct short  Bears and Bulls are fighting close  https://www.algowins.com/?wdt_column_filter[1]=AAPL', 'sentiment': 0}, {'message_body': '$AAPL nice squeeze on the daily. If jobs report goes well 330 tomorrow', 'sentiment': 2}, {'message_body': '$AAPL nice squeeze on the daily. If jobs report goes well 330 tomorrow', 'sentiment': 2}, {'message_body': '$AAPL nice squeeze on the daily. If jobs report goes well 330 tomorrow', 'sentiment': 2}, {'message_body': '$AAPL nice squeeze on the daily. If jobs report goes well 330 tomorrow', 'sentiment': 2}, {'message_body': '$AAPL nice squeeze on the daily. If jobs report goes well 330 tomorrow', 'sentiment': 2}, {'message_body': '$AAPL nice squeeze on the daily. If jobs report goes well 330 tomorrow', 'sentiment': 2}, {'message_body': '$AAPL nice squeeze on the daily. If jobs report goes well 330 tomorrow', 'sentiment': 2}, {'message_body': '$AAPL nice squeeze on the daily. If jobs report goes well 330 tomorrow', 'sentiment': 2}, {'message_body': '$AAPL nice squeeze on the daily. If jobs report goes well 330 tomorrow', 'sentiment': 2}]\n"
     ]
    }
   ],
   "source": [
    "with open('./output.json', 'r') as f:\n",
    "    twits = json.load(f)\n",
    "\n",
    "print(twits['data'][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データ件数の確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "593901\n"
     ]
    }
   ],
   "source": [
    "print(len(twits['data']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### メッセージ本文とセンチメント・ラベルのリスト化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [twit['message_body'] for twit in twits['data']]\n",
    "# Since the sentiment scores are discrete, we'll scale the sentiments to 0 to 4 for use in our network\n",
    "sentiments = [twit['sentiment'] + 2 for twit in twits['data']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データの前処理\n",
    "\n",
    "テキストを前処理します。\n",
    "\n",
    "本文に含まれるティッカーシンボル（「$シンボル」で示される）はセンチメントに関する情報を提供しないため削除します。\n",
    "また、「@ユーザー名」で、ユーザに関する情報が記載されていますが、これもまたセンチメント情報を提供しないため、削除します。\n",
    "URLも削除します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### プリプロセス関数の定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/cdsw/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "\n",
    "def preprocess(message):\n",
    "    \"\"\"\n",
    "    入力として文字列を受け取り、次の操作を実行する: \n",
    "        - 全てのアルファベットを小文字に変換\n",
    "        - URLを削除\n",
    "        - ティッカーシンボルを削除 \n",
    "        - 句読点を削除\n",
    "        - 文字列をスペースで分割しトークン化する\n",
    "        - シングル・キャラクターのトークンを削除\n",
    "    \n",
    "    パラメータ\n",
    "    ----------\n",
    "        message : 前処理の対象テキストメッセージ\n",
    "        \n",
    "    戻り値\n",
    "    -------\n",
    "        tokens: 前処理後のトークン配列\n",
    "    \"\"\" \n",
    "    #TODO: Implement \n",
    "    \n",
    "    # Lowercase the twit message\n",
    "    text = message.lower()\n",
    "    \n",
    "    # Replace URLs with a space in the message\n",
    "    text = re.sub(\"http(s)?://([\\w\\-]+\\.)+[\\w-]+(/[\\w\\- ./?%&=]*)?\",' ', text)\n",
    "    \n",
    "    # Replace ticker symbols with a space. The ticker symbols are any stock symbol that starts with $.\n",
    "    text = re.sub(\"\\$[^ \\t\\n\\r\\f]+\", ' ', text)\n",
    "    \n",
    "    # Replace StockTwits usernames with a space. The usernames are any word that starts with @.\n",
    "    text = re.sub(\"@[^ \\t\\n\\r\\f]+\", ' ', text)\n",
    "\n",
    "    # Replace everything not a letter with a space\n",
    "    text = re.sub(\"[^a-z]\", ' ', text)\n",
    "    \n",
    "    \n",
    "    # Tokenize by splitting the string on whitespace into a list of words\n",
    "    tokens = text.split()\n",
    "\n",
    "    # Lemmatize words using the WordNetLemmatizer. You can ignore any word that is not longer than one character.\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    tokens = [wnl.lemmatize(w, pos='v') for w in tokens if len(w) > 1]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitsメッセージ前処理\n",
    "上記で定義した`preprocess`関数を全てStockTwitメッセージ・データに適用します。\n",
    "\n",
    "※この処理には、データのサイズに応じて多少時間がかかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['have', 'approximately', 'usd', 'go', 'to', 'the', 'short', 'side', 'at', 'pct', 'short', 'bear', 'and', 'bull', 'be', 'fight', 'close', 'aapl'], ['nice', 'squeeze', 'on', 'the', 'daily', 'if', 'job', 'report', 'go', 'well', 'tomorrow'], ['nice', 'squeeze', 'on', 'the', 'daily', 'if', 'job', 'report', 'go', 'well', 'tomorrow']]\n",
      "593901\n"
     ]
    }
   ],
   "source": [
    "tokenized = list(map(preprocess, messages))\n",
    "\n",
    "print(tokenized[:3])\n",
    "print(len(tokenized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words\n",
    "\n",
    "すべてのメッセージがトークン化されたので、ボキャブラリ（語彙）データを作成します。\n",
    "その際に、コーパス全体で各単語が出現する頻度をカウントします\n",
    "（[`Counter`](https://docs.python.org/3.1/library/collections.html#collections.Counter)関数を利用）。\n",
    "\n",
    "※この処理には、データのサイズに応じて多少時間がかかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['have', 'approximately', 'usd', 'go', 'to', 'the', 'short', 'side', 'at', 'pct', 'short', 'bear', 'and']\n",
      "7600018\n",
      "593901\n",
      "[[16, 115, 111, 35, 2, 0, 19, 98, 23, 116, 19, 101, 7, 144, 3, 240, 86, 2310], [216, 485, 9, 0, 159, 120, 751, 12, 35, 227, 188], [216, 485, 9, 0, 159, 120, 751, 12, 35, 227, 188]]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "#words = []\n",
    "#for tokens in tokenized:\n",
    "#    for token in tokens:\n",
    "#        words.append(token)\n",
    "out_list = tokenized\n",
    "words = [element for in_list in out_list for element in in_list]\n",
    "\n",
    "print(words[:13])\n",
    "print(len(words))\n",
    "\n",
    "\"\"\"\n",
    "Create a vocabulary by using Bag of words\n",
    "\"\"\"\n",
    "\n",
    "word_counts = Counter(words)\n",
    "sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "int_to_vocab = {ii: word for ii, word in enumerate(sorted_vocab)}\n",
    "vocab_to_int = {word:ii for ii, word in int_to_vocab.items()}\n",
    "\n",
    "bow = []\n",
    "for tokens in tokenized:\n",
    "    bow.append([vocab_to_int[token] for token in tokens])\n",
    "\n",
    "print(len(bow))\n",
    "print(bow[:3])\n",
    "\n",
    "# This BOW will not be used because it is not filtered to eliminate common words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 単語の重要性（メッセージに現れる頻度）に応じた調整\n",
    "\n",
    "ボキャブラリーを使用して、「the」、「and」、「it」などの最も一般的な単語の一部を削除します。\n",
    "これらの単語は非常に一般的であるため、センチメントを特定する目的に寄与せず、ニューラルネットワークへの入力のノイズとなります。これらを除外することで、ネットワークの学習時間を短縮することができます。\n",
    "\n",
    "また、非常に稀にしか用いられない単語も削除します。\n",
    "ここでは、各単語のカウントをメッセージの数で除算する必要があります。\n",
    "\n",
    "次に、メッセージのごく一部にしか表示されない単語を削除します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(sorted_vocab): 5664\n",
      "sorted_vocab - top: ['the', 'of', 'to']\n",
      "sorted_vocab - least: ['php', 'ref', 'nqjktv', 'png', 'developments', 'sayin', 'geologist', 'specialization', 'fossil', 'deposit', 'convince', 'merit', 'laughable', 'terrify', 'verge']\n",
      "freqs[the]: 0.02588164922767288\n",
      "high_cutoff: 20\n",
      "low_cutoff: 2e-06\n",
      "K_most_common: ['the', 'of', 'to', 'be', 'amp', 'utm', 'file', 'and', 'in', 'on', 'form', 'for', 'report', 'share', 'sec', 'medium', 'have', 'by', 'campaign', 'short']\n",
      "len(filtered_words): 5644\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Set the following variables:\n",
    "    freqs\n",
    "    low_cutoff\n",
    "    high_cutoff\n",
    "    K_most_common\n",
    "\"\"\"\n",
    "\n",
    "print(\"len(sorted_vocab):\",len(sorted_vocab))\n",
    "print(\"sorted_vocab - top:\", sorted_vocab[:3])\n",
    "print(\"sorted_vocab - least:\", sorted_vocab[-15:])\n",
    "\n",
    "# Dictionart that contains the Frequency of words appearing in messages.\n",
    "# The key is the token and the value is the frequency of that word in the corpus.\n",
    "total_count = len(words)\n",
    "freqs = {word: count/total_count for word, count in word_counts.items()}\n",
    "\n",
    "#print(\"freqs[supplication]:\",freqs[\"supplication\"] )\n",
    "print(\"freqs[the]:\",freqs[\"the\"] )\n",
    "\n",
    "\"\"\"\n",
    "This was the post by Ricardo:\n",
    "\n",
    "there's no exact value for low_cutoff and high_cutoff, \n",
    "however I'd recommend you to use \n",
    "a low_cutoff that's around 0.000002 and 0.000007 \n",
    "(This depends on the values you get from your freqs calculations) and \n",
    "a high_cutofffrom 5 to 20 (this depends on the most_common values from the bow).\n",
    "\"\"\"\n",
    "\n",
    "# Float that is the frequency cutoff. Drop words with a frequency that is lower or equal to this number.\n",
    "low_cutoff = 0.000002\n",
    "\n",
    "# Integer that is the cut off for most common words. Drop words that are the `high_cutoff` most common words.\n",
    "\"\"\"\n",
    "example_count = []\n",
    "example_count.append(sorted_vocab.index(\"the\"))\n",
    "example_count.append(sorted_vocab.index(\"for\"))\n",
    "example_count.append(sorted_vocab.index(\"of\"))\n",
    "print(example_count)\n",
    "high_cutoff = min(example_count)\n",
    "\"\"\"\n",
    "high_cutoff = 20\n",
    "print(\"high_cutoff:\",high_cutoff)\n",
    "print(\"low_cutoff:\",low_cutoff)\n",
    "\n",
    "# The k most common words in the corpus. Use `high_cutoff` as the k.\n",
    "#K_most_common = [word for word in sorted_vocab[:high_cutoff]]\n",
    "K_most_common = sorted_vocab[:high_cutoff]\n",
    "\n",
    "print(\"K_most_common:\",K_most_common)\n",
    "\n",
    "\n",
    "filtered_words = [word for word in freqs if (freqs[word] > low_cutoff and word not in K_most_common)]\n",
    "\n",
    "print(\"len(filtered_words):\",len(filtered_words)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### フィルターされた単語を削除して語彙を更新\n",
    "ボキャブラリーに役立つ3つの変数を作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(tokenized): 593901\n",
      "len(filtered): 593901\n",
      "tokenized[:1] [['have', 'approximately', 'usd', 'go', 'to', 'the', 'short', 'side', 'at', 'pct', 'short', 'bear', 'and', 'bull', 'be', 'fight', 'close', 'aapl']]\n",
      "filtered[:1] [['approximately', 'usd', 'go', 'side', 'at', 'pct', 'bear', 'bull', 'fight', 'close', 'aapl']]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Set the following variables:\n",
    "    vocab\n",
    "    id2vocab\n",
    "    filtered\n",
    "\"\"\"\n",
    "\n",
    "# A dictionary for the `filtered_words`. The key is the word and value is an id that represents the word. \n",
    "vocab =  {word:ii for ii, word in enumerate(filtered_words)}\n",
    "# Reverse of the `vocab` dictionary. The key is word id and value is the word. \n",
    "id2vocab = {ii:word for word, ii in vocab.items()}\n",
    "# tokenized with the words not in `filtered_words` removed.\n",
    "\n",
    "print(\"len(tokenized):\", len(tokenized))\n",
    "\n",
    "filtered = [[token for token in tokens if token in vocab] for tokens in tokenized]\n",
    "print(\"len(filtered):\", len(filtered))\n",
    "print(\"tokenized[:1]\", tokenized[:1])\n",
    "print(\"filtered[:1]\",filtered[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分類クラス間のバランス\n",
    "\n",
    "訓練データのラベルには、一般に偏りがあることがよく見受けられます（例外的なデータは少ない）。\n",
    "例えば、データの50％がニュートラルであること場合、毎回0（ニュートラル）を予測するだけで、ネットワークの精度が50％になることを意味します。\n",
    "\n",
    "ネットワークが適切に学習できるように、クラスのバランスを取る必要があります。つまり、それぞれのセンチメントスコアがデータにほぼ同じ頻度で表含まれていることが望ましいと言えます。\n",
    "\n",
    "ここでは、中立的な感情を持つデータを全体の20%になるように、ランダムにドロップします。\n",
    "\n",
    "データに含まれるニュートラルデータのパーセンテージと、データ削除により期待されるパーセンテージの値を使って、\n",
    "データをドロップする確率を求めます。\n",
    "\n",
    "同時に、長さが0のメッセージを削除します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep prob: 0.0504957488448718\n"
     ]
    }
   ],
   "source": [
    "balanced = {'messages': [], 'sentiments':[]}\n",
    "\n",
    "n_neutral = sum(1 for each in sentiments if each == 2)\n",
    "N_examples = len(sentiments)\n",
    "\n",
    "keep_prob = (N_examples - n_neutral)/4/n_neutral\n",
    "\n",
    "print(\"keep prob:\", keep_prob)\n",
    "\n",
    "for idx, sentiment in enumerate(sentiments):\n",
    "    message = filtered[idx]\n",
    "    if len(message) == 0:\n",
    "        # skip this message because it has length zero\n",
    "        continue\n",
    "    elif sentiment != 2 or random.random() < keep_prob:\n",
    "        balanced['messages'].append(message)\n",
    "        balanced['sentiments'].append(sentiment) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "バランスされたデータ中、センチメントが「ニュートラル」であるデータの割合を確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21339761481857397"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_neutral = sum(1 for each in balanced['sentiments'] if each == 2)\n",
    "N_examples = len(balanced['sentiments'])\n",
    "n_neutral/N_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's convert our tokens into integer ids which we can pass to the network.\n",
    "\n",
    "メッセージをID（数値）に変換します。この処理は、ニューラルネットワークの入力として用いるために必要です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = [[vocab[word] for word in message] for message in balanced['messages']]\n",
    "sentiments = balanced['sentiments']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ボキャブラリ・ファイルを保存します。このファイルは、予測の際に、入力を変換するために必要になります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('vocab.pickle', 'wb') as f:\n",
    "    pickle.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの構築\n",
    "\n",
    "これでボキャブラリーができたので、トークンをIDに変換し、それをネットワークに渡すことができます。ネットワークを定義します\n",
    "\n",
    "下記は、ネットワークの概要です：\n",
    "\n",
    "#### Embed -> RNN -> Dense -> Softmax\n",
    "\n",
    "### SentimentClassifier (感情分類器)実装\n",
    "\n",
    "クラスは、3つの主要な部分で構成されています：: \n",
    "\n",
    "1. init function `__init__` \n",
    "2. forward pass `forward`  \n",
    "3. hidden state `init_hidden`. \n",
    "\n",
    "出力層では、softmaxを使用します。出力フォーマットによって出力層を選択します。\n",
    "\n",
    "（例えば、出力が２値/バイナリであれば、シグモイド関数）\n",
    "\n",
    "このネットワークでは、センチメントスコアには5つのクラスがあるためsoftmaxが適しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, lstm_size, output_size, lstm_layers=1, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "            vocab_size : The vocabulary size.\n",
    "            embed_size : The embedding layer size.\n",
    "            lstm_size : The LSTM layer size.\n",
    "            output_size : The output size.\n",
    "            lstm_layers : The number of LSTM layers.\n",
    "            dropout : The dropout probability.\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.lstm_size = lstm_size\n",
    "        self.output_size = output_size\n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embed_size)\n",
    "        self.lstm = nn.LSTM(self.embed_size, self.lstm_size, self.lstm_layers, dropout=self.dropout)\n",
    "        \n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(lstm_size, output_size)\n",
    "        \n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\" \n",
    "        Initializes hidden state\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "            batch_size : The size of batches.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "            hidden_state\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # 隠れ層として、n_layers x batch_size x hidden_dimの構造を持つテンソルを二つ作成し、ゼロで初期化\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        \n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        hidden = (weight.new(self.lstm_layers, batch_size,self.lstm_size).zero_(),\n",
    "                         weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_())\n",
    "        return hidden\n",
    "\n",
    "\n",
    "    def forward(self, nn_input, hidden_state):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on nn_input.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "            nn_input : The batch of input to the NN.\n",
    "            hidden_state : The LSTM hidden state.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            logps: log softmax output\n",
    "            hidden_state: The new hidden state.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = nn_input.size(0)\n",
    "        \n",
    "        # embed\n",
    "        embeds = self.embedding(nn_input)\n",
    "        \n",
    "        # LSTM\n",
    "        lstm_out, hidden_state = self.lstm(embeds, hidden_state)\n",
    "        \n",
    "        \"\"\"\n",
    "        remember here you do not have batch_first=True, \n",
    "        so accordingly shape your input. \n",
    "        Moreover, since now input is seq_length x batch you just need to transform lstm_out = lstm_out[-1,:,:].\n",
    "        you don't have to use batch_first=True in this case, \n",
    "        nor reshape the outputs with .view just transform your lstm_out as advised and you should be good to go.\n",
    "        \"\"\"\n",
    "        #lstm_out = lstm_out.contiguous().view(-1, self.lstm_size)    \n",
    "        lstm_out = lstm_out[-1,:,:]\n",
    "        \n",
    "        # dropout\n",
    "        out = self.dropout(lstm_out)\n",
    "        \n",
    "        # Dense Layer (nn.Linear) RNNの隠れ層から値を予測\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        # Softmax関数\n",
    "        logps = self.softmax(out)\n",
    "        \n",
    "        \n",
    "        return logps, hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルの確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.4577, -1.7675, -1.4326, -1.6824, -1.7611],\n",
      "        [-1.4568, -1.7760, -1.4235, -1.6869, -1.7618],\n",
      "        [-1.4543, -1.7657, -1.4373, -1.6784, -1.7653],\n",
      "        [-1.4654, -1.7506, -1.4481, -1.6645, -1.7659]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "model = SentimentClassifier(len(vocab), 10, 6, 5, dropout=0.1, lstm_layers=2)\n",
    "model.embedding.weight.data.uniform_(-1, 1)\n",
    "input = torch.randint(0, 1000, (5, 4), dtype=torch.int64)\n",
    "batch_size = 4\n",
    "hidden = model.init_hidden(4)\n",
    "\n",
    "logps, _ = model.forward(input, hidden)\n",
    "print(logps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoaderとバッチ処理\n",
    "ここで、データをループするために使用できるジェネレーターを構築します。\n",
    "\n",
    "効率化のため、シーケンスをバッチとして渡します。\n",
    "\n",
    "入力テンソルは次のような形になります：(sequence_length, batch_size)\n",
    "\n",
    "したがって、シーケンスが40トークンで、25シーケンスを渡す場合、入力サイズは(40, 25)になります。\n",
    "\n",
    "シーケンスの長さを40に設定した場合、40トークンより多いまたは少ないメッセージは、以下のように処理します。\n",
    "- 40トークン未満のメッセージの場合、空のスポットにゼロを埋め込む。\n",
    "   - データを処理する前にRNNが何も開始しないように、必ずパッドを残しておく必要がある。\n",
    "   - メッセージに20個のトークンがある場合、最初の20個のスポットは0になる。\n",
    "- メッセージに40個を超えるトークンがある場合、最初の40個のトークンを保持。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def dataloader(messages, labels, sequence_length=30, batch_size=32, shuffle=False):\n",
    "def dataloader(messages, labels, sequence_length=20, batch_size=32, shuffle=False):\n",
    "    \"\"\" \n",
    "    Build a dataloader.\n",
    "    \"\"\"\n",
    "    if shuffle:\n",
    "        indices = list(range(len(messages)))\n",
    "        random.shuffle(indices)\n",
    "        messages = [messages[idx] for idx in indices]\n",
    "        labels = [labels[idx] for idx in indices]\n",
    "\n",
    "    total_sequences = len(messages)\n",
    "\n",
    "    for ii in range(0, total_sequences, batch_size):\n",
    "        batch_messages = messages[ii: ii+batch_size]\n",
    "        \n",
    "        # First initialize a tensor of all zeros\n",
    "        batch = torch.zeros((sequence_length, len(batch_messages)), dtype=torch.int64)\n",
    "        for batch_num, tokens in enumerate(batch_messages):\n",
    "            token_tensor = torch.tensor(tokens)\n",
    "            # Left pad!\n",
    "            start_idx = max(sequence_length - len(token_tensor), 0)\n",
    "            batch[start_idx:, batch_num] = token_tensor[:sequence_length]\n",
    "        \n",
    "        label_tensor = torch.tensor(labels[ii: ii+len(batch_messages)])\n",
    "        \n",
    "        yield batch, label_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  データの分割（訓練用と検証用）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Split data into training and validation datasets. Use an appropriate split size.\n",
    "The features are the `token_ids` and the labels are the `sentiments`.\n",
    "\"\"\"   \n",
    "\n",
    "split_frac = 0.98 # for small data\n",
    "#split_frac = 0.8 # for big data\n",
    "\n",
    "## split data into training, validation, and test data (features and labels, x and y)\n",
    "\n",
    "split_idx = int(len(token_ids)*split_frac)\n",
    "train_features, remaining_features = token_ids[:split_idx], token_ids[split_idx:]\n",
    "train_labels, remaining_labels = sentiments[:split_idx], sentiments[split_idx:]\n",
    "\n",
    "test_idx = int(len(remaining_features)*0.5)\n",
    "valid_features, test_features = remaining_features[:test_idx], remaining_features[test_idx:]\n",
    "valid_labels, test_labels = remaining_labels[:test_idx], remaining_labels[test_idx:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### トレーニング\n",
    "\n",
    "#### トレーニング準備\n",
    "\n",
    "利用可能なデバイス(CUDA/GPUまたはGPU)を確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentClassifier(\n",
       "  (embedding): Embedding(5645, 1024)\n",
       "  (lstm): LSTM(1024, 512, num_layers=2, dropout=0.2)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (fc): Linear(in_features=512, out_features=5, bias=True)\n",
       "  (softmax): LogSoftmax()\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model = SentimentClassifier(len(vocab)+1, 200, 128, 5, dropout=0.)\n",
    "model = SentimentClassifier(len(vocab)+1, 1024, 512, 5, lstm_layers=2, dropout=0.2)\n",
    "model.embedding.weight.data.uniform_(-1, 1)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### トレーニング実施\n",
    "\n",
    "トレーニングを実行します。モデルの訓練の進行度合を確認するために、定期的にLossを出力します。\n",
    "\n",
    "※この処理には、データのサイズに応じて、十分な時間が必要です。\n",
    "\n",
    "GPUを備えた環境で実行する場合、ターミナルで以下のコマンドを実行することで、GPUが利用されていることを確認することができます（ GPU実行中、コマンド実行により表示されるテーブルの右上のVolatile GPU-Utilのパーセンテージ値が増えます）\n",
    "```\n",
    "$ watch nvidia-smi\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "Epoch: 1/5... Step: 100... Loss: 0.040382... Total Loss: 0.219460\n",
      "Epoch: 1/5... Step: 200... Loss: 0.000827... Total Loss: 0.118474\n",
      "Starting epoch 2\n",
      "Epoch: 2/5... Step: 100... Loss: 0.002647... Total Loss: 0.076874\n",
      "Epoch: 2/5... Step: 200... Loss: 0.001187... Total Loss: 0.059721\n",
      "Starting epoch 3\n",
      "Epoch: 3/5... Step: 100... Loss: 0.000218... Total Loss: 0.047139\n",
      "Epoch: 3/5... Step: 200... Loss: 0.000292... Total Loss: 0.040001\n",
      "Starting epoch 4\n",
      "Epoch: 4/5... Step: 100... Loss: 0.000775... Total Loss: 0.034031\n",
      "Epoch: 4/5... Step: 200... Loss: 0.000757... Total Loss: 0.030247\n",
      "Starting epoch 5\n",
      "Epoch: 5/5... Step: 100... Loss: 0.000326... Total Loss: 0.026750\n",
      "Epoch: 5/5... Step: 200... Loss: 0.001380... Total Loss: 0.024398\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "epochs = 5\n",
    "batch_size =  64\n",
    "batch_size =  512\n",
    "learning_rate = 0.001\n",
    "\n",
    "print_every = 100\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "model.train()\n",
    "\n",
    "#val_losses = []\n",
    "total_losses = []\n",
    "#accuracy = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Starting epoch {}'.format(epoch + 1))\n",
    "    \n",
    "    steps = 0\n",
    "    for text_batch, labels in dataloader(\n",
    "            train_features, train_labels, batch_size=batch_size, sequence_length=20, shuffle=True):\n",
    "        steps += 1\n",
    "        hidden = model.init_hidden(labels.shape[0]) \n",
    "        \n",
    "        # デバイス(CPU, GPU)の設定\n",
    "        text_batch, labels = text_batch.to(device), labels.to(device)\n",
    "        for each in hidden:\n",
    "            each.to(device)\n",
    "        \n",
    "        # モデルのトレーニング\n",
    "        hidden = tuple([each.data for each in hidden])\n",
    "        model.zero_grad()\n",
    "        output, hidden = model(text_batch, hidden)\n",
    "        loss = criterion(output.squeeze(), labels)\n",
    "        loss.backward()\n",
    "        clip = 5\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate loss\n",
    "        #val_losses.append(loss.item())\n",
    "        total_losses.append(loss.item())\n",
    "        \n",
    "        correct_count = 0.0\n",
    "        if steps % print_every == 0:\n",
    "            model.eval()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            ps = torch.exp(output)\n",
    "            top_p, top_class = ps.topk(1, dim=1)\n",
    "            #?top_class = top_class.to(device)\n",
    "            #?labels = labels.to(device)\n",
    "\n",
    "            correct_count += torch.sum(top_class.squeeze()== labels)\n",
    "            #accuracy.append(100*correct_count/len(labels))\n",
    "            \n",
    "            # Print metrics\n",
    "            print(\"Epoch: {}/{}...\".format(epoch+1, epochs),\n",
    "                 \"Step: {}...\".format(steps),\n",
    "                 \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                 \"Total Loss: {:.6f}\".format(np.mean(total_losses)),\n",
    "                 #\"Collect Count: {}\".format(correct_count),\n",
    "                 #\"Accuracy: {:.2f}\".format((100*correct_count/len(labels))),\n",
    "                 # AttributeError: 'torch.dtype' object has no attribute 'type'\n",
    "                 #\"Accuracy Avg: {:.2f}\".format(np.mean(accuracy))\n",
    "                 )\n",
    "            \n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'state_dict': model.state_dict()}, 'checkpoint.pth.tar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 予測\n",
    "\n",
    "### Prediction関数の作成\n",
    "\n",
    "訓練されたモデルを使って、入力されたテキストから予測結果を生成するpredict関数を実装します。\n",
    "\n",
    "テキストは、ネットワークに渡される前に前処理される必要があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/cdsw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/cdsw/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import pickle\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "cur_dir = os.path.dirname(os.path.abspath('__file__'))\n",
    "print(cur_dir)\n",
    "sys.path.append(cur_dir)\n",
    "\n",
    "vocab_filename = 'vocab.pickle'\n",
    "vocab_path = cur_dir + \"/\" + vocab_filename\n",
    "vocab_l = pickle.load(open(vocab_path, 'rb'))\n",
    "\n",
    "#model_path = cur_dir + \"/\" + \"model.torch\"\n",
    "#model_l = torch.load(model_path, map_location='cpu')\n",
    "\n",
    "model_l = SentimentClassifier(len(vocab_l)+1, 1024, 512, 5, lstm_layers=2, dropout=0.2)\n",
    "checkpoint = torch.load('./checkpoint.pth.tar')\n",
    "model_l.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "class UnknownWordsError(Exception):\n",
    "  \"Only unknown words are included in text\"\n",
    "\n",
    "\n",
    "def predict_func(text, model, vocab):\n",
    "    \"\"\" \n",
    "    Make a prediction on a single sentence.\n",
    "    Parameters\n",
    "    ----------\n",
    "        text : The string to make a prediction on.\n",
    "        model : The model to use for making the prediction.\n",
    "        vocab : Dictionary for word to word ids. The key is the word and the value is the word id.\n",
    "    Returns\n",
    "    -------\n",
    "        pred : 予測値（numpyベクトル）\n",
    "    \"\"\"\n",
    "\n",
    "    tokens = preprocess(text)    \n",
    "\n",
    "    # Filter non-vocab words\n",
    "    tokens = [token for token in tokens if token in vocab] #pass\n",
    "    # Convert words to ids\n",
    "    tokens = [vocab[token] for token in tokens] #pass\n",
    "\n",
    "    if len(tokens) == 0:\n",
    "        raise UnknownWordsError\n",
    "\n",
    "    # Adding a batch dimension\n",
    "    text_input = torch.from_numpy(np.asarray(torch.LongTensor(tokens).view(-1, 1)))\n",
    "\n",
    "    # Get the NN output       \n",
    "    batch_size = 1\n",
    "    hidden = model.init_hidden(batch_size) #pass\n",
    "    \n",
    "    logps, _ = model(text_input, hidden) #pass\n",
    "    # Take the exponent of the NN output to get a range of 0 to 1 for each label.\n",
    "    pred = torch.round(logps.squeeze())#pass\n",
    "    pred = torch.exp(logps) \n",
    "    \n",
    "    return pred\n",
    "\n",
    "\n",
    "def predict_api(args):\n",
    "    \"\"\" \n",
    "    Make a prediction on a single sentence.\n",
    "    Parameters\n",
    "    ----------\n",
    "        args : 入力（Pythonディクショナリ）\n",
    "    Returns\n",
    "    -------\n",
    "        pred : 予測値（Python配列）\n",
    "    \"\"\"\n",
    "    text = args.get('text')\n",
    "    try:\n",
    "        result = predict_func(text, model_l, vocab_l)\n",
    "        return result.detach().numpy()[0]\n",
    "    except UnknownWordsError:\n",
    "        return [0,0,1,0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ポジティブなセンチメントを連想させる文章を入力として予測（適宜、文章を変更して実行してみることができます）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.02395445  0.02243885  0.03738181  0.02565586  0.89056903]\n"
     ]
    }
   ],
   "source": [
    "args = {\"text\": \"I'm bullish on $goog\"}\n",
    "result = predict_api(args)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ネガティブなセンチメントを連想させる文章を入力として予測（適宜、文章を変更して実行してみることができます）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.76727909  0.07537365  0.05268526  0.08244011  0.02222182]\n"
     ]
    }
   ],
   "source": [
    "args = {\"text\": \"I'm bearish on $goog\"}\n",
    "result = predict_api(args)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ボキャブラリ辞書に存在しない単語のみの文章を入力として予測（適宜、文章を変更して実行してみることができます）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "args = {\"text\": \"kono yoshiyuki\"}\n",
    "result = predict_api(args)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
