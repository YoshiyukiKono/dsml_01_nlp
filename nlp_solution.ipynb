{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science / Machine Learning Meetup #1 Deep Learning Hands-on\n",
    "# オルタナティブ・データと自然言語処理\n",
    "\n",
    "## 手順\n",
    "各問題は、実装する関数と、関数の実装方法に関する指示で構成されています。実装する必要がある関数の部分には# TODOコメントが付いています。\n",
    "\n",
    "## 1. 環境準備\n",
    "\n",
    "### パッケージのインストールとインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in ./.local/lib/python3.6/site-packages (3.4.5)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n",
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: torch in ./.local/lib/python3.6/site-packages (1.4.0)\n",
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install nltk\n",
    "!pip3 install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipython-sql in ./.local/lib/python3.6/site-packages (0.3.9)\n",
      "Requirement already satisfied: prettytable in ./.local/lib/python3.6/site-packages (from ipython-sql) (0.7.2)\n",
      "Requirement already satisfied: sqlalchemy>=0.6.7 in ./.local/lib/python3.6/site-packages (from ipython-sql) (1.3.13)\n",
      "Requirement already satisfied: ipython-genutils>=0.1.0 in /usr/local/lib/python3.6/dist-packages (from ipython-sql) (0.2.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from ipython-sql) (1.12.0)\n",
      "Requirement already satisfied: sqlparse in ./.local/lib/python3.6/site-packages (from ipython-sql) (0.3.0)\n",
      "Requirement already satisfied: ipython>=1.0 in /usr/local/lib/python3.6/dist-packages (from ipython-sql) (5.1.0)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython>=1.0->ipython-sql) (2.4.2)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython>=1.0->ipython-sql) (4.4.0)\n",
      "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython>=1.0->ipython-sql) (4.3.2)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython>=1.0->ipython-sql) (41.0.1)\n",
      "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=1.0->ipython-sql) (0.8.1)\n",
      "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.3 in /usr/local/lib/python3.6/dist-packages (from ipython>=1.0->ipython-sql) (1.0.15)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=1.0->ipython-sql) (0.7.5)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=1.0->ipython-sql) (4.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.3->ipython>=1.0->ipython-sql) (0.1.7)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=1.0->ipython-sql) (0.6.0)\n",
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install ipython-sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyhive in ./.local/lib/python3.6/site-packages (0.6.1)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from pyhive) (2.8.0)\n",
      "Requirement already satisfied: future in ./.local/lib/python3.6/site-packages (from pyhive) (0.18.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil->pyhive) (1.12.0)\n",
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: sqlalchemy in ./.local/lib/python3.6/site-packages (1.3.13)\n",
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting thrift\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/97/1e/3284d19d7be99305eda145b8aa46b0c33244e4a496ec66440dac19f8274d/thrift-0.13.0.tar.gz (59kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 6.6MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.7.2 in /usr/local/lib/python3.6/dist-packages (from thrift) (1.12.0)\n",
      "Building wheels for collected packages: thrift\n",
      "  Building wheel for thrift (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/cdsw/.cache/pip/wheels/02/a2/46/689ccfcf40155c23edc7cdbd9de488611c8fdf49ff34b1706e\n",
      "Successfully built thrift\n",
      "Installing collected packages: thrift\n",
      "Successfully installed thrift-0.13.0\n",
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pyhive\n",
    "!pip3 install sqlalchemy\n",
    "!pip3 install thrift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hive_service\n",
      "\u001b[31m  ERROR: Could not find a version that satisfies the requirement hive_service (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for hive_service\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install hive_service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install sasl\n",
    "!pip3 install thrift_sasl\n",
    "# These are not used but it's easier than the server-side configuration mentioned below.\n",
    "# https://github.com/apache/incubator-superset/issues/4782"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import subprocess\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "import nltk\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Web Scraping\n",
    "\n",
    "下記のようなエラーメッセージを受け取ることがあります。\n",
    "\n",
    "```\n",
    "{\"response\":{\"status\":429},\"errors\":[{\"message\":\"Rate limit exceeded. Client may not make more than 200 requests an hour.\"}]}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A\\n', 'AA\\n', 'AAL\\n', 'AAN\\n', 'AAOI\\n', 'AAON\\n', 'AAP\\n', 'AAPL\\n', 'AAWW\\n', 'AAXN\\n', 'ABBV\\n', 'ABC\\n', 'ABCB\\n', 'ABEO\\n', 'ABG\\n', 'ABM\\n', 'ABMD\\n', 'ABT\\n', 'ABTX\\n', 'ACA\\n', 'ACAD\\n', 'ACCO\\n', 'ACEL\\n', 'ACGL\\n', 'ACHC\\n', 'ACHN\\n', 'ACHV\\n', 'ACIA\\n', 'ACIW\\n', 'ACLS\\n', 'ACM\\n', 'ACN\\n', 'ACNB\\n', 'ACOR\\n', 'ACRS\\n', 'ACRX\\n', 'ACTG\\n', 'ADBE\\n', 'ADES\\n', 'ADI\\n', 'ADM\\n', 'ADMA\\n', 'ADMP\\n', 'ADMS\\n', 'ADP\\n', 'ADPT\\n', 'ADRO\\n', 'ADS\\n', 'ADSK\\n', 'ADSW\\n', 'ADT\\n', 'ADTN\\n', 'ADUS\\n', 'ADVM\\n', 'ADXS\\n', 'AE\\n', 'AEE\\n', 'AEGN\\n', 'AEIS\\n', 'AEL\\n', 'AEM\\n', 'AEMD\\n', 'AEO\\n', 'AEP\\n', 'AERI\\n', 'AES\\n', 'AFG\\n', 'AFI\\n', 'AFL\\n', 'AG\\n', 'AGCO\\n', 'AGEN\\n', 'AGFS\\n', 'AGI\\n', 'AGIO\\n', 'AGLE\\n', 'AGM\\n', 'AGN\\n', 'AGO\\n', 'AGR\\n', 'AGRX\\n', 'AGS\\n', 'AGTC\\n', 'AGX\\n', 'AGYS\\n', 'AHC\\n', 'AHCO\\n', 'AIG\\n', 'AIMC\\n', 'AIMT\\n', 'AIN\\n', 'AIR\\n', 'AIRG\\n', 'AIRT\\n', 'AIT\\n', 'AIZ\\n', 'AJG\\n', 'AJRD\\n', 'AKAM\\n', 'AKBA\\n', 'AKCA\\n', 'AKRO\\n', 'AKRX\\n', 'AKS\\n', 'AL\\n', 'ALB\\n', 'ALCO\\n', 'ALDX\\n', 'ALE\\n', 'ALEC\\n', 'ALG\\n', 'ALGN\\n', 'ALGT\\n', 'ALIM\\n', 'ALK\\n', 'ALKS\\n', 'ALL\\n', 'ALLK\\n', 'ALLO\\n', 'ALLY\\n', 'ALNY\\n', 'ALOT\\n', 'ALPN\\n', 'ALRM\\n', 'ALRN\\n', 'ALSK\\n', 'ALSN\\n', 'ALT\\n', 'ALTR\\n', 'ALV\\n', 'ALXN\\n', 'AM\\n', 'AMAG\\n', 'AMAL\\n', 'AMAT\\n', 'AMBA\\n', 'AMBC\\n', 'AMC\\n', 'AMCX\\n', 'AMD\\n', 'AME\\n', 'AMED\\n', 'AMEH\\n', 'AMG\\n', 'AMGN\\n', 'AMK\\n', 'AMKR\\n', 'AMN\\n', 'AMNB\\n', 'AMOT\\n', 'AMP\\n', 'AMPE\\n', 'AMPH\\n', 'AMRC\\n', 'AMRS\\n', 'AMRX\\n', 'AMSC\\n', 'AMSF\\n', 'AMSWA\\n', 'AMTB\\n', 'AMTD\\n', 'AMTX\\n', 'AMWD\\n', 'AMZN\\n', 'AN\\n', 'ANAB\\n', 'ANAT\\n', 'ANDA\\n', 'ANDE\\n', 'ANET\\n', 'ANF\\n', 'ANGI\\n', 'ANGO\\n', 'ANIK\\n', 'ANIP\\n', 'ANIX\\n', 'ANSS\\n', 'ANTM\\n', 'AOBC\\n', 'AON\\n', 'AOS\\n', 'AP\\n', 'APA\\n', 'APD\\n', 'APDN\\n', 'APEI\\n', 'APEN\\n', 'APH\\n', 'APLS\\n', 'APLT\\n', 'APOG\\n', 'APPF\\n', 'APPN\\n', 'APPS\\n', 'APRE\\n', 'APRN\\n', 'APT\\n', 'APTV\\n', 'APY\\n', 'AQB\\n', 'AQN\\n', 'AQUA\\n', 'AR\\n', 'ARA\\n', 'ARAV\\n', 'ARAY\\n', 'ARCB\\n', 'ARCH\\n', 'ARCO\\n', 'ARCT\\n', 'ARDS\\n', 'ARDX\\n', 'ARES\\n', 'ARGO\\n', 'ARKR\\n', 'ARLO\\n', 'ARMK\\n', 'ARMP\\n', 'ARNA\\n', 'ARNC\\n', 'AROC\\n', 'AROW\\n', 'ARTNA\\n', 'ARVN\\n', 'ARW\\n', 'ARWR\\n', 'ASB\\n', 'ASFI\\n', 'ASGN\\n', 'ASH\\n', 'ASIX\\n', 'ASMB\\n', 'ASNA\\n', 'ASPS\\n', 'ASPU\\n', 'ASRT\\n', 'ASTE\\n', 'ASYS\\n', 'ATEC\\n', 'ATEN\\n', 'ATEX\\n', 'ATGE\\n', 'ATH\\n', 'ATHX\\n', 'ATI\\n', 'ATKR\\n', 'ATLO\\n', 'ATNI\\n', 'ATNX\\n', 'ATO\\n', 'ATR\\n', 'ATRA\\n', 'ATRC\\n', 'ATRI\\n', 'ATRO\\n', 'ATRS\\n', 'ATSG\\n', 'ATUS\\n', 'ATVI\\n', 'AUB\\n', 'AUMN\\n', 'AUPH\\n', 'AUTO\\n', 'AUY\\n', 'AVA\\n', 'AVAV\\n', 'AVD\\n', 'AVGO\\n', 'AVID\\n', 'AVLR\\n', 'AVNS\\n', 'AVNW\\n', 'AVRO\\n', 'AVT\\n', 'AVTR\\n', 'AVX\\n', 'AVXL\\n', 'AVY\\n', 'AVYA\\n', 'AWI\\n', 'AWK\\n', 'AWR\\n', 'AWRE\\n', 'AX\\n', 'AXAS\\n', 'AXDX\\n', 'AXGN\\n', 'AXGT\\n', 'AXL\\n', 'AXLA\\n', 'AXNX\\n', 'AXP\\n', 'AXS\\n', 'AXSM\\n', 'AXTA\\n', 'AXTI\\n', 'AYI\\n', 'AYX\\n', 'AZO\\n', 'AZPN\\n', 'AZZ\\n', 'B\\n', 'BA\\n', 'BAC\\n', 'BAH\\n', 'BAM\\n', 'BANC\\n', 'BAND\\n', 'BANF\\n', 'BANR\\n', 'BAP\\n', 'BAX\\n', 'BB\\n', 'BBBY\\n', 'BBI\\n', 'BBIO\\n', 'BBQ\\n', 'BBW\\n', 'BBY\\n', 'BC\\n', 'BCBP\\n', 'BCC\\n', 'BCE\\n', 'BCEI\\n', 'BCEL\\n', 'BCLI\\n', 'BCO\\n', 'BCOR\\n', 'BCOV\\n', 'BCPC\\n', 'BCRX\\n', 'BDC\\n', 'BDGE\\n', 'BDSI\\n', 'BDX\\n', 'BE\\n', 'BEAT\\n', 'BECN\\n', 'BELFB\\n', 'BEN\\n', 'BERY\\n', 'BFAM\\n', 'BFC\\n', 'BFIN\\n', 'BG\\n', 'BGCP\\n', 'BGFV\\n', 'BGG\\n', 'BGS\\n', 'BH\\n', 'BHB\\n', 'BHC\\n', 'BHE\\n', 'BHF\\n', 'BHLB\\n', 'BIG\\n', 'BIIB\\n', 'BIMI\\n', 'BIO\\n', 'BIOS\\n', 'BJ\\n', 'BJRI\\n', 'BK\\n', 'BKD\\n', 'BKE\\n', 'BKH\\n', 'BKI\\n', 'BKNG\\n', 'BKR\\n', 'BKU\\n', 'BL\\n', 'BLBD\\n', 'BLCM\\n', 'BLD\\n', 'BLDP\\n', 'BLDR\\n', 'BLFS\\n', 'BLK\\n', 'BLKB\\n', 'BLL\\n', 'BLMN\\n', 'BLNK\\n', 'BLUE\\n', 'BLX\\n', 'BMCH\\n', 'BMI\\n', 'BMO\\n', 'BMRC\\n', 'BMRN\\n', 'BMTC\\n', 'BMY\\n', 'BNED\\n', 'BNFT\\n', 'BNGO\\n', 'BNS\\n', 'BOH\\n', 'BOKF\\n', 'BOMN\\n', 'BOOM\\n', 'BOOT\\n', 'BOX\\n', 'BPFH\\n', 'BPMC\\n', 'BPTH\\n', 'BR\\n', 'BRC\\n', 'BRKB\\n', 'BRKL\\n', 'BRKR\\n', 'BRKS\\n', 'BRMK\\n', 'BRO\\n', 'BRY\\n', 'BSET\\n', 'BSIG\\n', 'BSQR\\n', 'BSRR\\n', 'BSTC\\n', 'BSX\\n', 'BTG\\n', 'BURL\\n', 'BUSE\\n', 'BV\\n', 'BWA\\n', 'BWEN\\n', 'BWXT\\n', 'BX\\n', 'BXC\\n', 'BXG\\n', 'BXS\\n', 'BY\\n', 'BYD\\n', 'BYND\\n', 'BZH\\n', 'C\\n', 'CABO\\n', 'CAC\\n', 'CACC\\n', 'CACI\\n', 'CADE\\n', 'CAG\\n', 'CAH\\n', 'CAI\\n', 'CAKE\\n', 'CAL\\n', 'CALA\\n', 'CALM\\n', 'CALX\\n', 'CAMP\\n', 'CAPR\\n', 'CAR\\n', 'CARA\\n', 'CARE\\n', 'CARG\\n', 'CARS\\n', 'CASA\\n', 'CASH\\n', 'CASI\\n', 'CASS\\n', 'CASY\\n', 'CAT\\n', 'CATB\\n', 'CATM\\n', 'CATO\\n', 'CATY\\n', 'CBIO\\n', 'CBMG\\n', 'CBPO\\n', 'CBPX\\n', 'CBRE\\n', 'CBRL\\n', 'CBSH\\n', 'CBT\\n', 'CBTX\\n', 'CBU\\n', 'CBZ\\n', 'CC\\n', 'CCBG\\n', 'CCEP\\n', 'CCF\\n', 'CCJ\\n', 'CCK\\n', 'CCL\\n', 'CCMP\\n', 'CCNE\\n', 'CCO\\n', 'CCOI\\n', 'CCRN\\n', 'CCS\\n', 'CCXI\\n', 'CDAY\\n', 'CDE\\n', 'CDK\\n', 'CDLX\\n', 'CDMO\\n', 'CDNA\\n', 'CDNS\\n', 'CDW\\n', 'CDXS\\n', 'CE\\n', 'CECE\\n', 'CEIX\\n', 'CEL\\n', 'CELH\\n', 'CEMI\\n', 'CENT\\n', 'CENTA\\n', 'CENX\\n', 'CERN\\n', 'CERS\\n', 'CEVA\\n', 'CF\\n', 'CFB\\n', 'CFFI\\n', 'CFFN\\n', 'CFG\\n', 'CFMS\\n', 'CFR\\n', 'CFX\\n', 'CGC\\n', 'CGEN\\n', 'CGNX\\n', 'CHCO\\n', 'CHD\\n', 'CHDN\\n', 'CHE\\n', 'CHEF\\n', 'CHGG\\n', 'CHH\\n', 'CHK\\n', 'CHKP\\n', 'CHMA\\n', 'CHMG\\n', 'CHNG\\n', 'CHRS\\n', 'CHRW\\n', 'CHS\\n', 'CHTR\\n', 'CHUY\\n', 'CHWY\\n', 'CI\\n', 'CIEN\\n', 'CINF\\n', 'CIR\\n', 'CIT\\n', 'CKH\\n', 'CKPT\\n', 'CL\\n', 'CLAR\\n', 'CLBK\\n', 'CLBS\\n', 'CLCT\\n', 'CLDR\\n', 'CLDX\\n', 'CLF\\n', 'CLFD\\n', 'CLGX\\n', 'CLH\\n', 'CLNE\\n', 'CLPS\\n', 'CLR\\n', 'CLSD\\n', 'CLSN\\n', 'CLVS\\n', 'CLW\\n', 'CLX\\n', 'CLXT\\n', 'CM\\n', 'CMA\\n', 'CMBM\\n', 'CMC\\n', 'CMCO\\n', 'CMCSA\\n', 'CMD\\n', 'CME\\n', 'CMG\\n', 'CMI\\n', 'CMLS\\n', 'CMP\\n', 'CMS\\n', 'CMTL\\n', 'CNA\\n', 'CNBKA\\n', 'CNC\\n', 'CNCE\\n', 'CNDT\\n', 'CNI\\n', 'CNK\\n', 'CNMD\\n', 'CNNE\\n', 'CNO\\n', 'CNOB\\n', 'CNP\\n', 'CNQ\\n', 'CNR\\n', 'CNS\\n', 'CNSL\\n', 'CNST\\n', 'CNX\\n', 'CNXN\\n', 'CO\\n', 'COF\\n', 'COG\\n', 'COHR\\n', 'COHU\\n', 'COKE\\n', 'COLB\\n', 'COLL\\n', 'COLM\\n', 'COMM\\n', 'CONN\\n', 'COO\\n', 'COOP\\n', 'COP\\n', 'CORE\\n', 'CORT\\n', 'COST\\n', 'COT\\n', 'COTY\\n', 'COUP\\n', 'CP\\n', 'CPA\\n', 'CPB\\n', 'CPE\\n', 'CPF\\n', 'CPG\\n', 'CPIX\\n', 'CPK\\n', 'CPRI\\n', 'CPRT\\n', 'CPRX\\n', 'CPS\\n', 'CPSI\\n', 'CPST\\n', 'CR\\n', 'CRAI\\n', 'CRBP\\n', 'CRC\\n', 'CRCM\\n', 'CREE\\n', 'CRI\\n', 'CRIS\\n', 'CRK\\n', 'CRL\\n', 'CRM\\n', 'CRMD\\n', 'CRMT\\n', 'CRNC\\n', 'CRNT\\n', 'CRNX\\n', 'CRON\\n', 'CROX\\n', 'CRS\\n', 'CRTX\\n', 'CRUS\\n', 'CRVL\\n', 'CRVS\\n', 'CRWD\\n', 'CRWS\\n', 'CRY\\n', 'CSBR\\n', 'CSCO\\n', 'CSFL\\n', 'CSGP\\n', 'CSGS\\n', 'CSII\\n', 'CSIQ\\n', 'CSL\\n', 'CSOD\\n', 'CSPI\\n', 'CSS\\n', 'CSSE\\n', 'CSTL\\n', 'CSU\\n', 'CSV\\n', 'CSWI\\n', 'CSX\\n', 'CTAS\\n', 'CTB\\n', 'CTBI\\n', 'CTG\\n', 'CTIC\\n', 'CTL\\n', 'CTLT\\n', 'CTMX\\n', 'CTO\\n', 'CTRA\\n', 'CTRN\\n', 'CTS\\n', 'CTSH\\n', 'CTSO\\n', 'CTVA\\n', 'CTXS\\n', 'CUB\\n', 'CUBI\\n', 'CULP\\n', 'CURO\\n', 'CUTR\\n', 'CVA\\n', 'CVBF\\n', 'CVCO\\n', 'CVE\\n', 'CVET\\n', 'CVGW\\n', 'CVI\\n', 'CVLT\\n', 'CVLY\\n', 'CVM\\n', 'CVNA\\n', 'CVS\\n', 'CVTI\\n', 'CVU\\n', 'CVX\\n', 'CW\\n', 'CWBC\\n', 'CWCO\\n', 'CWEN\\n', 'CWH\\n', 'CWK\\n', 'CWST\\n', 'CWT\\n', 'CXDC\\n', 'CXO\\n', 'CY\\n', 'CYAN\\n', 'CYBE\\n', 'CYBR\\n', 'CYCN\\n', 'CYD\\n', 'CYH\\n', 'CYRX\\n', 'CYTK\\n', 'CZNC\\n', 'CZR\\n', 'CZWI\\n', 'CZZ\\n', 'D\\n', 'DAIO\\n', 'DAKT\\n', 'DAL\\n', 'DAN\\n', 'DAR\\n', 'DARE\\n', 'DB\\n', 'DBD\\n', 'DBI\\n', 'DBX\\n', 'DCI\\n', 'DCO\\n', 'DCOM\\n', 'DCPH\\n', 'DD\\n', 'DDD\\n', 'DDOG\\n', 'DDS\\n', 'DE\\n', 'DECK\\n', 'DELL\\n', 'DENN\\n', 'DERM\\n', 'DFIN\\n', 'DFS\\n', 'DG\\n', 'DGICA\\n', 'DGII\\n', 'DGLY\\n', 'DGX\\n', 'DHI\\n', 'DHIL\\n', 'DHR\\n', 'DHT\\n', 'DIN\\n', 'DIOD\\n', 'DIS\\n', 'DISCA\\n', 'DISCK\\n', 'DISH\\n', 'DJCO\\n', 'DK\\n', 'DKS\\n', 'DLA\\n', 'DLB\\n', 'DLTR\\n', 'DLX\\n', 'DMRC\\n', 'DNKN\\n', 'DNLI\\n', 'DNOW\\n', 'DNR\\n', 'DO\\n', 'DOCU\\n', 'DOMO\\n', 'DOOR\\n', 'DORM\\n', 'DOV\\n', 'DOW\\n', 'DPLO\\n', 'DPZ\\n', 'DRAD\\n', 'DRI\\n', 'DRNA\\n', 'DRQ\\n', 'DSGX\\n', 'DSKE\\n', 'DSPG\\n', 'DT\\n', 'DTE\\n', 'DTIL\\n', 'DUK\\n', 'DVA\\n', 'DVAX\\n', 'DVD\\n', 'DVN\\n', 'DXC\\n', 'DXCM\\n', 'DXPE\\n', 'DXR\\n', 'DY\\n', 'DZSI\\n', 'EA\\n', 'EAF\\n', 'EAT\\n', 'EB\\n', 'EBAY\\n', 'EBF\\n', 'EBIX\\n', 'EBS\\n', 'EBSB\\n', 'EBTC\\n', 'ECHO\\n', 'ECL\\n', 'ECOL\\n', 'ECOM\\n', 'ECPG\\n', 'ED\\n', 'EDIT\\n', 'EDSA\\n', 'EDTX\\n', 'EDUC\\n', 'EE\\n', 'EEFT\\n', 'EEX\\n', 'EFC\\n', 'EFSC\\n', 'EFX\\n', 'EGAN\\n', 'EGBN\\n', 'EGHT\\n', 'EGO\\n', 'EGOV\\n', 'EGRX\\n', 'EGY\\n', 'EHC\\n', 'EHTH\\n', 'EIDX\\n', 'EIG\\n', 'EIGI\\n', 'EIGR\\n', 'EIX\\n', 'EKSO\\n', 'EL\\n', 'ELAN\\n', 'ELF\\n', 'ELGX\\n', 'ELY\\n', 'EMCF\\n', 'EME\\n', 'EMKR\\n', 'EML\\n', 'EMMS\\n', 'EMN\\n', 'EMR\\n', 'ENB\\n', 'ENDP\\n', 'ENLV\\n', 'ENPH\\n', 'ENR\\n', 'ENS\\n', 'ENSG\\n', 'ENTA\\n', 'ENTG\\n', 'ENV\\n', 'ENVA\\n', 'EOG\\n', 'EOLS\\n', 'EPAC\\n', 'EPAM\\n', 'EPAY\\n', 'EPC\\n', 'EPIX\\n', 'EPM\\n', 'EPZM\\n', 'EQH\\n', 'EQT\\n', 'ERA\\n', 'ERF\\n', 'ERI\\n', 'ERIE\\n', 'ERII\\n', 'ES\\n', 'ESCA\\n', 'ESE\\n', 'ESGR\\n', 'ESLT\\n', 'ESNT\\n', 'ESPR\\n', 'ESSA\\n', 'ETFC\\n', 'ETH\\n', 'ETM\\n', 'ETN\\n', 'ETNB\\n', 'ETR\\n', 'ETSY\\n', 'EV\\n', 'EVBG\\n', 'EVC\\n', 'EVER\\n', 'EVFM\\n', 'EVH\\n', 'EVOP\\n', 'EVR\\n', 'EVRG\\n', 'EVRI\\n', 'EW\\n', 'EWBC\\n', 'EXAS\\n', 'EXC\\n', 'EXEL\\n', 'EXK\\n', 'EXLS\\n', 'EXP\\n', 'EXPD\\n', 'EXPE\\n', 'EXPI\\n', 'EXPO\\n', 'EXPR\\n', 'EXTR\\n', 'EYE\\n', 'EYPT\\n', 'EZPW\\n', 'F\\n', 'FAF\\n', 'FANG\\n', 'FARM\\n', 'FARO\\n', 'FAST\\n', 'FAT\\n', 'FATE\\n', 'FB\\n', 'FBC\\n', 'FBHS\\n', 'FBIZ\\n', 'FBK\\n', 'FBM\\n', 'FBMS\\n', 'FBNC\\n', 'FC\\n', 'FCBC\\n', 'FCEL\\n', 'FCF\\n', 'FCFS\\n', 'FCN\\n', 'FCNCA\\n', 'FCX\\n', 'FDEF\\n', 'FDP\\n', 'FDS\\n', 'FDX\\n', 'FE\\n', 'FEIM\\n', 'FELE\\n', 'FEYE\\n', 'FF\\n', 'FFBC\\n', 'FFG\\n', 'FFIC\\n', 'FFIN\\n', 'FFIV\\n', 'FFWM\\n', 'FG\\n', 'FGEN\\n', 'FHB\\n', 'FHN\\n', 'FIBK\\n', 'FICO\\n', 'FII\\n', 'FIS\\n', 'FISI\\n', 'FISV\\n', 'FIT\\n', 'FITB\\n', 'FIVE\\n', 'FIVN\\n', 'FIX\\n', 'FIXX\\n', 'FIZZ\\n', 'FL\\n', 'FLDM\\n', 'FLEX\\n', 'FLGT\\n', 'FLIC\\n', 'FLIR\\n', 'FLMN\\n', 'FLNT\\n', 'FLO\\n', 'FLOW\\n', 'FLR\\n', 'FLS\\n', 'FLT\\n', 'FLWS\\n', 'FLXN\\n', 'FLXS\\n', 'FMBH\\n', 'FMBI\\n', 'FMC\\n', 'FMNB\\n', 'FN\\n', 'FNB\\n', 'FND\\n', 'FNF\\n', 'FNHC\\n', 'FNJN\\n', 'FNKO\\n', 'FNLC\\n', 'FNV\\n', 'FOCS\\n', 'FOE\\n', 'FOLD\\n', 'FOMX\\n', 'FONR\\n', 'FOR\\n', 'FORM\\n', 'FORR\\n', 'FOSL\\n', 'FOX\\n', 'FOXA\\n', 'FOXF\\n', 'FPRX\\n', 'FRAN\\n', 'FRC\\n', 'FREQ\\n', 'FRGI\\n', 'FRHC\\n', 'FRME\\n', 'FRO\\n', 'FRPH\\n', 'FRPT\\n', 'FRTA\\n', 'FSB\\n', 'FSBW\\n', 'FSCT\\n', 'FSFG\\n', 'FSI\\n', 'FSLR\\n', 'FSLY\\n', 'FSM\\n', 'FSS\\n', 'FSTR\\n', 'FTCH\\n', 'FTDR\\n', 'FTEK\\n', 'FTI\\n', 'FTNT\\n', 'FTR\\n', 'FTS\\n', 'FTSV\\n', 'FTV\\n', 'FUL\\n', 'FULC\\n', 'FULT\\n', 'FVE\\n', 'FVRR\\n', 'FWONA\\n', 'FWRD\\n', 'GABC\\n', 'GALT\\n', 'GATX\\n', 'GBCI\\n', 'GBL\\n', 'GBLI\\n', 'GBT\\n', 'GBX\\n', 'GCAP\\n', 'GCBC\\n', 'GCI\\n', 'GCO\\n', 'GCP\\n', 'GD\\n', 'GDDY\\n', 'GDEN\\n', 'GDI\\n', 'GDOT\\n', 'GE\\n', 'GEC\\n', 'GEF\\n', 'GEOS\\n', 'GERN\\n', 'GES\\n', 'GEVO\\n', 'GFF\\n', 'GGG\\n', 'GH\\n', 'GHC\\n', 'GHL\\n', 'GHM\\n', 'GIB\\n', 'GIFI\\n', 'GIII\\n', 'GIL\\n', 'GILD\\n', 'GIS\\n', 'GKOS\\n', 'GL\\n', 'GLDD\\n', 'GLMD\\n', 'GLNG\\n', 'GLRE\\n', 'GLT\\n', 'GLUU\\n', 'GLW\\n', 'GLYC\\n', 'GM\\n', 'GME\\n', 'GMED\\n', 'GMS\\n', 'GNC\\n', 'GNCA\\n', 'GNE\\n', 'GNMK\\n', 'GNMX\\n', 'GNRC\\n', 'GNTX\\n', 'GNW\\n', 'GO\\n', 'GOGO\\n', 'GOLD\\n', 'GOLF\\n', 'GOOG\\n', 'GOOGL\\n', 'GOOS\\n', 'GORO\\n', 'GOSS\\n', 'GPC\\n', 'GPI\\n', 'GPK\\n', 'GPN\\n', 'GPOR\\n', 'GPRE\\n', 'GPRK\\n', 'GPRO\\n', 'GPS\\n', 'GPX\\n', 'GRA\\n', 'GRBK\\n', 'GRC\\n', 'GRIF\\n', 'GRPN\\n', 'GRTS\\n', 'GRUB\\n', 'GS\\n', 'GSB\\n', 'GSBC\\n', 'GSHD\\n', 'GSKY\\n', 'GT\\n', 'GTES\\n', 'GTHX\\n', 'GTLS\\n', 'GTN\\n', 'GTT\\n', 'GTX\\n', 'GVA\\n', 'GWB\\n', 'GWRE\\n', 'GWRS\\n', 'GWW\\n', 'H\\n', 'HA\\n', 'HABT\\n', 'HAE\\n', 'HAFC\\n', 'HAIN\\n', 'HAL\\n', 'HALO\\n', 'HARP\\n', 'HAS\\n', 'HAYN\\n', 'HBAN\\n', 'HBB\\n', 'HBCP\\n', 'HBI\\n', 'HBIO\\n', 'HBM\\n', 'HBNC\\n', 'HBT\\n', 'HCA\\n', 'HCAT\\n', 'HCC\\n', 'HCCI\\n', 'HCI\\n', 'HCKT\\n', 'HCSG\\n', 'HD\\n', 'HDS\\n', 'HDSN\\n', 'HE\\n', 'HEAR\\n', 'HEES\\n', 'HEI\\n', 'HELE\\n', 'HEPA\\n', 'HES\\n', 'HFBL\\n', 'HFC\\n', 'HFFG\\n', 'HFWA\\n', 'HGV\\n', 'HHC\\n', 'HHS\\n', 'HI\\n', 'HIBB\\n', 'HIFS\\n', 'HIG\\n', 'HII\\n', 'HIIQ\\n', 'HJLI\\n', 'HL\\n', 'HLF\\n', 'HLI\\n', 'HLIO\\n', 'HLIT\\n', 'HLT\\n', 'HLX\\n', 'HMHC\\n', 'HMN\\n', 'HMST\\n', 'HMSY\\n', 'HMTV\\n', 'HNGR\\n', 'HNI\\n', 'HOFT\\n', 'HOG\\n', 'HOLX\\n', 'HOMB\\n', 'HOME\\n', 'HON\\n', 'HONE\\n', 'HOPE\\n', 'HOV\\n', 'HP\\n', 'HPE\\n', 'HPQ\\n', 'HQY\\n', 'HRB\\n', 'HRC\\n', 'HRI\\n', 'HRL\\n', 'HROW\\n', 'HRTG\\n', 'HRTX\\n', 'HSC\\n', 'HSIC\\n', 'HSII\\n', 'HSKA\\n', 'HSTM\\n', 'HSY\\n', 'HTBI\\n', 'HTBK\\n', 'HTGM\\n', 'HTH\\n', 'HTLD\\n', 'HTLF\\n', 'HUBB\\n', 'HUBG\\n', 'HUBS\\n', 'HUD\\n', 'HUM\\n', 'HUN\\n', 'HURC\\n', 'HURN\\n', 'HVT\\n', 'HWC\\n', 'HWCC\\n', 'HWKN\\n', 'HXL\\n', 'HY\\n', 'HYRE\\n', 'HZO\\n', 'IAA\\n', 'IAC\\n', 'IAG\\n', 'IART\\n', 'IBCP\\n', 'IBKC\\n', 'IBKR\\n', 'IBM\\n', 'IBOC\\n', 'IBP\\n', 'IBTX\\n', 'ICBK\\n', 'ICE\\n', 'ICFI\\n', 'ICHR\\n', 'ICON\\n', 'ICPT\\n', 'ICUI\\n', 'IDA\\n', 'IDCC\\n', 'IDRA\\n', 'IDT\\n', 'IDXX\\n', 'IESC\\n', 'IEX\\n', 'IFF\\n', 'IGMS\\n', 'IGT\\n', 'IHC\\n', 'IIIN\\n', 'IIIV\\n', 'IIVI\\n', 'ILMN\\n', 'IMAX\\n', 'IMGN\\n', 'IMH\\n', 'IMKTA\\n', 'IMMR\\n', 'IMMU\\n', 'IMUX\\n', 'IMXI\\n', 'INAP\\n', 'INBK\\n', 'INCY\\n', 'INDB\\n', 'INFN\\n', 'INFO\\n', 'INGN\\n', 'INGR\\n', 'INMD\\n', 'INO\\n', 'INOD\\n', 'INOV\\n', 'INS\\n', 'INSG\\n', 'INSM\\n', 'INSP\\n', 'INST\\n', 'INT\\n', 'INTC\\n', 'INTL\\n', 'INTU\\n', 'INVA\\n', 'INVE\\n', 'IO\\n', 'IONS\\n', 'IOSP\\n', 'IOTS\\n', 'IOVA\\n', 'IP\\n', 'IPAR\\n', 'IPG\\n', 'IPGP\\n', 'IPHI\\n', 'IPHS\\n', 'IPI\\n', 'IPWR\\n', 'IQV\\n', 'IR\\n', 'IRBT\\n', 'IRDM\\n', 'IRMD\\n', 'IRTC\\n', 'IRWD\\n', 'ISBC\\n', 'ISEE\\n', 'ISNS\\n', 'ISRG\\n', 'IT\\n', 'ITCI\\n', 'ITGR\\n', 'ITIC\\n', 'ITRI\\n', 'ITT\\n', 'ITW\\n', 'IVC\\n', 'IVZ\\n', 'J\\n', 'JACK\\n', 'JAZZ\\n', 'JBHT\\n', 'JBL\\n', 'JBLU\\n', 'JBSS\\n', 'JBT\\n', 'JCI\\n', 'JCOM\\n', 'JCP\\n', 'JEF\\n', 'JELD\\n', 'JILL\\n', 'JJSF\\n', 'JKHY\\n', 'JLL\\n', 'JNCE\\n', 'JNJ\\n', 'JNPR\\n', 'JOE\\n', 'JOUT\\n', 'JPM\\n', 'JRVR\\n', 'JVA\\n', 'JWN\\n', 'JYNT\\n', 'K\\n', 'KAI\\n', 'KALA\\n', 'KALU\\n', 'KALV\\n', 'KAMN\\n', 'KAR\\n', 'KBH\\n', 'KBR\\n', 'KDMN\\n', 'KDP\\n', 'KE\\n', 'KELYA\\n', 'KEM\\n', 'KEX\\n', 'KEY\\n', 'KEYS\\n', 'KFRC\\n', 'KFS\\n', 'KFY\\n', 'KGC\\n', 'KHC\\n', 'KIDS\\n', 'KIRK\\n', 'KL\\n', 'KLAC\\n', 'KLIC\\n', 'KMB\\n', 'KMI\\n', 'KMPR\\n', 'KMT\\n', 'KMX\\n', 'KN\\n', 'KNDI\\n', 'KNL\\n', 'KNSA\\n', 'KNSL\\n', 'KNX\\n', 'KO\\n', 'KOD\\n', 'KODK\\n', 'KOP\\n', 'KOPN\\n', 'KOS\\n', 'KPTI\\n', 'KR\\n', 'KRA\\n', 'KRNT\\n', 'KRNY\\n', 'KRO\\n', 'KRTX\\n', 'KRUS\\n', 'KRYS\\n', 'KSS\\n', 'KSU\\n', 'KTB\\n', 'KTCC\\n', 'KTOS\\n', 'KURA\\n', 'KVHI\\n', 'KW\\n', 'KWR\\n', 'L\\n', 'LAD\\n', 'LAKE\\n', 'LANC\\n', 'LASR\\n', 'LAUR\\n', 'LAWS\\n', 'LB\\n', 'LBAI\\n', 'LBC\\n', 'LBRDA\\n', 'LBRDK\\n', 'LBRT\\n', 'LBTYA\\n', 'LBTYK\\n', 'LBY\\n', 'LC\\n', 'LCI\\n', 'LCII\\n', 'LCNB\\n', 'LCUT\\n', 'LDL\\n', 'LDOS\\n', 'LE\\n', 'LEA\\n', 'LEAF\\n', 'LECO\\n', 'LEG\\n', 'LEGH\\n', 'LEN\\n', 'LEVI\\n', 'LFUS\\n', 'LFVN\\n', 'LGIH\\n', 'LGND\\n', 'LH\\n', 'LHCG\\n', 'LHX\\n', 'LII\\n', 'LILA\\n', 'LILAK\\n', 'LIN\\n', 'LINC\\n', 'LIND\\n', 'LITE\\n', 'LIVE\\n', 'LIVN\\n', 'LJPC\\n', 'LKFN\\n', 'LKQ\\n', 'LL\\n', 'LLNW\\n', 'LLY\\n', 'LM\\n', 'LMAT\\n', 'LMNR\\n', 'LMNX\\n', 'LMT\\n', 'LNC\\n', 'LNDC\\n', 'LNG\\n', 'LNN\\n', 'LNT\\n', 'LNTH\\n', 'LOB\\n', 'LOCO\\n', 'LOGC\\n', 'LOGM\\n', 'LOOP\\n', 'LOPE\\n', 'LORL\\n', 'LOVE\\n', 'LOW\\n', 'LPCN\\n', 'LPI\\n', 'LPLA\\n', 'LPSN\\n', 'LPX\\n', 'LQDA\\n', 'LQDT\\n', 'LRCX\\n', 'LRN\\n', 'LSCC\\n', 'LSTR\\n', 'LTHM\\n', 'LTRPA\\n', 'LTS\\n', 'LULU\\n', 'LUNA\\n', 'LUV\\n', 'LVGO\\n', 'LVS\\n', 'LW\\n', 'LWAY\\n', 'LXRX\\n', 'LXU\\n', 'LYFT\\n', 'LYTS\\n', 'LYV\\n', 'LZB\\n', 'M\\n', 'MA\\n', 'MACK\\n', 'MAGS\\n', 'MAN\\n', 'MANH\\n', 'MANT\\n', 'MANU\\n', 'MAR\\n', 'MARA\\n', 'MAS\\n', 'MASI\\n', 'MAT\\n', 'MATW\\n', 'MATX\\n', 'MAXR\\n', 'MBI\\n', 'MBIN\\n', 'MBIO\\n', 'MBOT\\n', 'MBUU\\n', 'MBWM\\n', 'MC\\n', 'MCD\\n', 'MCF\\n', 'MCHP\\n', 'MCHX\\n', 'MCK\\n', 'MCO\\n', 'MCRB\\n', 'MCRI\\n', 'MCS\\n', 'MCY\\n', 'MD\\n', 'MDB\\n', 'MDC\\n', 'MDGL\\n', 'MDLA\\n', 'MDLZ\\n', 'MDP\\n', 'MDRX\\n', 'MDT\\n', 'MDU\\n', 'MDWD\\n', 'MEC\\n', 'MED\\n', 'MEDP\\n', 'MEET\\n', 'MEI\\n', 'MELI\\n', 'MEOH\\n', 'MERC\\n', 'MESA\\n', 'MET\\n', 'MFC\\n', 'MFIN\\n', 'MFSF\\n', 'MG\\n', 'MGA\\n', 'MGEE\\n', 'MGI\\n', 'MGIC\\n', 'MGLN\\n', 'MGM\\n', 'MGNX\\n', 'MGPI\\n', 'MGRC\\n', 'MGTA\\n', 'MGTX\\n', 'MGY\\n', 'MHK\\n', 'MHO\\n', 'MIC\\n', 'MIDD\\n', 'MIK\\n', 'MIME\\n', 'MINI\\n', 'MIRM\\n', 'MIST\\n', 'MITK\\n', 'MKC\\n', 'MKL\\n', 'MKSI\\n', 'MKTX\\n', 'MLAB\\n', 'MLHR\\n', 'MLI\\n', 'MLM\\n', 'MLND\\n', 'MLNT\\n', 'MLNX\\n', 'MLR\\n', 'MMC\\n', 'MMM\\n', 'MMS\\n', 'MMSI\\n', 'MMYT\\n', 'MNI\\n', 'MNK\\n', 'MNKD\\n', 'MNLO\\n', 'MNOV\\n', 'MNRO\\n', 'MNST\\n', 'MNTA\\n', 'MO\\n', 'MOBL\\n', 'MOD\\n', 'MODN\\n', 'MOFG\\n', 'MOH\\n', 'MORF\\n', 'MORN\\n', 'MOS\\n', 'MOV\\n', 'MPAA\\n', 'MPC\\n', 'MPWR\\n', 'MPX\\n', 'MRAM\\n', 'MRC\\n', 'MRCY\\n', 'MRIN\\n', 'MRK\\n', 'MRKR\\n', 'MRLN\\n', 'MRNA\\n', 'MRNS\\n', 'MRO\\n', 'MRTN\\n', 'MRTX\\n', 'MRVL\\n', 'MS\\n', 'MSA\\n', 'MSBI\\n', 'MSCI\\n', 'MSEX\\n', 'MSFT\\n', 'MSG\\n', 'MSGN\\n', 'MSI\\n', 'MSM\\n', 'MSON\\n', 'MSTR\\n', 'MTB\\n', 'MTBC\\n', 'MTCH\\n', 'MTD\\n', 'MTDR\\n', 'MTEM\\n', 'MTEX\\n', 'MTG\\n', 'MTH\\n', 'MTN\\n', 'MTOR\\n', 'MTRN\\n', 'MTRX\\n', 'MTSC\\n', 'MTSI\\n', 'MTW\\n', 'MTX\\n', 'MTZ\\n', 'MU\\n', 'MUR\\n', 'MUSA\\n', 'MUX\\n', 'MVIS\\n', 'MWA\\n', 'MXIM\\n', 'MXL\\n', 'MYE\\n', 'MYGN\\n', 'MYL\\n', 'MYOK\\n', 'MYOV\\n', 'MYRG\\n', 'NAII\\n', 'NAT\\n', 'NATH\\n', 'NATI\\n', 'NATR\\n', 'NAV\\n', 'NAVI\\n', 'NBEV\\n', 'NBHC\\n', 'NBIX\\n', 'NBL\\n', 'NBR\\n', 'NBSE\\n', 'NBTB\\n', 'NC\\n', 'NCBS\\n', 'NCLH\\n', 'NCMI\\n', 'NCR\\n', 'NDAQ\\n', 'NDLS\\n', 'NDSN\\n', 'NEE\\n', 'NEM\\n', 'NEO\\n', 'NEOG\\n', 'NEON\\n', 'NEP\\n', 'NERV\\n', 'NET\\n', 'NETE\\n', 'NEU\\n', 'NEWR\\n', 'NEXT\\n', 'NFBK\\n', 'NFE\\n', 'NFG\\n', 'NFLX\\n', 'NG\\n', 'NGHC\\n', 'NGM\\n', 'NGS\\n', 'NGVC\\n', 'NGVT\\n', 'NHC\\n', 'NHTC\\n', 'NI\\n', 'NJR\\n', 'NK\\n', 'NKE\\n', 'NKSH\\n', 'NKTR\\n', 'NL\\n', 'NLNK\\n', 'NLOK\\n', 'NLSN\\n', 'NLTX\\n', 'NMIH\\n', 'NMRK\\n', 'NNBR\\n', 'NNI\\n', 'NOC\\n', 'NOV\\n', 'NOVA\\n', 'NOVN\\n', 'NOVT\\n', 'NOW\\n', 'NP\\n', 'NPK\\n', 'NPO\\n', 'NPTN\\n', 'NR\\n', 'NRC\\n', 'NRG\\n', 'NRIM\\n', 'NSC\\n', 'NSIT\\n', 'NSP\\n', 'NSSC\\n', 'NSTG\\n', 'NTAP\\n', 'NTB\\n', 'NTCT\\n', 'NTGR\\n', 'NTLA\\n', 'NTNX\\n', 'NTR\\n', 'NTRA\\n', 'NTRS\\n', 'NTUS\\n', 'NTWK\\n', 'NUAN\\n', 'NUE\\n', 'NUS\\n', 'NUVA\\n', 'NVAX\\n', 'NVCN\\n', 'NVCR\\n', 'NVDA\\n', 'NVEC\\n', 'NVEE\\n', 'NVMI\\n', 'NVR\\n', 'NVRO\\n', 'NVST\\n', 'NVTA\\n', 'NWBI\\n', 'NWE\\n', 'NWFL\\n', 'NWHM\\n', 'NWL\\n', 'NWLI\\n', 'NWN\\n', 'NWPX\\n', 'NWS\\n', 'NWSA\\n', 'NX\\n', 'NXGN\\n', 'NXPI\\n', 'NXST\\n', 'NXTC\\n', 'NYCB\\n', 'NYT\\n', 'OAS\\n', 'OBCI\\n', 'OBNK\\n', 'OC\\n', 'OCFC\\n', 'OCGN\\n', 'OCN\\n', 'OCUL\\n', 'ODC\\n', 'ODFL\\n', 'ODP\\n', 'ODT\\n', 'OFIX\\n', 'OFLX\\n', 'OGE\\n', 'OGS\\n', 'OI\\n', 'OII\\n', 'OIS\\n', 'OKE\\n', 'OKTA\\n', 'OLED\\n', 'OLLI\\n', 'OLN\\n', 'OMC\\n', 'OMCL\\n', 'OMER\\n', 'OMEX\\n', 'OMF\\n', 'OMI\\n', 'ON\\n', 'ONB\\n', 'ONCS\\n', 'ONCT\\n', 'ONDK\\n', 'ONTO\\n', 'ONTX\\n', 'OOMA\\n', 'OPB\\n', 'OPES\\n', 'OPGN\\n', 'OPK\\n', 'OPRT\\n', 'OPTN\\n', 'OPTT\\n', 'OPY\\n', 'ORA\\n', 'ORBC\\n', 'ORCC\\n', 'ORCL\\n', 'ORI\\n', 'ORLY\\n', 'ORMP\\n', 'ORRF\\n', 'OSIS\\n', 'OSK\\n', 'OSPN\\n', 'OSTK\\n', 'OSUR\\n', 'OTEX\\n', 'OTIC\\n', 'OTTR\\n', 'OVV\\n', 'OXM\\n', 'OXY\\n', 'OZK\\n', 'P\\n', 'PAAS\\n', 'PACB\\n', 'PACQ\\n', 'PACW\\n', 'PAG\\n', 'PAGP\\n', 'PAGS\\n', 'PAH\\n', 'PAHC\\n', 'PANW\\n', 'PAR\\n', 'PARR\\n', 'PATK\\n', 'PAYC\\n', 'PAYS\\n', 'PAYX\\n', 'PB\\n', 'PBCT\\n', 'PBF\\n', 'PBH\\n', 'PBI\\n', 'PBPB\\n', 'PBYI\\n', 'PCAR\\n', 'PCG\\n', 'PCOM\\n', 'PCRX\\n', 'PCTI\\n', 'PCTY\\n', 'PCYG\\n', 'PD\\n', 'PDCE\\n', 'PDCO\\n', 'PDFS\\n', 'PDLI\\n', 'PE\\n', 'PEBO\\n', 'PEG\\n', 'PEGA\\n', 'PEGI\\n', 'PEIX\\n', 'PEN\\n', 'PENN\\n', 'PEP\\n', 'PERI\\n', 'PETQ\\n', 'PETS\\n', 'PFBC\\n', 'PFBI\\n', 'PFE\\n', 'PFG\\n', 'PFGC\\n', 'PFIS\\n', 'PFNX\\n', 'PFPT\\n', 'PFS\\n', 'PFSI\\n', 'PFSW\\n', 'PG\\n', 'PGC\\n', 'PGNX\\n', 'PGNY\\n', 'PGR\\n', 'PGTI\\n', 'PH\\n', 'PHAS\\n', 'PHAT\\n', 'PHM\\n', 'PHR\\n', 'PHX\\n', 'PI\\n', 'PICO\\n', 'PII\\n', 'PINC\\n', 'PING\\n', 'PINS\\n', 'PIR\\n', 'PIRS\\n', 'PJT\\n', 'PKE\\n', 'PKG\\n', 'PKI\\n', 'PKOH\\n', 'PLAB\\n', 'PLAN\\n', 'PLAY\\n', 'PLCE\\n', 'PLIN\\n', 'PLMR\\n', 'PLNT\\n', 'PLOW\\n', 'PLPC\\n', 'PLSE\\n', 'PLT\\n', 'PLUG\\n', 'PLUS\\n', 'PLXP\\n', 'PLXS\\n', 'PLYA\\n', 'PM\\n', 'PMD\\n', 'PME\\n', 'PNC\\n', 'PNFP\\n', 'PNM\\n', 'PNR\\n', 'PNRG\\n', 'PNTG\\n', 'PNW\\n', 'PODD\\n', 'POL\\n', 'POOL\\n', 'POR\\n', 'POST\\n', 'POWI\\n', 'POWL\\n', 'PPBI\\n', 'PPC\\n', 'PPG\\n', 'PPIH\\n', 'PPL\\n', 'PPSI\\n', 'PQG\\n', 'PRA\\n', 'PRAA\\n', 'PRAH\\n', 'PRCP\\n', 'PRFT\\n', 'PRGO\\n', 'PRGS\\n', 'PRI\\n', 'PRIM\\n', 'PRK\\n', 'PRLB\\n', 'PRMW\\n', 'PRNB\\n', 'PRO\\n', 'PROS\\n', 'PROV\\n', 'PRPL\\n', 'PRSC\\n', 'PRSP\\n', 'PRTA\\n', 'PRTK\\n', 'PRTY\\n', 'PRU\\n', 'PRVB\\n', 'PRVL\\n', 'PS\\n', 'PSMT\\n', 'PSN\\n', 'PSNL\\n', 'PSTG\\n', 'PSTI\\n', 'PSTV\\n', 'PSX\\n', 'PTC\\n', 'PTCT\\n', 'PTEN\\n', 'PTGX\\n', 'PTI\\n', 'PTLA\\n', 'PTON\\n', 'PTSI\\n', 'PTVCB\\n', 'PUB\\n', 'PUMP\\n', 'PVG\\n', 'PVH\\n', 'PWOD\\n', 'PWR\\n', 'PXD\\n', 'PXLW\\n', 'PYPL\\n', 'PZN\\n', 'PZZA\\n', 'QADA\\n', 'QCOM\\n', 'QCRH\\n', 'QDEL\\n', 'QEP\\n', 'QLYS\\n', 'QNST\\n', 'QRTEA\\n', 'QRVO\\n', 'QSR\\n', 'QTRX\\n', 'QTWO\\n', 'QUAD\\n', 'QUIK\\n', 'QUMU\\n', 'QUOT\\n', 'R\\n', 'RACE\\n', 'RAD\\n', 'RADA\\n', 'RAIL\\n', 'RAMP\\n', 'RAPT\\n', 'RARE\\n', 'RAVE\\n', 'RAVN\\n', 'RBA\\n', 'RBBN\\n', 'RBC\\n', 'RBCAA\\n', 'RBCN\\n', 'RCI\\n', 'RCII\\n', 'RCKT\\n', 'RCKY\\n', 'RCL\\n', 'RCM\\n', 'RCUS\\n', 'RDFN\\n', 'RDI\\n', 'RDN\\n', 'RDNT\\n', 'RDUS\\n', 'RDWR\\n', 'RE\\n', 'REAL\\n', 'RECN\\n', 'REGI\\n', 'REGN\\n', 'RELL\\n', 'REPH\\n', 'REPL\\n', 'RES\\n', 'RESN\\n', 'RETA\\n', 'REV\\n', 'REVG\\n', 'REX\\n', 'REZI\\n', 'RF\\n', 'RFIL\\n', 'RFL\\n', 'RFP\\n', 'RGA\\n', 'RGEN\\n', 'RGLD\\n', 'RGNX\\n', 'RGR\\n', 'RGS\\n', 'RH\\n', 'RHI\\n', 'RICK\\n', 'RILY\\n', 'RIOT\\n', 'RJF\\n', 'RL\\n', 'RLGY\\n', 'RLH\\n', 'RLI\\n', 'RM\\n', 'RMAX\\n', 'RMBS\\n', 'RMCF\\n', 'RMD\\n', 'RMNI\\n', 'RMR\\n', 'RMTI\\n', 'RNET\\n', 'RNG\\n', 'RNR\\n', 'RNST\\n', 'RNWK\\n', 'ROCK\\n', 'ROG\\n', 'ROK\\n', 'ROKU\\n', 'ROL\\n', 'ROLL\\n', 'ROP\\n', 'ROST\\n', 'RP\\n', 'RPAY\\n', 'RPD\\n', 'RPM\\n', 'RRC\\n', 'RRD\\n', 'RRGB\\n', 'RRR\\n', 'RRTS\\n', 'RS\\n', 'RSG\\n', 'RST\\n', 'RTIX\\n', 'RTN\\n', 'RTRX\\n', 'RUBI\\n', 'RUBY\\n', 'RUN\\n', 'RUSHA\\n', 'RUTH\\n', 'RVLV\\n', 'RVNC\\n', 'RWLK\\n', 'RXN\\n', 'RY\\n', 'RYI\\n', 'RYTM\\n', 'S\\n', 'SA\\n', 'SABR\\n', 'SAFM\\n', 'SAFT\\n', 'SAGE\\n', 'SAH\\n', 'SAIA\\n', 'SAIC\\n', 'SAIL\\n', 'SAM\\n', 'SANM\\n', 'SANW\\n', 'SASR\\n', 'SATS\\n', 'SAVE\\n', 'SBCF\\n', 'SBGI\\n', 'SBH\\n', 'SBNY\\n', 'SBPH\\n', 'SBSI\\n', 'SBT\\n', 'SBUX\\n', 'SC\\n', 'SCCO\\n', 'SCHL\\n', 'SCHN\\n', 'SCHW\\n', 'SCI\\n', 'SCKT\\n', 'SCL\\n', 'SCOR\\n', 'SCPL\\n', 'SCS\\n', 'SCSC\\n', 'SCU\\n', 'SCVL\\n', 'SCWX\\n', 'SCX\\n', 'SDC\\n', 'SDRL\\n', 'SEAS\\n', 'SEB\\n', 'SEDG\\n', 'SEE\\n', 'SEIC\\n', 'SEM\\n', 'SENEA\\n', 'SERV\\n', 'SF\\n', 'SFBS\\n', 'SFE\\n', 'SFIX\\n', 'SFM\\n', 'SFNC\\n', 'SG\\n', 'SGA\\n', 'SGBX\\n', 'SGC\\n', 'SGEN\\n', 'SGH\\n', 'SGMO\\n', 'SGMS\\n', 'SGRY\\n', 'SGU\\n', 'SHAK\\n', 'SHEN\\n', 'SHLD\\n', 'SHLO\\n', 'SHOO\\n', 'SHOP\\n', 'SHW\\n', 'SIBN\\n', 'SIEN\\n', 'SIF\\n', 'SIG\\n', 'SIGA\\n', 'SIGI\\n', 'SILK\\n', 'SINA\\n', 'SINT\\n', 'SIRI\\n', 'SITE\\n', 'SIVB\\n', 'SIX\\n', 'SJI\\n', 'SJM\\n', 'SJR\\n', 'SJW\\n', 'SKX\\n', 'SKY\\n', 'SKYW\\n', 'SLAB\\n', 'SLB\\n', 'SLCA\\n', 'SLCT\\n', 'SLDB\\n', 'SLF\\n', 'SLGG\\n', 'SLGN\\n', 'SLM\\n', 'SLP\\n', 'SLRX\\n', 'SM\\n', 'SMAR\\n', 'SMBC\\n', 'SMED\\n', 'SMG\\n', 'SMMF\\n', 'SMP\\n', 'SMPL\\n', 'SMTC\\n', 'SMTX\\n', 'SNA\\n', 'SNAP\\n', 'SNBR\\n', 'SNCR\\n', 'SND\\n', 'SNDX\\n', 'SNOA\\n', 'SNPS\\n', 'SNSS\\n', 'SNV\\n', 'SNX\\n', 'SO\\n', 'SOI\\n', 'SON\\n', 'SONM\\n', 'SONO\\n', 'SORL\\n', 'SP\\n', 'SPAR\\n', 'SPB\\n', 'SPCE\\n', 'SPFI\\n', 'SPGI\\n', 'SPKE\\n', 'SPLK\\n', 'SPN\\n', 'SPNE\\n', 'SPNS\\n', 'SPOK\\n', 'SPOT\\n', 'SPPI\\n', 'SPR\\n', 'SPSC\\n', 'SPTN\\n', 'SPWR\\n', 'SPXC\\n', 'SQ\\n', 'SR\\n', 'SRAX\\n', 'SRCE\\n', 'SRCL\\n', 'SRDX\\n', 'SRE\\n', 'SRI\\n', 'SRL\\n', 'SRPT\\n', 'SRRK\\n', 'SSB\\n', 'SSD\\n', 'SSI\\n', 'SSNC\\n', 'SSP\\n', 'SSRM\\n', 'SSTI\\n', 'SSTK\\n', 'SSYS\\n', 'ST\\n', 'STAA\\n', 'STBA\\n', 'STC\\n', 'STE\\n', 'STFC\\n', 'STIM\\n', 'STL\\n', 'STLD\\n', 'STMP\\n', 'STNE\\n', 'STOK\\n', 'STRA\\n', 'STRL\\n', 'STRO\\n', 'STRS\\n', 'STRT\\n', 'STSA\\n', 'STT\\n', 'STX\\n', 'STZ\\n', 'SU\\n', 'SUM\\n', 'SUP\\n', 'SUPN\\n', 'SVMK\\n', 'SWAV\\n', 'SWCH\\n', 'SWIR\\n', 'SWK\\n', 'SWKS\\n', 'SWM\\n', 'SWN\\n', 'SWTX\\n', 'SWX\\n', 'SXC\\n', 'SXI\\n', 'SXT\\n', 'SYBT\\n', 'SYBX\\n', 'SYF\\n', 'SYK\\n', 'SYKE\\n', 'SYNA\\n', 'SYNC\\n', 'SYNH\\n', 'SYRS\\n', 'SYX\\n', 'SYY\\n', 'T\\n', 'TA\\n', 'TACO\\n', 'TACT\\n', 'TALO\\n', 'TAP\\n', 'TARA\\n', 'TARO\\n', 'TAST\\n', 'TBBK\\n', 'TBI\\n', 'TBIO\\n', 'TBK\\n', 'TBNK\\n', 'TBPH\\n', 'TCBI\\n', 'TCBK\\n', 'TCDA\\n', 'TCMD\\n', 'TCON\\n', 'TCRR\\n', 'TCS\\n', 'TCX\\n', 'TD\\n', 'TDC\\n', 'TDG\\n', 'TDOC\\n', 'TDS\\n', 'TDW\\n', 'TDY\\n', 'TEAM\\n', 'TECD\\n', 'TECH\\n', 'TECK\\n', 'TEL\\n', 'TELL\\n', 'TEN\\n', 'TENB\\n', 'TER\\n', 'TERP\\n', 'TESS\\n', 'TEX\\n', 'TFC\\n', 'TFSL\\n', 'TFX\\n', 'TG\\n', 'TGE\\n', 'TGEN\\n', 'TGH\\n', 'TGI\\n', 'TGLS\\n', 'TGNA\\n', 'TGP\\n', 'TGT\\n', 'TGTX\\n', 'THC\\n', 'THFF\\n', 'THG\\n', 'THMO\\n', 'THO\\n', 'THR\\n', 'THRM\\n', 'THS\\n', 'TIF\\n', 'TILE\\n', 'TISI\\n', 'TIVO\\n', 'TJX\\n', 'TKKS\\n', 'TKR\\n', 'TLRA\\n', 'TLRD\\n', 'TLRY\\n', 'TLYS\\n', 'TMHC\\n', 'TMO\\n', 'TMP\\n', 'TMST\\n', 'TMUS\\n', 'TNAV\\n', 'TNC\\n', 'TNDM\\n', 'TNET\\n', 'TNP\\n', 'TOL\\n', 'TORC\\n', 'TOWN\\n', 'TPC\\n', 'TPCO\\n', 'TPH\\n', 'TPIC\\n', 'TPR\\n', 'TPRE\\n', 'TPTX\\n', 'TPX\\n', 'TR\\n', 'TRC\\n', 'TREE\\n', 'TREX\\n', 'TRGP\\n', 'TRHC\\n', 'TRI\\n', 'TRIP\\n', 'TRMB\\n', 'TRMK\\n', 'TRN\\n', 'TROW\\n', 'TROX\\n', 'TRP\\n', 'TRQ\\n', 'TRS\\n', 'TRST\\n', 'TRT\\n', 'TRTN\\n', 'TRU\\n', 'TRUE\\n', 'TRUP\\n', 'TRV\\n', 'TRWH\\n', 'TRXC\\n', 'TSC\\n', 'TSCO\\n', 'TSEM\\n', 'TSG\\n', 'TSLA\\n', 'TSN\\n', 'TSQ\\n', 'TSRI\\n', 'TTC\\n', 'TTD\\n', 'TTEC\\n', 'TTEK\\n', 'TTGT\\n', 'TTMI\\n', 'TTOO\\n', 'TTPH\\n', 'TTWO\\n', 'TUFN\\n', 'TUP\\n', 'TUSK\\n', 'TVTY\\n', 'TW\\n', 'TWIN\\n', 'TWLO\\n', 'TWNK\\n', 'TWOU\\n', 'TWST\\n', 'TWTR\\n', 'TXG\\n', 'TXMD\\n', 'TXN\\n', 'TXRH\\n', 'TXT\\n', 'TYL\\n', 'TZOO\\n', 'UAA\\n', 'UAL\\n', 'UBER\\n', 'UBSI\\n', 'UBX\\n', 'UCBI\\n', 'UCTT\\n', 'UEIC\\n', 'UFCS\\n', 'UFI\\n', 'UFPI\\n', 'UFPT\\n', 'UFS\\n', 'UG\\n', 'UGI\\n', 'UHAL\\n', 'UHS\\n', 'UI\\n', 'UIHC\\n', 'UIS\\n', 'ULBI\\n', 'ULH\\n', 'ULTA\\n', 'UMBF\\n', 'UMPQ\\n', 'UNB\\n', 'UNF\\n', 'UNFI\\n', 'UNH\\n', 'UNM\\n', 'UNP\\n', 'UNT\\n', 'UNVR\\n', 'UPLD\\n', 'UPS\\n', 'UPWK\\n', 'URBN\\n', 'URGN\\n', 'URI\\n', 'UROV\\n', 'USAK\\n', 'USAP\\n', 'USAS\\n', 'USB\\n', 'USCR\\n', 'USFD\\n', 'USLM\\n', 'USM\\n', 'USNA\\n', 'USPH\\n', 'USX\\n', 'UTHR\\n', 'UTI\\n', 'UTL\\n', 'UTMD\\n', 'UTSI\\n', 'UTX\\n', 'UVE\\n', 'UVSP\\n', 'UVV\\n', 'V\\n', 'VAC\\n', 'VAL\\n', 'VALU\\n', 'VAPO\\n', 'VAR\\n', 'VBTX\\n', 'VC\\n', 'VCEL\\n', 'VCNX\\n', 'VCRA\\n', 'VCYT\\n', 'VEC\\n', 'VECO\\n', 'VEEV\\n', 'VERI\\n', 'VERU\\n', 'VFC\\n', 'VGR\\n', 'VHI\\n', 'VIAC\\n', 'VIAV\\n', 'VICR\\n', 'VIE\\n', 'VIR\\n', 'VIRT\\n', 'VIVE\\n', 'VIVO\\n', 'VKTX\\n', 'VLGEA\\n', 'VLO\\n', 'VLY\\n', 'VMC\\n', 'VMI\\n', 'VMW\\n', 'VNCE\\n', 'VNDA\\n', 'VNE\\n', 'VOXX\\n', 'VOYA\\n', 'VPG\\n', 'VRA\\n', 'VRAY\\n', 'VRCA\\n', 'VREX\\n', 'VRNS\\n', 'VRNT\\n', 'VRRM\\n', 'VRS\\n', 'VRSK\\n', 'VRSN\\n', 'VRTS\\n', 'VRTU\\n', 'VRTV\\n', 'VRTX\\n', 'VSAT\\n', 'VSEC\\n', 'VSH\\n', 'VSLR\\n', 'VST\\n', 'VSTO\\n', 'VUZI\\n', 'VVI\\n', 'VVUS\\n', 'VVV\\n', 'VYGR\\n', 'VZ\\n', 'W\\n', 'WAB\\n', 'WABC\\n', 'WAFD\\n', 'WAL\\n', 'WASH\\n', 'WAT\\n', 'WATT\\n', 'WBA\\n', 'WBS\\n', 'WBT\\n', 'WCC\\n', 'WCN\\n', 'WD\\n', 'WDAY\\n', 'WDC\\n', 'WDFC\\n', 'WDR\\n', 'WEC\\n', 'WEN\\n', 'WERN\\n', 'WETF\\n', 'WEX\\n', 'WEYS\\n', 'WFC\\n', 'WGO\\n', 'WH\\n', 'WHD\\n', 'WHG\\n', 'WHR\\n', 'WIFI\\n', 'WINA\\n', 'WING\\n', 'WINS\\n', 'WIRE\\n', 'WIX\\n', 'WK\\n', 'WLDN\\n', 'WLFC\\n', 'WLH\\n', 'WLK\\n', 'WLL\\n', 'WLTW\\n', 'WM\\n', 'WMB\\n', 'WMK\\n', 'WMS\\n', 'WMT\\n', 'WNC\\n', 'WNEB\\n', 'WOR\\n', 'WORK\\n', 'WORX\\n', 'WOW\\n', 'WPM\\n', 'WPRT\\n', 'WPX\\n', 'WRB\\n', 'WRK\\n', 'WRLD\\n', 'WRTC\\n', 'WSBC\\n', 'WSBF\\n', 'WSC\\n', 'WSFS\\n', 'WSM\\n', 'WSO\\n', 'WST\\n', 'WTBA\\n', 'WTFC\\n', 'WTI\\n', 'WTM\\n', 'WTR\\n', 'WTRE\\n', 'WTS\\n', 'WU\\n', 'WVE\\n', 'WVFC\\n', 'WVVI\\n', 'WW\\n', 'WWD\\n', 'WWE\\n', 'WWR\\n', 'WWW\\n', 'WYND\\n', 'WYNN\\n', 'X\\n', 'XAIR\\n', 'XBIT\\n', 'XEC\\n', 'XEL\\n', 'XENE\\n', 'XENT\\n', 'XLNX\\n', 'XLRN\\n', 'XNCR\\n', 'XOG\\n', 'XOM\\n', 'XOMA\\n', 'XON\\n', 'XONE\\n', 'XPEL\\n', 'XPER\\n', 'XPO\\n', 'XRAY\\n', 'XRX\\n', 'XYL\\n', 'Y\\n', 'YELP\\n', 'YETI\\n', 'YEXT\\n', 'YMAB\\n', 'YNDX\\n', 'YORW\\n', 'YRCW\\n', 'YUM\\n', 'YUMA\\n', 'YUMC\\n', 'ZAGG\\n', 'ZBH\\n', 'ZBRA\\n', 'ZEN\\n', 'ZFGN\\n', 'ZG\\n', 'ZGNX\\n', 'ZION\\n', 'ZIOP\\n', 'ZIXI\\n', 'ZM\\n', 'ZNGA\\n', 'ZS\\n', 'ZTS\\n', 'ZUMZ\\n', 'ZUO\\n', 'ZVO\\n', 'ZYME\\n', 'ZYNE\\n']\n",
      "['A', 'AA', 'AAL', 'AAN', 'AAOI', 'AAON', 'AAP', 'AAPL', 'AAWW', 'AAXN', 'ABBV', 'ABC', 'ABCB', 'ABEO', 'ABG', 'ABM', 'ABMD', 'ABT', 'ABTX', 'ACA', 'ACAD', 'ACCO', 'ACEL', 'ACGL', 'ACHC', 'ACHN', 'ACHV', 'ACIA', 'ACIW', 'ACLS', 'ACM', 'ACN', 'ACNB', 'ACOR', 'ACRS', 'ACRX', 'ACTG', 'ADBE', 'ADES', 'ADI', 'ADM', 'ADMA', 'ADMP', 'ADMS', 'ADP', 'ADPT', 'ADRO', 'ADS', 'ADSK', 'ADSW', 'ADT', 'ADTN', 'ADUS', 'ADVM', 'ADXS', 'AE', 'AEE', 'AEGN', 'AEIS', 'AEL', 'AEM', 'AEMD', 'AEO', 'AEP', 'AERI', 'AES', 'AFG', 'AFI', 'AFL', 'AG', 'AGCO', 'AGEN', 'AGFS', 'AGI', 'AGIO', 'AGLE', 'AGM', 'AGN', 'AGO', 'AGR', 'AGRX', 'AGS', 'AGTC', 'AGX', 'AGYS', 'AHC', 'AHCO', 'AIG', 'AIMC', 'AIMT', 'AIN', 'AIR', 'AIRG', 'AIRT', 'AIT', 'AIZ', 'AJG', 'AJRD', 'AKAM', 'AKBA', 'AKCA', 'AKRO', 'AKRX', 'AKS', 'AL', 'ALB', 'ALCO', 'ALDX', 'ALE', 'ALEC', 'ALG', 'ALGN', 'ALGT', 'ALIM', 'ALK', 'ALKS', 'ALL', 'ALLK', 'ALLO', 'ALLY', 'ALNY', 'ALOT', 'ALPN', 'ALRM', 'ALRN', 'ALSK', 'ALSN', 'ALT', 'ALTR', 'ALV', 'ALXN', 'AM', 'AMAG', 'AMAL', 'AMAT', 'AMBA', 'AMBC', 'AMC', 'AMCX', 'AMD', 'AME', 'AMED', 'AMEH', 'AMG', 'AMGN', 'AMK', 'AMKR', 'AMN', 'AMNB', 'AMOT', 'AMP', 'AMPE', 'AMPH', 'AMRC', 'AMRS', 'AMRX', 'AMSC', 'AMSF', 'AMSWA', 'AMTB', 'AMTD', 'AMTX', 'AMWD', 'AMZN', 'AN', 'ANAB', 'ANAT', 'ANDA', 'ANDE', 'ANET', 'ANF', 'ANGI', 'ANGO', 'ANIK', 'ANIP', 'ANIX', 'ANSS', 'ANTM', 'AOBC', 'AON', 'AOS', 'AP', 'APA', 'APD', 'APDN', 'APEI', 'APEN', 'APH', 'APLS', 'APLT', 'APOG', 'APPF', 'APPN', 'APPS', 'APRE', 'APRN', 'APT', 'APTV', 'APY', 'AQB', 'AQN', 'AQUA', 'AR', 'ARA', 'ARAV', 'ARAY', 'ARCB', 'ARCH', 'ARCO', 'ARCT', 'ARDS', 'ARDX', 'ARES', 'ARGO', 'ARKR', 'ARLO', 'ARMK', 'ARMP', 'ARNA', 'ARNC', 'AROC', 'AROW', 'ARTNA', 'ARVN', 'ARW', 'ARWR', 'ASB', 'ASFI', 'ASGN', 'ASH', 'ASIX', 'ASMB', 'ASNA', 'ASPS', 'ASPU', 'ASRT', 'ASTE', 'ASYS', 'ATEC', 'ATEN', 'ATEX', 'ATGE', 'ATH', 'ATHX', 'ATI', 'ATKR', 'ATLO', 'ATNI', 'ATNX', 'ATO', 'ATR', 'ATRA', 'ATRC', 'ATRI', 'ATRO', 'ATRS', 'ATSG', 'ATUS', 'ATVI', 'AUB', 'AUMN', 'AUPH', 'AUTO', 'AUY', 'AVA', 'AVAV', 'AVD', 'AVGO', 'AVID', 'AVLR', 'AVNS', 'AVNW', 'AVRO', 'AVT', 'AVTR', 'AVX', 'AVXL', 'AVY', 'AVYA', 'AWI', 'AWK', 'AWR', 'AWRE', 'AX', 'AXAS', 'AXDX', 'AXGN', 'AXGT', 'AXL', 'AXLA', 'AXNX', 'AXP', 'AXS', 'AXSM', 'AXTA', 'AXTI', 'AYI', 'AYX', 'AZO', 'AZPN', 'AZZ', 'B', 'BA', 'BAC', 'BAH', 'BAM', 'BANC', 'BAND', 'BANF', 'BANR', 'BAP', 'BAX', 'BB', 'BBBY', 'BBI', 'BBIO', 'BBQ', 'BBW', 'BBY', 'BC', 'BCBP', 'BCC', 'BCE', 'BCEI', 'BCEL', 'BCLI', 'BCO', 'BCOR', 'BCOV', 'BCPC', 'BCRX', 'BDC', 'BDGE', 'BDSI', 'BDX', 'BE', 'BEAT', 'BECN', 'BELFB', 'BEN', 'BERY', 'BFAM', 'BFC', 'BFIN', 'BG', 'BGCP', 'BGFV', 'BGG', 'BGS', 'BH', 'BHB', 'BHC', 'BHE', 'BHF', 'BHLB', 'BIG', 'BIIB', 'BIMI', 'BIO', 'BIOS', 'BJ', 'BJRI', 'BK', 'BKD', 'BKE', 'BKH', 'BKI', 'BKNG', 'BKR', 'BKU', 'BL', 'BLBD', 'BLCM', 'BLD', 'BLDP', 'BLDR', 'BLFS', 'BLK', 'BLKB', 'BLL', 'BLMN', 'BLNK', 'BLUE', 'BLX', 'BMCH', 'BMI', 'BMO', 'BMRC', 'BMRN', 'BMTC', 'BMY', 'BNED', 'BNFT', 'BNGO', 'BNS', 'BOH', 'BOKF', 'BOMN', 'BOOM', 'BOOT', 'BOX', 'BPFH', 'BPMC', 'BPTH', 'BR', 'BRC', 'BRKB', 'BRKL', 'BRKR', 'BRKS', 'BRMK', 'BRO', 'BRY', 'BSET', 'BSIG', 'BSQR', 'BSRR', 'BSTC', 'BSX', 'BTG', 'BURL', 'BUSE', 'BV', 'BWA', 'BWEN', 'BWXT', 'BX', 'BXC', 'BXG', 'BXS', 'BY', 'BYD', 'BYND', 'BZH', 'C', 'CABO', 'CAC', 'CACC', 'CACI', 'CADE', 'CAG', 'CAH', 'CAI', 'CAKE', 'CAL', 'CALA', 'CALM', 'CALX', 'CAMP', 'CAPR', 'CAR', 'CARA', 'CARE', 'CARG', 'CARS', 'CASA', 'CASH', 'CASI', 'CASS', 'CASY', 'CAT', 'CATB', 'CATM', 'CATO', 'CATY', 'CBIO', 'CBMG', 'CBPO', 'CBPX', 'CBRE', 'CBRL', 'CBSH', 'CBT', 'CBTX', 'CBU', 'CBZ', 'CC', 'CCBG', 'CCEP', 'CCF', 'CCJ', 'CCK', 'CCL', 'CCMP', 'CCNE', 'CCO', 'CCOI', 'CCRN', 'CCS', 'CCXI', 'CDAY', 'CDE', 'CDK', 'CDLX', 'CDMO', 'CDNA', 'CDNS', 'CDW', 'CDXS', 'CE', 'CECE', 'CEIX', 'CEL', 'CELH', 'CEMI', 'CENT', 'CENTA', 'CENX', 'CERN', 'CERS', 'CEVA', 'CF', 'CFB', 'CFFI', 'CFFN', 'CFG', 'CFMS', 'CFR', 'CFX', 'CGC', 'CGEN', 'CGNX', 'CHCO', 'CHD', 'CHDN', 'CHE', 'CHEF', 'CHGG', 'CHH', 'CHK', 'CHKP', 'CHMA', 'CHMG', 'CHNG', 'CHRS', 'CHRW', 'CHS', 'CHTR', 'CHUY', 'CHWY', 'CI', 'CIEN', 'CINF', 'CIR', 'CIT', 'CKH', 'CKPT', 'CL', 'CLAR', 'CLBK', 'CLBS', 'CLCT', 'CLDR', 'CLDX', 'CLF', 'CLFD', 'CLGX', 'CLH', 'CLNE', 'CLPS', 'CLR', 'CLSD', 'CLSN', 'CLVS', 'CLW', 'CLX', 'CLXT', 'CM', 'CMA', 'CMBM', 'CMC', 'CMCO', 'CMCSA', 'CMD', 'CME', 'CMG', 'CMI', 'CMLS', 'CMP', 'CMS', 'CMTL', 'CNA', 'CNBKA', 'CNC', 'CNCE', 'CNDT', 'CNI', 'CNK', 'CNMD', 'CNNE', 'CNO', 'CNOB', 'CNP', 'CNQ', 'CNR', 'CNS', 'CNSL', 'CNST', 'CNX', 'CNXN', 'CO', 'COF', 'COG', 'COHR', 'COHU', 'COKE', 'COLB', 'COLL', 'COLM', 'COMM', 'CONN', 'COO', 'COOP', 'COP', 'CORE', 'CORT', 'COST', 'COT', 'COTY', 'COUP', 'CP', 'CPA', 'CPB', 'CPE', 'CPF', 'CPG', 'CPIX', 'CPK', 'CPRI', 'CPRT', 'CPRX', 'CPS', 'CPSI', 'CPST', 'CR', 'CRAI', 'CRBP', 'CRC', 'CRCM', 'CREE', 'CRI', 'CRIS', 'CRK', 'CRL', 'CRM', 'CRMD', 'CRMT', 'CRNC', 'CRNT', 'CRNX', 'CRON', 'CROX', 'CRS', 'CRTX', 'CRUS', 'CRVL', 'CRVS', 'CRWD', 'CRWS', 'CRY', 'CSBR', 'CSCO', 'CSFL', 'CSGP', 'CSGS', 'CSII', 'CSIQ', 'CSL', 'CSOD', 'CSPI', 'CSS', 'CSSE', 'CSTL', 'CSU', 'CSV', 'CSWI', 'CSX', 'CTAS', 'CTB', 'CTBI', 'CTG', 'CTIC', 'CTL', 'CTLT', 'CTMX', 'CTO', 'CTRA', 'CTRN', 'CTS', 'CTSH', 'CTSO', 'CTVA', 'CTXS', 'CUB', 'CUBI', 'CULP', 'CURO', 'CUTR', 'CVA', 'CVBF', 'CVCO', 'CVE', 'CVET', 'CVGW', 'CVI', 'CVLT', 'CVLY', 'CVM', 'CVNA', 'CVS', 'CVTI', 'CVU', 'CVX', 'CW', 'CWBC', 'CWCO', 'CWEN', 'CWH', 'CWK', 'CWST', 'CWT', 'CXDC', 'CXO', 'CY', 'CYAN', 'CYBE', 'CYBR', 'CYCN', 'CYD', 'CYH', 'CYRX', 'CYTK', 'CZNC', 'CZR', 'CZWI', 'CZZ', 'D', 'DAIO', 'DAKT', 'DAL', 'DAN', 'DAR', 'DARE', 'DB', 'DBD', 'DBI', 'DBX', 'DCI', 'DCO', 'DCOM', 'DCPH', 'DD', 'DDD', 'DDOG', 'DDS', 'DE', 'DECK', 'DELL', 'DENN', 'DERM', 'DFIN', 'DFS', 'DG', 'DGICA', 'DGII', 'DGLY', 'DGX', 'DHI', 'DHIL', 'DHR', 'DHT', 'DIN', 'DIOD', 'DIS', 'DISCA', 'DISCK', 'DISH', 'DJCO', 'DK', 'DKS', 'DLA', 'DLB', 'DLTR', 'DLX', 'DMRC', 'DNKN', 'DNLI', 'DNOW', 'DNR', 'DO', 'DOCU', 'DOMO', 'DOOR', 'DORM', 'DOV', 'DOW', 'DPLO', 'DPZ', 'DRAD', 'DRI', 'DRNA', 'DRQ', 'DSGX', 'DSKE', 'DSPG', 'DT', 'DTE', 'DTIL', 'DUK', 'DVA', 'DVAX', 'DVD', 'DVN', 'DXC', 'DXCM', 'DXPE', 'DXR', 'DY', 'DZSI', 'EA', 'EAF', 'EAT', 'EB', 'EBAY', 'EBF', 'EBIX', 'EBS', 'EBSB', 'EBTC', 'ECHO', 'ECL', 'ECOL', 'ECOM', 'ECPG', 'ED', 'EDIT', 'EDSA', 'EDTX', 'EDUC', 'EE', 'EEFT', 'EEX', 'EFC', 'EFSC', 'EFX', 'EGAN', 'EGBN', 'EGHT', 'EGO', 'EGOV', 'EGRX', 'EGY', 'EHC', 'EHTH', 'EIDX', 'EIG', 'EIGI', 'EIGR', 'EIX', 'EKSO', 'EL', 'ELAN', 'ELF', 'ELGX', 'ELY', 'EMCF', 'EME', 'EMKR', 'EML', 'EMMS', 'EMN', 'EMR', 'ENB', 'ENDP', 'ENLV', 'ENPH', 'ENR', 'ENS', 'ENSG', 'ENTA', 'ENTG', 'ENV', 'ENVA', 'EOG', 'EOLS', 'EPAC', 'EPAM', 'EPAY', 'EPC', 'EPIX', 'EPM', 'EPZM', 'EQH', 'EQT', 'ERA', 'ERF', 'ERI', 'ERIE', 'ERII', 'ES', 'ESCA', 'ESE', 'ESGR', 'ESLT', 'ESNT', 'ESPR', 'ESSA', 'ETFC', 'ETH', 'ETM', 'ETN', 'ETNB', 'ETR', 'ETSY', 'EV', 'EVBG', 'EVC', 'EVER', 'EVFM', 'EVH', 'EVOP', 'EVR', 'EVRG', 'EVRI', 'EW', 'EWBC', 'EXAS', 'EXC', 'EXEL', 'EXK', 'EXLS', 'EXP', 'EXPD', 'EXPE', 'EXPI', 'EXPO', 'EXPR', 'EXTR', 'EYE', 'EYPT', 'EZPW', 'F', 'FAF', 'FANG', 'FARM', 'FARO', 'FAST', 'FAT', 'FATE', 'FB', 'FBC', 'FBHS', 'FBIZ', 'FBK', 'FBM', 'FBMS', 'FBNC', 'FC', 'FCBC', 'FCEL', 'FCF', 'FCFS', 'FCN', 'FCNCA', 'FCX', 'FDEF', 'FDP', 'FDS', 'FDX', 'FE', 'FEIM', 'FELE', 'FEYE', 'FF', 'FFBC', 'FFG', 'FFIC', 'FFIN', 'FFIV', 'FFWM', 'FG', 'FGEN', 'FHB', 'FHN', 'FIBK', 'FICO', 'FII', 'FIS', 'FISI', 'FISV', 'FIT', 'FITB', 'FIVE', 'FIVN', 'FIX', 'FIXX', 'FIZZ', 'FL', 'FLDM', 'FLEX', 'FLGT', 'FLIC', 'FLIR', 'FLMN', 'FLNT', 'FLO', 'FLOW', 'FLR', 'FLS', 'FLT', 'FLWS', 'FLXN', 'FLXS', 'FMBH', 'FMBI', 'FMC', 'FMNB', 'FN', 'FNB', 'FND', 'FNF', 'FNHC', 'FNJN', 'FNKO', 'FNLC', 'FNV', 'FOCS', 'FOE', 'FOLD', 'FOMX', 'FONR', 'FOR', 'FORM', 'FORR', 'FOSL', 'FOX', 'FOXA', 'FOXF', 'FPRX', 'FRAN', 'FRC', 'FREQ', 'FRGI', 'FRHC', 'FRME', 'FRO', 'FRPH', 'FRPT', 'FRTA', 'FSB', 'FSBW', 'FSCT', 'FSFG', 'FSI', 'FSLR', 'FSLY', 'FSM', 'FSS', 'FSTR', 'FTCH', 'FTDR', 'FTEK', 'FTI', 'FTNT', 'FTR', 'FTS', 'FTSV', 'FTV', 'FUL', 'FULC', 'FULT', 'FVE', 'FVRR', 'FWONA', 'FWRD', 'GABC', 'GALT', 'GATX', 'GBCI', 'GBL', 'GBLI', 'GBT', 'GBX', 'GCAP', 'GCBC', 'GCI', 'GCO', 'GCP', 'GD', 'GDDY', 'GDEN', 'GDI', 'GDOT', 'GE', 'GEC', 'GEF', 'GEOS', 'GERN', 'GES', 'GEVO', 'GFF', 'GGG', 'GH', 'GHC', 'GHL', 'GHM', 'GIB', 'GIFI', 'GIII', 'GIL', 'GILD', 'GIS', 'GKOS', 'GL', 'GLDD', 'GLMD', 'GLNG', 'GLRE', 'GLT', 'GLUU', 'GLW', 'GLYC', 'GM', 'GME', 'GMED', 'GMS', 'GNC', 'GNCA', 'GNE', 'GNMK', 'GNMX', 'GNRC', 'GNTX', 'GNW', 'GO', 'GOGO', 'GOLD', 'GOLF', 'GOOG', 'GOOGL', 'GOOS', 'GORO', 'GOSS', 'GPC', 'GPI', 'GPK', 'GPN', 'GPOR', 'GPRE', 'GPRK', 'GPRO', 'GPS', 'GPX', 'GRA', 'GRBK', 'GRC', 'GRIF', 'GRPN', 'GRTS', 'GRUB', 'GS', 'GSB', 'GSBC', 'GSHD', 'GSKY', 'GT', 'GTES', 'GTHX', 'GTLS', 'GTN', 'GTT', 'GTX', 'GVA', 'GWB', 'GWRE', 'GWRS', 'GWW', 'H', 'HA', 'HABT', 'HAE', 'HAFC', 'HAIN', 'HAL', 'HALO', 'HARP', 'HAS', 'HAYN', 'HBAN', 'HBB', 'HBCP', 'HBI', 'HBIO', 'HBM', 'HBNC', 'HBT', 'HCA', 'HCAT', 'HCC', 'HCCI', 'HCI', 'HCKT', 'HCSG', 'HD', 'HDS', 'HDSN', 'HE', 'HEAR', 'HEES', 'HEI', 'HELE', 'HEPA', 'HES', 'HFBL', 'HFC', 'HFFG', 'HFWA', 'HGV', 'HHC', 'HHS', 'HI', 'HIBB', 'HIFS', 'HIG', 'HII', 'HIIQ', 'HJLI', 'HL', 'HLF', 'HLI', 'HLIO', 'HLIT', 'HLT', 'HLX', 'HMHC', 'HMN', 'HMST', 'HMSY', 'HMTV', 'HNGR', 'HNI', 'HOFT', 'HOG', 'HOLX', 'HOMB', 'HOME', 'HON', 'HONE', 'HOPE', 'HOV', 'HP', 'HPE', 'HPQ', 'HQY', 'HRB', 'HRC', 'HRI', 'HRL', 'HROW', 'HRTG', 'HRTX', 'HSC', 'HSIC', 'HSII', 'HSKA', 'HSTM', 'HSY', 'HTBI', 'HTBK', 'HTGM', 'HTH', 'HTLD', 'HTLF', 'HUBB', 'HUBG', 'HUBS', 'HUD', 'HUM', 'HUN', 'HURC', 'HURN', 'HVT', 'HWC', 'HWCC', 'HWKN', 'HXL', 'HY', 'HYRE', 'HZO', 'IAA', 'IAC', 'IAG', 'IART', 'IBCP', 'IBKC', 'IBKR', 'IBM', 'IBOC', 'IBP', 'IBTX', 'ICBK', 'ICE', 'ICFI', 'ICHR', 'ICON', 'ICPT', 'ICUI', 'IDA', 'IDCC', 'IDRA', 'IDT', 'IDXX', 'IESC', 'IEX', 'IFF', 'IGMS', 'IGT', 'IHC', 'IIIN', 'IIIV', 'IIVI', 'ILMN', 'IMAX', 'IMGN', 'IMH', 'IMKTA', 'IMMR', 'IMMU', 'IMUX', 'IMXI', 'INAP', 'INBK', 'INCY', 'INDB', 'INFN', 'INFO', 'INGN', 'INGR', 'INMD', 'INO', 'INOD', 'INOV', 'INS', 'INSG', 'INSM', 'INSP', 'INST', 'INT', 'INTC', 'INTL', 'INTU', 'INVA', 'INVE', 'IO', 'IONS', 'IOSP', 'IOTS', 'IOVA', 'IP', 'IPAR', 'IPG', 'IPGP', 'IPHI', 'IPHS', 'IPI', 'IPWR', 'IQV', 'IR', 'IRBT', 'IRDM', 'IRMD', 'IRTC', 'IRWD', 'ISBC', 'ISEE', 'ISNS', 'ISRG', 'IT', 'ITCI', 'ITGR', 'ITIC', 'ITRI', 'ITT', 'ITW', 'IVC', 'IVZ', 'J', 'JACK', 'JAZZ', 'JBHT', 'JBL', 'JBLU', 'JBSS', 'JBT', 'JCI', 'JCOM', 'JCP', 'JEF', 'JELD', 'JILL', 'JJSF', 'JKHY', 'JLL', 'JNCE', 'JNJ', 'JNPR', 'JOE', 'JOUT', 'JPM', 'JRVR', 'JVA', 'JWN', 'JYNT', 'K', 'KAI', 'KALA', 'KALU', 'KALV', 'KAMN', 'KAR', 'KBH', 'KBR', 'KDMN', 'KDP', 'KE', 'KELYA', 'KEM', 'KEX', 'KEY', 'KEYS', 'KFRC', 'KFS', 'KFY', 'KGC', 'KHC', 'KIDS', 'KIRK', 'KL', 'KLAC', 'KLIC', 'KMB', 'KMI', 'KMPR', 'KMT', 'KMX', 'KN', 'KNDI', 'KNL', 'KNSA', 'KNSL', 'KNX', 'KO', 'KOD', 'KODK', 'KOP', 'KOPN', 'KOS', 'KPTI', 'KR', 'KRA', 'KRNT', 'KRNY', 'KRO', 'KRTX', 'KRUS', 'KRYS', 'KSS', 'KSU', 'KTB', 'KTCC', 'KTOS', 'KURA', 'KVHI', 'KW', 'KWR', 'L', 'LAD', 'LAKE', 'LANC', 'LASR', 'LAUR', 'LAWS', 'LB', 'LBAI', 'LBC', 'LBRDA', 'LBRDK', 'LBRT', 'LBTYA', 'LBTYK', 'LBY', 'LC', 'LCI', 'LCII', 'LCNB', 'LCUT', 'LDL', 'LDOS', 'LE', 'LEA', 'LEAF', 'LECO', 'LEG', 'LEGH', 'LEN', 'LEVI', 'LFUS', 'LFVN', 'LGIH', 'LGND', 'LH', 'LHCG', 'LHX', 'LII', 'LILA', 'LILAK', 'LIN', 'LINC', 'LIND', 'LITE', 'LIVE', 'LIVN', 'LJPC', 'LKFN', 'LKQ', 'LL', 'LLNW', 'LLY', 'LM', 'LMAT', 'LMNR', 'LMNX', 'LMT', 'LNC', 'LNDC', 'LNG', 'LNN', 'LNT', 'LNTH', 'LOB', 'LOCO', 'LOGC', 'LOGM', 'LOOP', 'LOPE', 'LORL', 'LOVE', 'LOW', 'LPCN', 'LPI', 'LPLA', 'LPSN', 'LPX', 'LQDA', 'LQDT', 'LRCX', 'LRN', 'LSCC', 'LSTR', 'LTHM', 'LTRPA', 'LTS', 'LULU', 'LUNA', 'LUV', 'LVGO', 'LVS', 'LW', 'LWAY', 'LXRX', 'LXU', 'LYFT', 'LYTS', 'LYV', 'LZB', 'M', 'MA', 'MACK', 'MAGS', 'MAN', 'MANH', 'MANT', 'MANU', 'MAR', 'MARA', 'MAS', 'MASI', 'MAT', 'MATW', 'MATX', 'MAXR', 'MBI', 'MBIN', 'MBIO', 'MBOT', 'MBUU', 'MBWM', 'MC', 'MCD', 'MCF', 'MCHP', 'MCHX', 'MCK', 'MCO', 'MCRB', 'MCRI', 'MCS', 'MCY', 'MD', 'MDB', 'MDC', 'MDGL', 'MDLA', 'MDLZ', 'MDP', 'MDRX', 'MDT', 'MDU', 'MDWD', 'MEC', 'MED', 'MEDP', 'MEET', 'MEI', 'MELI', 'MEOH', 'MERC', 'MESA', 'MET', 'MFC', 'MFIN', 'MFSF', 'MG', 'MGA', 'MGEE', 'MGI', 'MGIC', 'MGLN', 'MGM', 'MGNX', 'MGPI', 'MGRC', 'MGTA', 'MGTX', 'MGY', 'MHK', 'MHO', 'MIC', 'MIDD', 'MIK', 'MIME', 'MINI', 'MIRM', 'MIST', 'MITK', 'MKC', 'MKL', 'MKSI', 'MKTX', 'MLAB', 'MLHR', 'MLI', 'MLM', 'MLND', 'MLNT', 'MLNX', 'MLR', 'MMC', 'MMM', 'MMS', 'MMSI', 'MMYT', 'MNI', 'MNK', 'MNKD', 'MNLO', 'MNOV', 'MNRO', 'MNST', 'MNTA', 'MO', 'MOBL', 'MOD', 'MODN', 'MOFG', 'MOH', 'MORF', 'MORN', 'MOS', 'MOV', 'MPAA', 'MPC', 'MPWR', 'MPX', 'MRAM', 'MRC', 'MRCY', 'MRIN', 'MRK', 'MRKR', 'MRLN', 'MRNA', 'MRNS', 'MRO', 'MRTN', 'MRTX', 'MRVL', 'MS', 'MSA', 'MSBI', 'MSCI', 'MSEX', 'MSFT', 'MSG', 'MSGN', 'MSI', 'MSM', 'MSON', 'MSTR', 'MTB', 'MTBC', 'MTCH', 'MTD', 'MTDR', 'MTEM', 'MTEX', 'MTG', 'MTH', 'MTN', 'MTOR', 'MTRN', 'MTRX', 'MTSC', 'MTSI', 'MTW', 'MTX', 'MTZ', 'MU', 'MUR', 'MUSA', 'MUX', 'MVIS', 'MWA', 'MXIM', 'MXL', 'MYE', 'MYGN', 'MYL', 'MYOK', 'MYOV', 'MYRG', 'NAII', 'NAT', 'NATH', 'NATI', 'NATR', 'NAV', 'NAVI', 'NBEV', 'NBHC', 'NBIX', 'NBL', 'NBR', 'NBSE', 'NBTB', 'NC', 'NCBS', 'NCLH', 'NCMI', 'NCR', 'NDAQ', 'NDLS', 'NDSN', 'NEE', 'NEM', 'NEO', 'NEOG', 'NEON', 'NEP', 'NERV', 'NET', 'NETE', 'NEU', 'NEWR', 'NEXT', 'NFBK', 'NFE', 'NFG', 'NFLX', 'NG', 'NGHC', 'NGM', 'NGS', 'NGVC', 'NGVT', 'NHC', 'NHTC', 'NI', 'NJR', 'NK', 'NKE', 'NKSH', 'NKTR', 'NL', 'NLNK', 'NLOK', 'NLSN', 'NLTX', 'NMIH', 'NMRK', 'NNBR', 'NNI', 'NOC', 'NOV', 'NOVA', 'NOVN', 'NOVT', 'NOW', 'NP', 'NPK', 'NPO', 'NPTN', 'NR', 'NRC', 'NRG', 'NRIM', 'NSC', 'NSIT', 'NSP', 'NSSC', 'NSTG', 'NTAP', 'NTB', 'NTCT', 'NTGR', 'NTLA', 'NTNX', 'NTR', 'NTRA', 'NTRS', 'NTUS', 'NTWK', 'NUAN', 'NUE', 'NUS', 'NUVA', 'NVAX', 'NVCN', 'NVCR', 'NVDA', 'NVEC', 'NVEE', 'NVMI', 'NVR', 'NVRO', 'NVST', 'NVTA', 'NWBI', 'NWE', 'NWFL', 'NWHM', 'NWL', 'NWLI', 'NWN', 'NWPX', 'NWS', 'NWSA', 'NX', 'NXGN', 'NXPI', 'NXST', 'NXTC', 'NYCB', 'NYT', 'OAS', 'OBCI', 'OBNK', 'OC', 'OCFC', 'OCGN', 'OCN', 'OCUL', 'ODC', 'ODFL', 'ODP', 'ODT', 'OFIX', 'OFLX', 'OGE', 'OGS', 'OI', 'OII', 'OIS', 'OKE', 'OKTA', 'OLED', 'OLLI', 'OLN', 'OMC', 'OMCL', 'OMER', 'OMEX', 'OMF', 'OMI', 'ON', 'ONB', 'ONCS', 'ONCT', 'ONDK', 'ONTO', 'ONTX', 'OOMA', 'OPB', 'OPES', 'OPGN', 'OPK', 'OPRT', 'OPTN', 'OPTT', 'OPY', 'ORA', 'ORBC', 'ORCC', 'ORCL', 'ORI', 'ORLY', 'ORMP', 'ORRF', 'OSIS', 'OSK', 'OSPN', 'OSTK', 'OSUR', 'OTEX', 'OTIC', 'OTTR', 'OVV', 'OXM', 'OXY', 'OZK', 'P', 'PAAS', 'PACB', 'PACQ', 'PACW', 'PAG', 'PAGP', 'PAGS', 'PAH', 'PAHC', 'PANW', 'PAR', 'PARR', 'PATK', 'PAYC', 'PAYS', 'PAYX', 'PB', 'PBCT', 'PBF', 'PBH', 'PBI', 'PBPB', 'PBYI', 'PCAR', 'PCG', 'PCOM', 'PCRX', 'PCTI', 'PCTY', 'PCYG', 'PD', 'PDCE', 'PDCO', 'PDFS', 'PDLI', 'PE', 'PEBO', 'PEG', 'PEGA', 'PEGI', 'PEIX', 'PEN', 'PENN', 'PEP', 'PERI', 'PETQ', 'PETS', 'PFBC', 'PFBI', 'PFE', 'PFG', 'PFGC', 'PFIS', 'PFNX', 'PFPT', 'PFS', 'PFSI', 'PFSW', 'PG', 'PGC', 'PGNX', 'PGNY', 'PGR', 'PGTI', 'PH', 'PHAS', 'PHAT', 'PHM', 'PHR', 'PHX', 'PI', 'PICO', 'PII', 'PINC', 'PING', 'PINS', 'PIR', 'PIRS', 'PJT', 'PKE', 'PKG', 'PKI', 'PKOH', 'PLAB', 'PLAN', 'PLAY', 'PLCE', 'PLIN', 'PLMR', 'PLNT', 'PLOW', 'PLPC', 'PLSE', 'PLT', 'PLUG', 'PLUS', 'PLXP', 'PLXS', 'PLYA', 'PM', 'PMD', 'PME', 'PNC', 'PNFP', 'PNM', 'PNR', 'PNRG', 'PNTG', 'PNW', 'PODD', 'POL', 'POOL', 'POR', 'POST', 'POWI', 'POWL', 'PPBI', 'PPC', 'PPG', 'PPIH', 'PPL', 'PPSI', 'PQG', 'PRA', 'PRAA', 'PRAH', 'PRCP', 'PRFT', 'PRGO', 'PRGS', 'PRI', 'PRIM', 'PRK', 'PRLB', 'PRMW', 'PRNB', 'PRO', 'PROS', 'PROV', 'PRPL', 'PRSC', 'PRSP', 'PRTA', 'PRTK', 'PRTY', 'PRU', 'PRVB', 'PRVL', 'PS', 'PSMT', 'PSN', 'PSNL', 'PSTG', 'PSTI', 'PSTV', 'PSX', 'PTC', 'PTCT', 'PTEN', 'PTGX', 'PTI', 'PTLA', 'PTON', 'PTSI', 'PTVCB', 'PUB', 'PUMP', 'PVG', 'PVH', 'PWOD', 'PWR', 'PXD', 'PXLW', 'PYPL', 'PZN', 'PZZA', 'QADA', 'QCOM', 'QCRH', 'QDEL', 'QEP', 'QLYS', 'QNST', 'QRTEA', 'QRVO', 'QSR', 'QTRX', 'QTWO', 'QUAD', 'QUIK', 'QUMU', 'QUOT', 'R', 'RACE', 'RAD', 'RADA', 'RAIL', 'RAMP', 'RAPT', 'RARE', 'RAVE', 'RAVN', 'RBA', 'RBBN', 'RBC', 'RBCAA', 'RBCN', 'RCI', 'RCII', 'RCKT', 'RCKY', 'RCL', 'RCM', 'RCUS', 'RDFN', 'RDI', 'RDN', 'RDNT', 'RDUS', 'RDWR', 'RE', 'REAL', 'RECN', 'REGI', 'REGN', 'RELL', 'REPH', 'REPL', 'RES', 'RESN', 'RETA', 'REV', 'REVG', 'REX', 'REZI', 'RF', 'RFIL', 'RFL', 'RFP', 'RGA', 'RGEN', 'RGLD', 'RGNX', 'RGR', 'RGS', 'RH', 'RHI', 'RICK', 'RILY', 'RIOT', 'RJF', 'RL', 'RLGY', 'RLH', 'RLI', 'RM', 'RMAX', 'RMBS', 'RMCF', 'RMD', 'RMNI', 'RMR', 'RMTI', 'RNET', 'RNG', 'RNR', 'RNST', 'RNWK', 'ROCK', 'ROG', 'ROK', 'ROKU', 'ROL', 'ROLL', 'ROP', 'ROST', 'RP', 'RPAY', 'RPD', 'RPM', 'RRC', 'RRD', 'RRGB', 'RRR', 'RRTS', 'RS', 'RSG', 'RST', 'RTIX', 'RTN', 'RTRX', 'RUBI', 'RUBY', 'RUN', 'RUSHA', 'RUTH', 'RVLV', 'RVNC', 'RWLK', 'RXN', 'RY', 'RYI', 'RYTM', 'S', 'SA', 'SABR', 'SAFM', 'SAFT', 'SAGE', 'SAH', 'SAIA', 'SAIC', 'SAIL', 'SAM', 'SANM', 'SANW', 'SASR', 'SATS', 'SAVE', 'SBCF', 'SBGI', 'SBH', 'SBNY', 'SBPH', 'SBSI', 'SBT', 'SBUX', 'SC', 'SCCO', 'SCHL', 'SCHN', 'SCHW', 'SCI', 'SCKT', 'SCL', 'SCOR', 'SCPL', 'SCS', 'SCSC', 'SCU', 'SCVL', 'SCWX', 'SCX', 'SDC', 'SDRL', 'SEAS', 'SEB', 'SEDG', 'SEE', 'SEIC', 'SEM', 'SENEA', 'SERV', 'SF', 'SFBS', 'SFE', 'SFIX', 'SFM', 'SFNC', 'SG', 'SGA', 'SGBX', 'SGC', 'SGEN', 'SGH', 'SGMO', 'SGMS', 'SGRY', 'SGU', 'SHAK', 'SHEN', 'SHLD', 'SHLO', 'SHOO', 'SHOP', 'SHW', 'SIBN', 'SIEN', 'SIF', 'SIG', 'SIGA', 'SIGI', 'SILK', 'SINA', 'SINT', 'SIRI', 'SITE', 'SIVB', 'SIX', 'SJI', 'SJM', 'SJR', 'SJW', 'SKX', 'SKY', 'SKYW', 'SLAB', 'SLB', 'SLCA', 'SLCT', 'SLDB', 'SLF', 'SLGG', 'SLGN', 'SLM', 'SLP', 'SLRX', 'SM', 'SMAR', 'SMBC', 'SMED', 'SMG', 'SMMF', 'SMP', 'SMPL', 'SMTC', 'SMTX', 'SNA', 'SNAP', 'SNBR', 'SNCR', 'SND', 'SNDX', 'SNOA', 'SNPS', 'SNSS', 'SNV', 'SNX', 'SO', 'SOI', 'SON', 'SONM', 'SONO', 'SORL', 'SP', 'SPAR', 'SPB', 'SPCE', 'SPFI', 'SPGI', 'SPKE', 'SPLK', 'SPN', 'SPNE', 'SPNS', 'SPOK', 'SPOT', 'SPPI', 'SPR', 'SPSC', 'SPTN', 'SPWR', 'SPXC', 'SQ', 'SR', 'SRAX', 'SRCE', 'SRCL', 'SRDX', 'SRE', 'SRI', 'SRL', 'SRPT', 'SRRK', 'SSB', 'SSD', 'SSI', 'SSNC', 'SSP', 'SSRM', 'SSTI', 'SSTK', 'SSYS', 'ST', 'STAA', 'STBA', 'STC', 'STE', 'STFC', 'STIM', 'STL', 'STLD', 'STMP', 'STNE', 'STOK', 'STRA', 'STRL', 'STRO', 'STRS', 'STRT', 'STSA', 'STT', 'STX', 'STZ', 'SU', 'SUM', 'SUP', 'SUPN', 'SVMK', 'SWAV', 'SWCH', 'SWIR', 'SWK', 'SWKS', 'SWM', 'SWN', 'SWTX', 'SWX', 'SXC', 'SXI', 'SXT', 'SYBT', 'SYBX', 'SYF', 'SYK', 'SYKE', 'SYNA', 'SYNC', 'SYNH', 'SYRS', 'SYX', 'SYY', 'T', 'TA', 'TACO', 'TACT', 'TALO', 'TAP', 'TARA', 'TARO', 'TAST', 'TBBK', 'TBI', 'TBIO', 'TBK', 'TBNK', 'TBPH', 'TCBI', 'TCBK', 'TCDA', 'TCMD', 'TCON', 'TCRR', 'TCS', 'TCX', 'TD', 'TDC', 'TDG', 'TDOC', 'TDS', 'TDW', 'TDY', 'TEAM', 'TECD', 'TECH', 'TECK', 'TEL', 'TELL', 'TEN', 'TENB', 'TER', 'TERP', 'TESS', 'TEX', 'TFC', 'TFSL', 'TFX', 'TG', 'TGE', 'TGEN', 'TGH', 'TGI', 'TGLS', 'TGNA', 'TGP', 'TGT', 'TGTX', 'THC', 'THFF', 'THG', 'THMO', 'THO', 'THR', 'THRM', 'THS', 'TIF', 'TILE', 'TISI', 'TIVO', 'TJX', 'TKKS', 'TKR', 'TLRA', 'TLRD', 'TLRY', 'TLYS', 'TMHC', 'TMO', 'TMP', 'TMST', 'TMUS', 'TNAV', 'TNC', 'TNDM', 'TNET', 'TNP', 'TOL', 'TORC', 'TOWN', 'TPC', 'TPCO', 'TPH', 'TPIC', 'TPR', 'TPRE', 'TPTX', 'TPX', 'TR', 'TRC', 'TREE', 'TREX', 'TRGP', 'TRHC', 'TRI', 'TRIP', 'TRMB', 'TRMK', 'TRN', 'TROW', 'TROX', 'TRP', 'TRQ', 'TRS', 'TRST', 'TRT', 'TRTN', 'TRU', 'TRUE', 'TRUP', 'TRV', 'TRWH', 'TRXC', 'TSC', 'TSCO', 'TSEM', 'TSG', 'TSLA', 'TSN', 'TSQ', 'TSRI', 'TTC', 'TTD', 'TTEC', 'TTEK', 'TTGT', 'TTMI', 'TTOO', 'TTPH', 'TTWO', 'TUFN', 'TUP', 'TUSK', 'TVTY', 'TW', 'TWIN', 'TWLO', 'TWNK', 'TWOU', 'TWST', 'TWTR', 'TXG', 'TXMD', 'TXN', 'TXRH', 'TXT', 'TYL', 'TZOO', 'UAA', 'UAL', 'UBER', 'UBSI', 'UBX', 'UCBI', 'UCTT', 'UEIC', 'UFCS', 'UFI', 'UFPI', 'UFPT', 'UFS', 'UG', 'UGI', 'UHAL', 'UHS', 'UI', 'UIHC', 'UIS', 'ULBI', 'ULH', 'ULTA', 'UMBF', 'UMPQ', 'UNB', 'UNF', 'UNFI', 'UNH', 'UNM', 'UNP', 'UNT', 'UNVR', 'UPLD', 'UPS', 'UPWK', 'URBN', 'URGN', 'URI', 'UROV', 'USAK', 'USAP', 'USAS', 'USB', 'USCR', 'USFD', 'USLM', 'USM', 'USNA', 'USPH', 'USX', 'UTHR', 'UTI', 'UTL', 'UTMD', 'UTSI', 'UTX', 'UVE', 'UVSP', 'UVV', 'V', 'VAC', 'VAL', 'VALU', 'VAPO', 'VAR', 'VBTX', 'VC', 'VCEL', 'VCNX', 'VCRA', 'VCYT', 'VEC', 'VECO', 'VEEV', 'VERI', 'VERU', 'VFC', 'VGR', 'VHI', 'VIAC', 'VIAV', 'VICR', 'VIE', 'VIR', 'VIRT', 'VIVE', 'VIVO', 'VKTX', 'VLGEA', 'VLO', 'VLY', 'VMC', 'VMI', 'VMW', 'VNCE', 'VNDA', 'VNE', 'VOXX', 'VOYA', 'VPG', 'VRA', 'VRAY', 'VRCA', 'VREX', 'VRNS', 'VRNT', 'VRRM', 'VRS', 'VRSK', 'VRSN', 'VRTS', 'VRTU', 'VRTV', 'VRTX', 'VSAT', 'VSEC', 'VSH', 'VSLR', 'VST', 'VSTO', 'VUZI', 'VVI', 'VVUS', 'VVV', 'VYGR', 'VZ', 'W', 'WAB', 'WABC', 'WAFD', 'WAL', 'WASH', 'WAT', 'WATT', 'WBA', 'WBS', 'WBT', 'WCC', 'WCN', 'WD', 'WDAY', 'WDC', 'WDFC', 'WDR', 'WEC', 'WEN', 'WERN', 'WETF', 'WEX', 'WEYS', 'WFC', 'WGO', 'WH', 'WHD', 'WHG', 'WHR', 'WIFI', 'WINA', 'WING', 'WINS', 'WIRE', 'WIX', 'WK', 'WLDN', 'WLFC', 'WLH', 'WLK', 'WLL', 'WLTW', 'WM', 'WMB', 'WMK', 'WMS', 'WMT', 'WNC', 'WNEB', 'WOR', 'WORK', 'WORX', 'WOW', 'WPM', 'WPRT', 'WPX', 'WRB', 'WRK', 'WRLD', 'WRTC', 'WSBC', 'WSBF', 'WSC', 'WSFS', 'WSM', 'WSO', 'WST', 'WTBA', 'WTFC', 'WTI', 'WTM', 'WTR', 'WTRE', 'WTS', 'WU', 'WVE', 'WVFC', 'WVVI', 'WW', 'WWD', 'WWE', 'WWR', 'WWW', 'WYND', 'WYNN', 'X', 'XAIR', 'XBIT', 'XEC', 'XEL', 'XENE', 'XENT', 'XLNX', 'XLRN', 'XNCR', 'XOG', 'XOM', 'XOMA', 'XON', 'XONE', 'XPEL', 'XPER', 'XPO', 'XRAY', 'XRX', 'XYL', 'Y', 'YELP', 'YETI', 'YEXT', 'YMAB', 'YNDX', 'YORW', 'YRCW', 'YUM', 'YUMA', 'YUMC', 'ZAGG', 'ZBH', 'ZBRA', 'ZEN', 'ZFGN', 'ZG', 'ZGNX', 'ZION', 'ZIOP', 'ZIXI', 'ZM', 'ZNGA', 'ZS', 'ZTS', 'ZUMZ', 'ZUO', 'ZVO', 'ZYME', 'ZYNE']\n"
     ]
    }
   ],
   "source": [
    "myfile = open(\"ticker.txt\")\n",
    "data = myfile.readlines()\n",
    "myfile.close()\n",
    "#print(data)\n",
    "l_str = [i.rstrip('\\n') for i in data]\n",
    "print(l_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.stocktwits.com/api/2/streams/symbol/BBRY.json\n",
      "./data/BBRY_20200128_0647.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/AAPL.json\n",
      "./data/AAPL_20200128_0647.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/AMZN.json\n",
      "./data/AMZN_20200128_0647.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/BABA.json\n",
      "./data/BABA_20200128_0647.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/YHOO.json\n",
      "./data/YHOO_20200128_0647.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/LQMT.json\n",
      "./data/LQMT_20200128_0647.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/FB.json\n",
      "./data/FB_20200128_0647.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/GOOG.json\n",
      "./data/GOOG_20200128_0647.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/BBBY.json\n",
      "./data/BBBY_20200128_0647.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/JNUG.json\n",
      "./data/JNUG_20200128_0647.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/SBUX.json\n",
      "./data/SBUX_20200128_0647.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/MU.json\n",
      "./data/MU_20200128_0647.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/A.json\n",
      "./data/A_20200128_0647.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/AA.json\n",
      "./data/AA_20200128_0647.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/AAL.json\n",
      "./data/AAL_20200128_0647.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/AAN.json\n",
      "./data/AAN_20200128_0647.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/AAOI.json\n",
      "./data/AAOI_20200128_0647.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/AAON.json\n",
      "./data/AAON_20200128_0647.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/AAP.json\n",
      "./data/AAP_20200128_0647.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/AAPL.json\n",
      "./data/AAPL_20200128_0647.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/AAWW.json\n",
      "./data/AAWW_20200128_0647.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/AAXN.json\n",
      "./data/AAXN_20200128_0647.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ABBV.json\n",
      "./data/ABBV_20200128_0647.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ABC.json\n",
      "./data/ABC_20200128_0647.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ABCB.json\n",
      "./data/ABCB_20200128_0647.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ABEO.json\n",
      "./data/ABEO_20200128_0647.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ABG.json\n",
      "./data/ABG_20200128_0647.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ABM.json\n",
      "./data/ABM_20200128_0647.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ABMD.json\n",
      "./data/ABMD_20200128_0647.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ABT.json\n",
      "./data/ABT_20200128_0647.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ABTX.json\n",
      "./data/ABTX_20200128_0647.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ACA.json\n",
      "./data/ACA_20200128_0647.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ACAD.json\n",
      "./data/ACAD_20200128_0647.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ACCO.json\n",
      "./data/ACCO_20200128_0647.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ACEL.json\n",
      "./data/ACEL_20200128_0647.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ACGL.json\n",
      "./data/ACGL_20200128_0647.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ACHC.json\n",
      "./data/ACHC_20200128_0647.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ACHN.json\n",
      "./data/ACHN_20200128_0647.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ACHV.json\n",
      "./data/ACHV_20200128_0647.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ACIA.json\n",
      "./data/ACIA_20200128_0647.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ACIW.json\n",
      "./data/ACIW_20200128_0647.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ACLS.json\n",
      "./data/ACLS_20200128_0647.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ACM.json\n",
      "./data/ACM_20200128_0647.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ACN.json\n",
      "./data/ACN_20200128_0647.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ACNB.json\n",
      "./data/ACNB_20200128_0647.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ACOR.json\n",
      "./data/ACOR_20200128_0647.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ACRS.json\n",
      "./data/ACRS_20200128_0647.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ACRX.json\n",
      "./data/ACRX_20200128_0647.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ACTG.json\n",
      "./data/ACTG_20200128_0647.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ADBE.json\n",
      "./data/ADBE_20200128_0647.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ADES.json\n",
      "./data/ADES_20200128_0647.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ADI.json\n",
      "./data/ADI_20200128_0647.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ADM.json\n",
      "./data/ADM_20200128_0647.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ADMA.json\n",
      "./data/ADMA_20200128_0647.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ADMP.json\n",
      "./data/ADMP_20200128_0647.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ADMS.json\n",
      "./data/ADMS_20200128_0647.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ADP.json\n",
      "./data/ADP_20200128_0647.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ADPT.json\n",
      "./data/ADPT_20200128_0647.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ADRO.json\n",
      "./data/ADRO_20200128_0647.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ADS.json\n",
      "./data/ADS_20200128_0647.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ADSK.json\n",
      "./data/ADSK_20200128_0647.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ADSW.json\n",
      "./data/ADSW_20200128_0647.json\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "from datetime import datetime\n",
    "\n",
    "symbols = ['BBRY', 'AAPL', 'AMZN', 'BABA', 'YHOO', 'LQMT', 'FB', 'GOOG', 'BBBY', 'JNUG', 'SBUX', 'MU']\n",
    "symbols.extend(l_str[0:50])\n",
    "\n",
    "args = ['curl', '-X', 'GET', '']\n",
    "URL = \"https://api.stocktwits.com/api/2/streams/symbol/\"\n",
    "\n",
    "FILE_PATH = \"./data/\"\n",
    "\n",
    "for i in range(1):\n",
    "    start_datetime = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "    for symbol in symbols:\n",
    "        try:\n",
    "            args[3] = URL + symbol + \".json\"\n",
    "            print(args[3])\n",
    "            proc = subprocess.run(args,stdout = subprocess.PIPE, stderr = subprocess.PIPE)\n",
    "\n",
    "            path = FILE_PATH + symbol + \"_\" + start_datetime + \".json\"\n",
    "            print(path)\n",
    "            with open(path, mode='w') as f:\n",
    "                f.write(proc.stdout.decode(\"utf8\"))\n",
    "        except:\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            print(\"Error.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDFSへファイルをコピー"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hdfs', 'dfs', '-put', './data/MU_20200128_0647.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/ACOR_20200128_0647.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/AMZN_20200128_0647.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/ACIW_20200128_0647.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/ACCO_20200128_0647.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/LQMT_20200128_0647.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/ACHN_20200128_0647.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/ACGL_20200128_0647.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/AAL_20200128_0647.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/AAWW_20200128_0647.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/ABBV_20200128_0647.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/AAON_20200128_0647.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/JNUG_20200128_0647.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/ABCB_20200128_0647.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/AAPL_20200128_0647.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/ACLS_20200128_0647.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/ACTG_20200128_0647.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/ADM_20200128_0647.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/ACEL_20200128_0647.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/ADP_20200128_0647.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/ABG_20200128_0647.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/ADES_20200128_0647.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/ADSK_20200128_0647.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/AAP_20200128_0647.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/ABC_20200128_0647.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/ABEO_20200128_0647.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/ACHV_20200128_0647.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/BABA_20200128_0647.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/ABTX_20200128_0647.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/ABM_20200128_0647.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/ABMD_20200128_0647.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/ADRO_20200128_0647.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/AA_20200128_0647.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/ACM_20200128_0647.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/ACN_20200128_0647.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/ACA_20200128_0647.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/ACRS_20200128_0647.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/A_20200128_0647.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/ADI_20200128_0647.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/AAXN_20200128_0647.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/ADMP_20200128_0647.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/ACAD_20200128_0647.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/BBRY_20200128_0647.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/ADS_20200128_0647.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/SBUX_20200128_0647.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/ACIA_20200128_0647.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/YHOO_20200128_0647.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/ABT_20200128_0647.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/AAN_20200128_0647.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/ACRX_20200128_0647.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/BBBY_20200128_0647.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/AAOI_20200128_0647.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/ACHC_20200128_0647.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/ACNB_20200128_0647.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/ADBE_20200128_0647.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/ADPT_20200128_0647.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/ADMS_20200128_0647.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/ADSW_20200128_0647.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/FB_20200128_0647.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/ADMA_20200128_0647.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/GOOG_20200128_0647.json', './twits/']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#https://community.cloudera.com/t5/CDH-Manual-Installation/amp-quot-RuntimeException-core-site-xml-not-found-amp-quot/td-p/87590\n",
    "# So, I assume that the problem comes from the code where HIVE_CONF_DIR is appended to HADOOP_CONF_DIR. \n",
    "#\n",
    "#os.environ['HADOOP_CONF_DIR'] = \"/opt/cloudera/parcels/CDH-6.1.0-1.cdh6.1.0.p0.770702/lib/spark/conf/yarn-conf\"\n",
    "#os.environ['HADOOP_CONF_DIR'] = \"/etc/spark/conf/yarn-conf\"\n",
    "\n",
    "#os.environ['HADOOP_CONF_DIR'] = \"/opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/lib/spark/conf/yarn-conf\"\n",
    "os.environ['HADOOP_CONF_DIR'] = \"/etc/spark/conf/yarn-conf\"\n",
    "\n",
    "#args = ['hdfs', 'dfs', '-ls', '/tmp/']\n",
    "#HDFS_PATH_DIR = '/tmp/twits/'\n",
    "HDFS_PATH_DIR = './twits/'\n",
    "\n",
    "#HDFS_PATH_DIR = '/tmp/'\n",
    "\n",
    "args = ['hdfs', 'dfs', '-put', '', HDFS_PATH_DIR]\n",
    "\n",
    "\n",
    "try:\n",
    "  args_mkdir = ['hdfs', 'dfs', '-mkdir', HDFS_PATH_DIR]\n",
    "  proc = subprocess.run(args_mkdir,stdout = subprocess.PIPE, stderr = subprocess.PIPE)\n",
    "  \n",
    "except:\n",
    "  import traceback\n",
    "  traceback.print_exc()\n",
    "  print(\"Error.\")\n",
    "\n",
    "\n",
    "file_list = glob.glob(\"./data/*\")\n",
    "\n",
    "\n",
    "for file in file_list:\n",
    "    try:\n",
    "        #res = subprocess.check_call(args)\n",
    "        args[3] = file\n",
    "        print(args)\n",
    "        #subprocess.call([\"hadoop\", \"fs\", \"-ls\"])\n",
    "        proc = subprocess.run(args,stdout = subprocess.PIPE, stderr = subprocess.PIPE)\n",
    "        print(proc.stdout.decode(\"utf8\"))\n",
    "\n",
    "        #path = FILE_PATH + symbol + \".json\"\n",
    "\n",
    "        #with open(path, mode='w') as f:\n",
    "        #  f.write(proc.stdout.decode(\"utf8\"))\n",
    "    except:\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        print(\"Error.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hive データ変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#$ beeline -u 'jdbc:hive2://10.0.0.55:10000' -f tables.hql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hdfs', 'dfs', '-put', './lib/json-1.3.7.3.jar', './']\n",
      "stdout: \n",
      "['hdfs', 'dfs', '-put', './lib/README.md', './']\n",
      "stdout: \n",
      "['hdfs', 'dfs', '-put', './lib/brickhouse-0.7.1-SNAPSHOT.jar', './']\n",
      "stdout: \n",
      "['hdfs', 'dfs', '-put', './lib/json-serde-cdh5-shim-1.3.7.3.jar', './']\n",
      "stdout: \n",
      "['hdfs', 'dfs', '-put', './lib/json-serde-1.3.7.3.jar', './']\n",
      "stdout: \n"
     ]
    }
   ],
   "source": [
    "HDFS_PATH_DIR = '/tmp/'\n",
    "HDFS_PATH_DIR = './'\n",
    "\n",
    "args = ['hdfs', 'dfs', '-put', '', HDFS_PATH_DIR]\n",
    "\n",
    "file_list = glob.glob(\"./lib/*\")\n",
    "\n",
    "#proc = subprocess.call([\"hadoop\", \"fs\", \"-ls\"])\n",
    "#proc = subprocess.run([\"hdfs\", \"dfs\", \"-ls\"])\n",
    "#print(\"ls stdout:\", proc.stdout.decode(\"utf8\"))\n",
    "\n",
    "for file in file_list:\n",
    "  try:\n",
    "    #res = subprocess.check_call(args)\n",
    "    args[3] = file\n",
    "    print(args)\n",
    "\n",
    "    proc = subprocess.run(args,stdout = subprocess.PIPE, stderr = subprocess.PIPE)\n",
    "    print(\"stdout:\", proc.stdout.decode(\"utf8\"))\n",
    "    \n",
    "    #path = FILE_PATH + symbol + \".json\"\n",
    "    \n",
    "    #with open(path, mode='w') as f:\n",
    "    #  f.write(proc.stdout.decode(\"utf8\"))\n",
    "  \n",
    "  except:\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    print(\"Error.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhive import hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Engine(hive://ip-10-0-0-55.ap-northeast-1.compute.internal:10000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sqlalchemy\n",
    "sqlalchemy.create_engine('hive://ip-10-0-0-55.ap-northeast-1.compute.internal:10000')\n",
    "#sqlalchemy.create_engine('thrift://ip-10-0-0-55.ap-northeast-1.compute.internal:9083')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**下記のセルの中を適切なHiveサーバのURLに置換してください。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Connected: user1@None'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql hive://user1@ip-10-0-0-55.ap-northeast-1.compute.internal:10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hive://ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      " * hive://user1@ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>tab_name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>json_message</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>message_exploded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>message_extracted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>message_filtered</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>sentiment_data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>twits</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[('json_message',),\n",
       " ('message_exploded',),\n",
       " ('message_extracted',),\n",
       " ('message_filtered',),\n",
       " ('sentiment_data',),\n",
       " ('twits',)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "show tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user1\r\n"
     ]
    }
   ],
   "source": [
    "!echo $HADOOP_USER_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('json_message',), ('message_exploded',), ('message_extracted',), ('message_filtered',), ('sentiment_data',), ('twits',)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyhive import hive\n",
    "from TCLIService.ttypes import TOperationState\n",
    "cursor = hive.connect('ip-10-0-0-55.ap-northeast-1.compute.internal', port=10000).cursor()\n",
    "cursor.execute('show tables', async=True)\n",
    "\n",
    "status = cursor.poll().operationState\n",
    "while status in (TOperationState.INITIALIZED_STATE, TOperationState.RUNNING_STATE):\n",
    "    logs = cursor.fetch_logs()\n",
    "    for message in logs:\n",
    "        print(message)\n",
    "    status = cursor.poll().operationState\n",
    "\n",
    "print(cursor.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * hive://ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      "(pyhive.exc.OperationalError) TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=['*org.apache.hive.service.cli.HiveSQLException:Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Database user1 already exists:28:27', 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:329', 'org.apache.hive.service.cli.operation.SQLOperation:runQuery:SQLOperation.java:258', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:293', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:260', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:505', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatement:HiveSessionImpl.java:480', 'sun.reflect.GeneratedMethodAccessor32:invoke::-1', 'sun.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:498', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:-2', 'javax.security.auth.Subject:doAs:Subject.java:422', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1875', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'com.sun.proxy.$Proxy37:executeStatement::-1', 'org.apache.hive.service.cli.CLIService:executeStatement:CLIService.java:270', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:508', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1437', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1422', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:39', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:39', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:286', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1149', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:624', 'java.lang.Thread:run:Thread.java:748', '*org.apache.hadoop.hive.ql.metadata.HiveException:Database user1 already exists:36:9', 'org.apache.hadoop.hive.ql.exec.DDLTask:createDatabase:DDLTask.java:4198', 'org.apache.hadoop.hive.ql.exec.DDLTask:execute:DDLTask.java:308', 'org.apache.hadoop.hive.ql.exec.Task:executeTask:Task.java:199', 'org.apache.hadoop.hive.ql.exec.TaskRunner:runSequential:TaskRunner.java:97', 'org.apache.hadoop.hive.ql.Driver:launchTask:Driver.java:2200', 'org.apache.hadoop.hive.ql.Driver:execute:Driver.java:1843', 'org.apache.hadoop.hive.ql.Driver:runInternal:Driver.java:1563', 'org.apache.hadoop.hive.ql.Driver:run:Driver.java:1339', 'org.apache.hadoop.hive.ql.Driver:run:Driver.java:1334', 'org.apache.hive.service.cli.operation.SQLOperation:runQuery:SQLOperation.java:256', '*org.apache.hadoop.hive.metastore.api.AlreadyExistsException:Database user1 already exists:56:20', 'org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$create_database_result$create_database_resultStandardScheme:read:ThriftHiveMetastore.java:26211', 'org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$create_database_result$create_database_resultStandardScheme:read:ThriftHiveMetastore.java:26197', 'org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$create_database_result:read:ThriftHiveMetastore.java:26131', 'org.apache.thrift.TServiceClient:receiveBase:TServiceClient.java:86', 'org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client:recv_create_database:ThriftHiveMetastore.java:741', 'org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client:create_database:ThriftHiveMetastore.java:728', 'org.apache.hadoop.hive.metastore.HiveMetaStoreClient:createDatabase:HiveMetaStoreClient.java:800', 'sun.reflect.NativeMethodAccessorImpl:invoke0:NativeMethodAccessorImpl.java:-2', 'sun.reflect.NativeMethodAccessorImpl:invoke:NativeMethodAccessorImpl.java:62', 'sun.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:498', 'org.apache.hadoop.hive.metastore.RetryingMetaStoreClient:invoke:RetryingMetaStoreClient.java:154', 'com.sun.proxy.$Proxy36:createDatabase::-1', 'sun.reflect.NativeMethodAccessorImpl:invoke0:NativeMethodAccessorImpl.java:-2', 'sun.reflect.NativeMethodAccessorImpl:invoke:NativeMethodAccessorImpl.java:62', 'sun.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:498', 'org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler:invoke:HiveMetaStoreClient.java:2562', 'com.sun.proxy.$Proxy36:createDatabase::-1', 'org.apache.hadoop.hive.ql.metadata.Hive:createDatabase:Hive.java:433', 'org.apache.hadoop.hive.ql.exec.DDLTask:createDatabase:DDLTask.java:4194'], sqlState='42000', errorCode=1, errorMessage='Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Database user1 already exists'), operationHandle=None)\n",
      "[SQL: create database user1]\n",
      "(Background on this error at: http://sqlalche.me/e/e3q8)\n"
     ]
    }
   ],
   "source": [
    "%%sql\n",
    "create database user1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hive://ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      " * hive://user1@ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "use user1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hive://ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      " * hive://user1@ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>tab_name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>test</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[('test',)]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "show tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * hive://ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "add jar hdfs:/tmp/json-1.3.7.3.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * hive://ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "create table test(text string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * hive://ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      "(pyhive.exc.OperationalError) TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=[\"*org.apache.hive.service.cli.HiveSQLException:Error while compiling statement: FAILED: ParseException line 1:18 cannot recognize input near 'test' ';' '<EOF>' in from source 0:28:27\", 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:329', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:207', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:290', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:260', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:505', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatement:HiveSessionImpl.java:480', 'sun.reflect.GeneratedMethodAccessor32:invoke::-1', 'sun.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:498', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:-2', 'javax.security.auth.Subject:doAs:Subject.java:422', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1875', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'com.sun.proxy.$Proxy37:executeStatement::-1', 'org.apache.hive.service.cli.CLIService:executeStatement:CLIService.java:270', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:508', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1437', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1422', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:39', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:39', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:286', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1149', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:624', 'java.lang.Thread:run:Thread.java:748', \"*org.apache.hadoop.hive.ql.parse.ParseException:line 1:18 cannot recognize input near 'test' ';' '<EOF>' in from source 0:33:6\", 'org.apache.hadoop.hive.ql.parse.ParseDriver:parse:ParseDriver.java:221', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:75', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:68', 'org.apache.hadoop.hive.ql.Driver:compile:Driver.java:564', 'org.apache.hadoop.hive.ql.Driver:compileInternal:Driver.java:1425', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1398', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:205'], sqlState='42000', errorCode=40000, errorMessage=\"Error while compiling statement: FAILED: ParseException line 1:18 cannot recognize input near 'test' ';' '<EOF>' in from source 0\"), operationHandle=None)\n",
      "[SQL: select * from test;]\n",
      "(Background on this error at: http://sqlalche.me/e/e3q8)\n"
     ]
    }
   ],
   "source": [
    "%%sql\n",
    "select * from test;\n",
    "select text from test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hive://ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      " * hive://user1@ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      "Done.\n",
      "   hive://ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      " * hive://user1@ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      "Done.\n",
      "   hive://ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      " * hive://user1@ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql add jar hdfs:/tmp/json-1.3.7.3.jar\n",
    "%sql add jar hdfs:/tmp/json-serde-1.3.7.3.jar\n",
    "%sql add jar hdfs:/tmp/json-serde-cdh5-shim-1.3.7.3.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hive://ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      " * hive://user1@ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      "Done.\n",
      "   hive://ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      " * hive://user1@ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      "Done.\n",
      "   hive://ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      " * hive://user1@ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      "Done.\n",
      "   hive://ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      " * hive://user1@ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      "Done.\n",
      "   hive://ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      " * hive://user1@ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql DROP TABLE IF EXISTS twits\n",
    "%sql DROP TABLE IF EXISTS message_extracted\n",
    "%sql DROP TABLE IF EXISTS message_filtered\n",
    "%sql DROP TABLE IF EXISTS message_exploded\n",
    "%sql DROP TABLE IF EXISTS sentiment_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hive://ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      " * hive://user1@ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "CREATE EXTERNAL TABLE twits (\n",
    "\tmessages \n",
    "\tARRAY<\n",
    "\t    STRUCT<body: STRING,\n",
    "\t        symbols:ARRAY<STRUCT<symbol:STRING>>,\n",
    "\t        entities:STRUCT<sentiment:STRUCT<basic:STRING>>\n",
    "\t    >\n",
    "\t>\n",
    ")\n",
    "ROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe' \n",
    "STORED AS TEXTFILE\n",
    "LOCATION '/tmp/twits'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hive://ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      " * hive://user1@ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>messages</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>[{&quot;body&quot;:&quot;It’s ‘Star Trek’ vs. ‘Star Wars’ in European Streaming Battle  $CMCSA $VIAC $AAPL $AMZN $DIS \\n\\nhttps://newsfilter.io/a/0860a502f00979509c755ec03401ed14&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;AMZN&quot;},{&quot;symbol&quot;:&quot;CMCSA&quot;},{&quot;symbol&quot;:&quot;DIS&quot;},{&quot;symbol&quot;:&quot;VIAC&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$JD $AAPL&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;JD&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$SPY $AAPL $CLVS $AMRN \\nThis is the world we live in folks\\nhttps://archive.is/971HH#selection-5385.84-5385.224&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;AMRN&quot;},{&quot;symbol&quot;:&quot;SPY&quot;},{&quot;symbol&quot;:&quot;CLVS&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$AAPL Repo 8AM tomorrow&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$AAPL bought 320 puts at the hod should open nicely looking for 70-80%+ at open&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bearish&quot;}}},{&quot;body&quot;:&quot;$AAPL VIRUS GANG&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bearish&quot;}}},{&quot;body&quot;:&quot;$SPY think of the last person in china who touched your new iphone before you open the box\\n\\nWas he wearing a glove or did he $AAPL&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;SPY&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bearish&quot;}}},{&quot;body&quot;:&quot;$SPY $AMD $FB $BABA $AAPL By Morning everyone will be like&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;AMD&quot;},{&quot;symbol&quot;:&quot;SPY&quot;},{&quot;symbol&quot;:&quot;FB&quot;},{&quot;symbol&quot;:&quot;BABA&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;Apple Reports Earnings Next Week: Here’s How to Tell if Apple Will Meet Expectations $AAPL @apple #apple #earnings http://bit.ly/2RigUf2&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$AAPL $335 before ER&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$AAPL this trades in really tight, neat channels all year. if there is a correction maximum drawdown is 220, worst case scenario. still looking good, hitting the top of the upper channel here, expecting a pullback&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;Spyware Is Getting So Smart Even The Billionaires Aren’t Immune  $AAPL $FB $FCN $AMZN \\n\\nhttps://newsfilter.io/a/0049e30e93429d2379af7c9f4c706b8c&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;AMZN&quot;},{&quot;symbol&quot;:&quot;FCN&quot;},{&quot;symbol&quot;:&quot;FB&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$SPY $AAPL $TSLA $TWTR $GOOGL Real life Umbrella CORP in China&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;SPY&quot;},{&quot;symbol&quot;:&quot;TWTR&quot;},{&quot;symbol&quot;:&quot;TSLA&quot;},{&quot;symbol&quot;:&quot;GOOGL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$AAPL was analyzed by 29. The buy percentage consensus is at 81. So analysts seem to be very confident about $AAPL. https://www.chartmill.com/stock/quote/AAPL/analyst-ratings?utm_source=stocktwits&amp;amp;utm_medium=ANALYST&amp;amp;utm_content=AAPL&amp;amp;utm_campaign=social_tracking&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$SPY $TSLA $AAPL $LK $BABA matter of time before things get out of control. China travel banned is another options&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;SPY&quot;},{&quot;symbol&quot;:&quot;TSLA&quot;},{&quot;symbol&quot;:&quot;BABA&quot;},{&quot;symbol&quot;:&quot;LK&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bearish&quot;}}},{&quot;body&quot;:&quot;$AAPL $SPY $QQQ $BABA $ JD&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;SPY&quot;},{&quot;symbol&quot;:&quot;QQQ&quot;},{&quot;symbol&quot;:&quot;BABA&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;The $TSLA room is way better than the $AAPL room, but the $TSLA trolls are way worse 😂&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;TSLA&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$AAPL Upcoming ER here and there 👻. Wanna hedge.&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$SPCE Will outperform every other stock in 2020. It is the next $NFLX $TSLA $AAPL style runner. Investors are starving for growth potential.&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;NFLX&quot;},{&quot;symbol&quot;:&quot;TSLA&quot;},{&quot;symbol&quot;:&quot;SPCE&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$SPY $TSLA $AAPL $BYND  this is where SARS started. Beyond disgusting. 😖&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;SPY&quot;},{&quot;symbol&quot;:&quot;TSLA&quot;},{&quot;symbol&quot;:&quot;BYND&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bearish&quot;}}},{&quot;body&quot;:&quot;Thursday’s Watch Part 1: $AAPL $AMZN $AMD $BABA $BA 💖✅&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;AMZN&quot;},{&quot;symbol&quot;:&quot;AMD&quot;},{&quot;symbol&quot;:&quot;BA&quot;},{&quot;symbol&quot;:&quot;BABA&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$AAPL US &amp;amp; World Markets cant take day after day of Asia falling like this without it also reacting. This is how we know there are a lot of novice Traders in this Market. \\nSo many think its just business as usual &amp;amp; you BTD &amp;amp; all is well. Thats nots whats going on now folks, dont keep your head in the sand\\n$SPY $MSFT $QQQ $BABA&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;MSFT&quot;},{&quot;symbol&quot;:&quot;SPY&quot;},{&quot;symbol&quot;:&quot;QQQ&quot;},{&quot;symbol&quot;:&quot;BABA&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;Adobe Flash disabled in latest Safari Technology $ADBE Apple coming for the kill $AAPL&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;ADBE&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bearish&quot;}}},{&quot;body&quot;:&quot;$ADBE about to get destroyed by $AAPL https://appleinsider.com/articles/20/01/22/adobe-flash-disabled-in-latest-safari-technology-preview&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;ADBE&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bearish&quot;}}},{&quot;body&quot;:&quot;$AAPL $SPY $QQQ $DIA  good to see a slight dip in futures this eve - could make for a better setup for Thur Jan 23, 2020.  AAPL 319.99 print today is now resistance.&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;DIA&quot;},{&quot;symbol&quot;:&quot;SPY&quot;},{&quot;symbol&quot;:&quot;QQQ&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$AAPL fed pump buy&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$SPY Bulls.... $AAPL $BA $ROKU $BYND 🤣🤦‍♂️😆🤥&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;BA&quot;},{&quot;symbol&quot;:&quot;SPY&quot;},{&quot;symbol&quot;:&quot;ROKU&quot;},{&quot;symbol&quot;:&quot;BYND&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bearish&quot;}}},{&quot;body&quot;:&quot;$AAPL late cycle for this tech and this market cycle. Over valued sector. Market performance induced by the Fed.&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bearish&quot;}}},{&quot;body&quot;:&quot;Peak profit for the last 6 expired option alerts for $AAPL -18.08  | 529.17  | 3.02  | 354.20  | 402.11  | 294.90  |&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$AAPL [Jan-31 28.00 Puts] Option volume Up +372.73 % |  Volume: 52 vs 11 https://www.sleekoptions.com/sleekscan.aspx?sub1=dscan&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>[{&quot;body&quot;:&quot;$SPY $QQQ $AAPL.   Asia Bloodbath.&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;SPY&quot;},{&quot;symbol&quot;:&quot;QQQ&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$BYND meatless. $SPY $AAPL $SPCE $TSLA&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;SPY&quot;},{&quot;symbol&quot;:&quot;TSLA&quot;},{&quot;symbol&quot;:&quot;BYND&quot;},{&quot;symbol&quot;:&quot;SPCE&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;It’s ‘Star Trek’ vs. ‘Star Wars’ in European Streaming Battle  $CMCSA $VIAC $AAPL $AMZN $DIS \\n\\nhttps://newsfilter.io/a/0860a502f00979509c755ec03401ed14&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;AMZN&quot;},{&quot;symbol&quot;:&quot;CMCSA&quot;},{&quot;symbol&quot;:&quot;DIS&quot;},{&quot;symbol&quot;:&quot;VIAC&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$JD $AAPL&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;JD&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$SPY $AAPL $CLVS $AMRN \\nThis is the world we live in folks\\nhttps://archive.is/971HH#selection-5385.84-5385.224&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;AMRN&quot;},{&quot;symbol&quot;:&quot;SPY&quot;},{&quot;symbol&quot;:&quot;CLVS&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$AAPL Repo 8AM tomorrow&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$AAPL bought 320 puts at the hod should open nicely looking for 70-80%+ at open&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bearish&quot;}}},{&quot;body&quot;:&quot;$AAPL VIRUS GANG&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bearish&quot;}}},{&quot;body&quot;:&quot;$SPY think of the last person in china who touched your new iphone before you open the box\\n\\nWas he wearing a glove or did he $AAPL&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;SPY&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bearish&quot;}}},{&quot;body&quot;:&quot;$SPY $AMD $FB $BABA $AAPL By Morning everyone will be like&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;AMD&quot;},{&quot;symbol&quot;:&quot;SPY&quot;},{&quot;symbol&quot;:&quot;FB&quot;},{&quot;symbol&quot;:&quot;BABA&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;Apple Reports Earnings Next Week: Here’s How to Tell if Apple Will Meet Expectations $AAPL @apple #apple #earnings http://bit.ly/2RigUf2&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$AAPL $335 before ER&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$AAPL this trades in really tight, neat channels all year. if there is a correction maximum drawdown is 220, worst case scenario. still looking good, hitting the top of the upper channel here, expecting a pullback&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;Spyware Is Getting So Smart Even The Billionaires Aren’t Immune  $AAPL $FB $FCN $AMZN \\n\\nhttps://newsfilter.io/a/0049e30e93429d2379af7c9f4c706b8c&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;AMZN&quot;},{&quot;symbol&quot;:&quot;FCN&quot;},{&quot;symbol&quot;:&quot;FB&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$SPY $AAPL $TSLA $TWTR $GOOGL Real life Umbrella CORP in China&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;SPY&quot;},{&quot;symbol&quot;:&quot;TWTR&quot;},{&quot;symbol&quot;:&quot;TSLA&quot;},{&quot;symbol&quot;:&quot;GOOGL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$AAPL was analyzed by 29. The buy percentage consensus is at 81. So analysts seem to be very confident about $AAPL. https://www.chartmill.com/stock/quote/AAPL/analyst-ratings?utm_source=stocktwits&amp;amp;utm_medium=ANALYST&amp;amp;utm_content=AAPL&amp;amp;utm_campaign=social_tracking&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$SPY $TSLA $AAPL $LK $BABA matter of time before things get out of control. China travel banned is another options&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;SPY&quot;},{&quot;symbol&quot;:&quot;TSLA&quot;},{&quot;symbol&quot;:&quot;BABA&quot;},{&quot;symbol&quot;:&quot;LK&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bearish&quot;}}},{&quot;body&quot;:&quot;$AAPL $SPY $QQQ $BABA $ JD&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;SPY&quot;},{&quot;symbol&quot;:&quot;QQQ&quot;},{&quot;symbol&quot;:&quot;BABA&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;The $TSLA room is way better than the $AAPL room, but the $TSLA trolls are way worse 😂&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;TSLA&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$AAPL Upcoming ER here and there 👻. Wanna hedge.&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$SPCE Will outperform every other stock in 2020. It is the next $NFLX $TSLA $AAPL style runner. Investors are starving for growth potential.&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;NFLX&quot;},{&quot;symbol&quot;:&quot;TSLA&quot;},{&quot;symbol&quot;:&quot;SPCE&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$SPY $TSLA $AAPL $BYND  this is where SARS started. Beyond disgusting. 😖&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;SPY&quot;},{&quot;symbol&quot;:&quot;TSLA&quot;},{&quot;symbol&quot;:&quot;BYND&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bearish&quot;}}},{&quot;body&quot;:&quot;Thursday’s Watch Part 1: $AAPL $AMZN $AMD $BABA $BA 💖✅&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;AMZN&quot;},{&quot;symbol&quot;:&quot;AMD&quot;},{&quot;symbol&quot;:&quot;BA&quot;},{&quot;symbol&quot;:&quot;BABA&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$AAPL US &amp;amp; World Markets cant take day after day of Asia falling like this without it also reacting. This is how we know there are a lot of novice Traders in this Market. \\nSo many think its just business as usual &amp;amp; you BTD &amp;amp; all is well. Thats nots whats going on now folks, dont keep your head in the sand\\n$SPY $MSFT $QQQ $BABA&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;MSFT&quot;},{&quot;symbol&quot;:&quot;SPY&quot;},{&quot;symbol&quot;:&quot;QQQ&quot;},{&quot;symbol&quot;:&quot;BABA&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;Adobe Flash disabled in latest Safari Technology $ADBE Apple coming for the kill $AAPL&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;ADBE&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bearish&quot;}}},{&quot;body&quot;:&quot;$ADBE about to get destroyed by $AAPL https://appleinsider.com/articles/20/01/22/adobe-flash-disabled-in-latest-safari-technology-preview&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;ADBE&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bearish&quot;}}},{&quot;body&quot;:&quot;$AAPL $SPY $QQQ $DIA  good to see a slight dip in futures this eve - could make for a better setup for Thur Jan 23, 2020.  AAPL 319.99 print today is now resistance.&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;DIA&quot;},{&quot;symbol&quot;:&quot;SPY&quot;},{&quot;symbol&quot;:&quot;QQQ&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$AAPL fed pump buy&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$SPY Bulls.... $AAPL $BA $ROKU $BYND 🤣🤦‍♂️😆🤥&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;BA&quot;},{&quot;symbol&quot;:&quot;SPY&quot;},{&quot;symbol&quot;:&quot;ROKU&quot;},{&quot;symbol&quot;:&quot;BYND&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bearish&quot;}}},{&quot;body&quot;:&quot;$AAPL late cycle for this tech and this market cycle. Over valued sector. Market performance induced by the Fed.&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bearish&quot;}}}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>[{&quot;body&quot;:&quot;$AAPL 📈&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$SPY $AAPL Shorting and volatility is a thing of the past, only to be found in a museum&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;SPY&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$SPY https://www.dailystar.co.uk/news/world-news/scientists-blame-coronavirus-bats-pics-21337997 Lord! Wtf is wrong with people nowadays! $TSLA $FB $AAPL $LK&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;SPY&quot;},{&quot;symbol&quot;:&quot;FB&quot;},{&quot;symbol&quot;:&quot;TSLA&quot;},{&quot;symbol&quot;:&quot;LK&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$AMZN $BABA $AAPL $TSLA \\nKeep an Eye Out for Bank Earnings Before the Open!&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;AMZN&quot;},{&quot;symbol&quot;:&quot;TSLA&quot;},{&quot;symbol&quot;:&quot;BABA&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$SPY $AAPL $TSLA  In China they have also Mr.Repo. They will print more Money. Stock Market will go up. World go under. -&amp;gt; $BTC.X&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;SPY&quot;},{&quot;symbol&quot;:&quot;TSLA&quot;},{&quot;symbol&quot;:&quot;BTC.X&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$JAGX $BPMX $INTC $AAPL $SPY ANALYSIS&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;INTC&quot;},{&quot;symbol&quot;:&quot;SPY&quot;},{&quot;symbol&quot;:&quot;JAGX&quot;},{&quot;symbol&quot;:&quot;BPMX&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$SPY $AAPL People will pull all their money out of Asia and will start panic buying US stocks for safety.\\n\\nSPY 340 is right around the corner&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;SPY&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$AAPL $GPRO $SNAP&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;GPRO&quot;},{&quot;symbol&quot;:&quot;SNAP&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$SPY $QQQ $AAPL.   Asia Bloodbath.&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;SPY&quot;},{&quot;symbol&quot;:&quot;QQQ&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$BYND meatless. $SPY $AAPL $SPCE $TSLA&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;SPY&quot;},{&quot;symbol&quot;:&quot;TSLA&quot;},{&quot;symbol&quot;:&quot;BYND&quot;},{&quot;symbol&quot;:&quot;SPCE&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;It’s ‘Star Trek’ vs. ‘Star Wars’ in European Streaming Battle  $CMCSA $VIAC $AAPL $AMZN $DIS \\n\\nhttps://newsfilter.io/a/0860a502f00979509c755ec03401ed14&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;AMZN&quot;},{&quot;symbol&quot;:&quot;CMCSA&quot;},{&quot;symbol&quot;:&quot;DIS&quot;},{&quot;symbol&quot;:&quot;VIAC&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$JD $AAPL&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;JD&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$SPY $AAPL $CLVS $AMRN \\nThis is the world we live in folks\\nhttps://archive.is/971HH#selection-5385.84-5385.224&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;AMRN&quot;},{&quot;symbol&quot;:&quot;SPY&quot;},{&quot;symbol&quot;:&quot;CLVS&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$AAPL Repo 8AM tomorrow&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$AAPL bought 320 puts at the hod should open nicely looking for 70-80%+ at open&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bearish&quot;}}},{&quot;body&quot;:&quot;$AAPL VIRUS GANG&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bearish&quot;}}},{&quot;body&quot;:&quot;$SPY think of the last person in china who touched your new iphone before you open the box\\n\\nWas he wearing a glove or did he $AAPL&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;SPY&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bearish&quot;}}},{&quot;body&quot;:&quot;$SPY $AMD $FB $BABA $AAPL By Morning everyone will be like&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;AMD&quot;},{&quot;symbol&quot;:&quot;SPY&quot;},{&quot;symbol&quot;:&quot;FB&quot;},{&quot;symbol&quot;:&quot;BABA&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;Apple Reports Earnings Next Week: Here’s How to Tell if Apple Will Meet Expectations $AAPL @apple #apple #earnings http://bit.ly/2RigUf2&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$AAPL $335 before ER&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$AAPL this trades in really tight, neat channels all year. if there is a correction maximum drawdown is 220, worst case scenario. still looking good, hitting the top of the upper channel here, expecting a pullback&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;Spyware Is Getting So Smart Even The Billionaires Aren’t Immune  $AAPL $FB $FCN $AMZN \\n\\nhttps://newsfilter.io/a/0049e30e93429d2379af7c9f4c706b8c&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;AMZN&quot;},{&quot;symbol&quot;:&quot;FCN&quot;},{&quot;symbol&quot;:&quot;FB&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$SPY $AAPL $TSLA $TWTR $GOOGL Real life Umbrella CORP in China&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;SPY&quot;},{&quot;symbol&quot;:&quot;TWTR&quot;},{&quot;symbol&quot;:&quot;TSLA&quot;},{&quot;symbol&quot;:&quot;GOOGL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$AAPL was analyzed by 29. The buy percentage consensus is at 81. So analysts seem to be very confident about $AAPL. https://www.chartmill.com/stock/quote/AAPL/analyst-ratings?utm_source=stocktwits&amp;amp;utm_medium=ANALYST&amp;amp;utm_content=AAPL&amp;amp;utm_campaign=social_tracking&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$SPY $TSLA $AAPL $LK $BABA matter of time before things get out of control. China travel banned is another options&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;SPY&quot;},{&quot;symbol&quot;:&quot;TSLA&quot;},{&quot;symbol&quot;:&quot;BABA&quot;},{&quot;symbol&quot;:&quot;LK&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bearish&quot;}}},{&quot;body&quot;:&quot;$AAPL $SPY $QQQ $BABA $ JD&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;SPY&quot;},{&quot;symbol&quot;:&quot;QQQ&quot;},{&quot;symbol&quot;:&quot;BABA&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;The $TSLA room is way better than the $AAPL room, but the $TSLA trolls are way worse 😂&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;TSLA&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$AAPL Upcoming ER here and there 👻. Wanna hedge.&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$SPCE Will outperform every other stock in 2020. It is the next $NFLX $TSLA $AAPL style runner. Investors are starving for growth potential.&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;NFLX&quot;},{&quot;symbol&quot;:&quot;TSLA&quot;},{&quot;symbol&quot;:&quot;SPCE&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$SPY $TSLA $AAPL $BYND  this is where SARS started. Beyond disgusting. 😖&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;SPY&quot;},{&quot;symbol&quot;:&quot;TSLA&quot;},{&quot;symbol&quot;:&quot;BYND&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bearish&quot;}}}]</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[('[{\"body\":\"It’s ‘Star Trek’ vs. ‘Star Wars’ in European Streaming Battle  $CMCSA $VIAC $AAPL $AMZN $DIS \\\\n\\\\nhttps://newsfilter.io/a/0860a502f0097950 ... (6097 characters truncated) ... p +372.73 % |  Volume: 52 vs 11 https://www.sleekoptions.com/sleekscan.aspx?sub1=dscan\",\"symbols\":[{\"symbol\":\"AAPL\"}],\"entities\":{\"sentiment\":null}}]',),\n",
       " ('[{\"body\":\"$SPY $QQQ $AAPL.   Asia Bloodbath.\",\"symbols\":[{\"symbol\":\"AAPL\"},{\"symbol\":\"SPY\"},{\"symbol\":\"QQQ\"}],\"entities\":{\"sentiment\":null}},{\"body\": ... (6046 characters truncated) ... arket cycle. Over valued sector. Market performance induced by the Fed.\",\"symbols\":[{\"symbol\":\"AAPL\"}],\"entities\":{\"sentiment\":{\"basic\":\"Bearish\"}}}]',),\n",
       " ('[{\"body\":\"$AAPL 📈\",\"symbols\":[{\"symbol\":\"AAPL\"}],\"entities\":{\"sentiment\":{\"basic\":\"Bullish\"}}},{\"body\":\"$SPY $AAPL Shorting and volatility is a thing ... (5731 characters truncated) ... yond disgusting. 😖\",\"symbols\":[{\"symbol\":\"AAPL\"},{\"symbol\":\"SPY\"},{\"symbol\":\"TSLA\"},{\"symbol\":\"BYND\"}],\"entities\":{\"sentiment\":{\"basic\":\"Bearish\"}}}]',)]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "select * from twits limit 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hive://ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      " * hive://user1@ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      "Done.\n",
      "   hive://ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      " * hive://user1@ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      "Done.\n",
      "   hive://ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      " * hive://user1@ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      "Done.\n",
      "   hive://ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      " * hive://user1@ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql create table message_extracted (symbols array<struct<symbol:string>>, sentiment STRING, body STRING) STORED AS TEXTFILE\n",
    "%sql create table message_filtered (symbols array<struct<symbol:string>>, sentiment STRING, body STRING) STORED AS TEXTFILE\n",
    "%sql create table message_exploded (symbol string, sentiment STRING, body STRING) STORED AS TEXTFILE\n",
    "%sql create table sentiment_data (sentiment int, body STRING) STORED AS TEXTFILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hive://ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      " * hive://user1@ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "insert overwrite table message_extracted \n",
    "select message.symbols, message.entities.sentiment, message.body from twits \n",
    "lateral view explode(messages) messages as message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hive://ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      " * hive://user1@ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>symbols</th>\n",
       "        <th>sentiment</th>\n",
       "        <th>body</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;AMZN&quot;},{&quot;symbol&quot;:&quot;CMCSA&quot;},{&quot;symbol&quot;:&quot;DIS&quot;},{&quot;symbol&quot;:&quot;VIAC&quot;}]</td>\n",
       "        <td>None</td>\n",
       "        <td>It’s ‘Star Trek’ vs. ‘Star Wars’ in European Streaming Battle  $CMCSA $VIAC $AAPL $AMZN $DIS </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>[]</td>\n",
       "        <td>None</td>\n",
       "        <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>[{&quot;symbol&quot;:&quot;https://newsfilter.io/a/0860a502f00979509c755ec03401ed14&quot;}]</td>\n",
       "        <td>None</td>\n",
       "        <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;JD&quot;}]</td>\n",
       "        <td>None</td>\n",
       "        <td>$JD $AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;AMRN&quot;},{&quot;symbol&quot;:&quot;SPY&quot;},{&quot;symbol&quot;:&quot;CLVS&quot;}]</td>\n",
       "        <td>None</td>\n",
       "        <td>$SPY $AAPL $CLVS $AMRN </td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[('[{\"symbol\":\"AAPL\"},{\"symbol\":\"AMZN\"},{\"symbol\":\"CMCSA\"},{\"symbol\":\"DIS\"},{\"symbol\":\"VIAC\"}]', None, 'It’s ‘Star Trek’ vs. ‘Star Wars’ in European Streaming Battle  $CMCSA $VIAC $AAPL $AMZN $DIS '),\n",
       " ('[]', None, None),\n",
       " ('[{\"symbol\":\"https://newsfilter.io/a/0860a502f00979509c755ec03401ed14\"}]', None, None),\n",
       " ('[{\"symbol\":\"AAPL\"},{\"symbol\":\"JD\"}]', None, '$JD $AAPL'),\n",
       " ('[{\"symbol\":\"AAPL\"},{\"symbol\":\"AMRN\"},{\"symbol\":\"SPY\"},{\"symbol\":\"CLVS\"}]', None, '$SPY $AAPL $CLVS $AMRN ')]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "select * from message_extracted limit 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hive://ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      " * hive://user1@ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "insert overwrite table message_filtered \n",
    "select symbols, \n",
    "    case sentiment when 'Bearish' then -2 when 'Bullish' then 2 ELSE 0 END as sentiment, \n",
    "    body from message_extracted \n",
    "    where body is not null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hive://ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      " * hive://user1@ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>symbols</th>\n",
       "        <th>sentiment</th>\n",
       "        <th>body</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;AMZN&quot;},{&quot;symbol&quot;:&quot;CMCSA&quot;},{&quot;symbol&quot;:&quot;DIS&quot;},{&quot;symbol&quot;:&quot;VIAC&quot;}]</td>\n",
       "        <td>0</td>\n",
       "        <td>It’s ‘Star Trek’ vs. ‘Star Wars’ in European Streaming Battle  $CMCSA $VIAC $AAPL $AMZN $DIS </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;JD&quot;}]</td>\n",
       "        <td>0</td>\n",
       "        <td>$JD $AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;AMRN&quot;},{&quot;symbol&quot;:&quot;SPY&quot;},{&quot;symbol&quot;:&quot;CLVS&quot;}]</td>\n",
       "        <td>0</td>\n",
       "        <td>$SPY $AAPL $CLVS $AMRN </td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[('[{\"symbol\":\"AAPL\"},{\"symbol\":\"AMZN\"},{\"symbol\":\"CMCSA\"},{\"symbol\":\"DIS\"},{\"symbol\":\"VIAC\"}]', '0', 'It’s ‘Star Trek’ vs. ‘Star Wars’ in European Streaming Battle  $CMCSA $VIAC $AAPL $AMZN $DIS '),\n",
       " ('[{\"symbol\":\"AAPL\"},{\"symbol\":\"JD\"}]', '0', '$JD $AAPL'),\n",
       " ('[{\"symbol\":\"AAPL\"},{\"symbol\":\"AMRN\"},{\"symbol\":\"SPY\"},{\"symbol\":\"CLVS\"}]', '0', '$SPY $AAPL $CLVS $AMRN ')]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "select * from message_filtered limit 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hive://ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      " * hive://user1@ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "insert overwrite table message_exploded \n",
    "select symbol.symbol, sentiment, body from message_filtered lateral view explode(symbols) symbols as symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hive://ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      " * hive://user1@ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>symbol</th>\n",
       "        <th>sentiment</th>\n",
       "        <th>body</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>AAPL</td>\n",
       "        <td>0</td>\n",
       "        <td>It’s ‘Star Trek’ vs. ‘Star Wars’ in European Streaming Battle  $CMCSA $VIAC $AAPL $AMZN $DIS </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>AMZN</td>\n",
       "        <td>0</td>\n",
       "        <td>It’s ‘Star Trek’ vs. ‘Star Wars’ in European Streaming Battle  $CMCSA $VIAC $AAPL $AMZN $DIS </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>CMCSA</td>\n",
       "        <td>0</td>\n",
       "        <td>It’s ‘Star Trek’ vs. ‘Star Wars’ in European Streaming Battle  $CMCSA $VIAC $AAPL $AMZN $DIS </td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[('AAPL', '0', 'It’s ‘Star Trek’ vs. ‘Star Wars’ in European Streaming Battle  $CMCSA $VIAC $AAPL $AMZN $DIS '),\n",
       " ('AMZN', '0', 'It’s ‘Star Trek’ vs. ‘Star Wars’ in European Streaming Battle  $CMCSA $VIAC $AAPL $AMZN $DIS '),\n",
       " ('CMCSA', '0', 'It’s ‘Star Trek’ vs. ‘Star Wars’ in European Streaming Battle  $CMCSA $VIAC $AAPL $AMZN $DIS ')]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "select * from message_exploded limit 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hive://ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      " * hive://user1@ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "insert overwrite table sentiment_data \n",
    "select sentiment, body from message_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hive://ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      " * hive://user1@ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>sentiment</th>\n",
       "        <th>body</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>0</td>\n",
       "        <td>It’s ‘Star Trek’ vs. ‘Star Wars’ in European Streaming Battle  $CMCSA $VIAC $AAPL $AMZN $DIS </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>0</td>\n",
       "        <td>$JD $AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>0</td>\n",
       "        <td>$SPY $AAPL $CLVS $AMRN </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2</td>\n",
       "        <td>$AAPL Repo 8AM tomorrow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>-2</td>\n",
       "        <td>$AAPL bought 320 puts at the hod should open nicely looking for 70-80%+ at open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>-2</td>\n",
       "        <td>$AAPL VIRUS GANG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>-2</td>\n",
       "        <td>$SPY think of the last person in china who touched your new iphone before you open the box</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>0</td>\n",
       "        <td>$SPY $AMD $FB $BABA $AAPL By Morning everyone will be like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>0</td>\n",
       "        <td>Apple Reports Earnings Next Week: Here’s How to Tell if Apple Will Meet Expectations $AAPL @apple #apple #earnings http://bit.ly/2RigUf2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2</td>\n",
       "        <td>$AAPL $335 before ER</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(0, 'It’s ‘Star Trek’ vs. ‘Star Wars’ in European Streaming Battle  $CMCSA $VIAC $AAPL $AMZN $DIS '),\n",
       " (0, '$JD $AAPL'),\n",
       " (0, '$SPY $AAPL $CLVS $AMRN '),\n",
       " (2, '$AAPL Repo 8AM tomorrow'),\n",
       " (-2, '$AAPL bought 320 puts at the hod should open nicely looking for 70-80%+ at open'),\n",
       " (-2, '$AAPL VIRUS GANG'),\n",
       " (-2, '$SPY think of the last person in china who touched your new iphone before you open the box'),\n",
       " (0, '$SPY $AMD $FB $BABA $AAPL By Morning everyone will be like'),\n",
       " (0, 'Apple Reports Earnings Next Week: Here’s How to Tell if Apple Will Meet Expectations $AAPL @apple #apple #earnings http://bit.ly/2RigUf2'),\n",
       " (2, '$AAPL $335 before ER')]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "select * from sentiment_data limit 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%sql\n",
    "add jar hdfs:/tmp/json-1.3.7.3.jar;\n",
    "add jar hdfs:/tmp/json-serde-1.3.7.3.jar;\n",
    "add jar hdfs:/tmp/json-serde-cdh5-shim-1.3.7.3.jar;\n",
    "\n",
    "DROP TABLE IF EXISTS twits;\n",
    "CREATE EXTERNAL TABLE twits (\n",
    "\tmessages \n",
    "\tARRAY<\n",
    "\t    STRUCT<body: STRING,\n",
    "\t        symbols:ARRAY<STRUCT<symbol:STRING>>,\n",
    "\t        entities:STRUCT<sentiment:STRUCT<basic:STRING>>\n",
    "\t    >\n",
    "\t>\n",
    ")\n",
    "ROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe' \n",
    "STORED AS TEXTFILE\n",
    "LOCATION '/tmp/twits';\n",
    "\n",
    "create table message_extracted (symbols array<struct<symbol:string>>, sentiment STRING, body STRING) STORED AS TEXTFILE;\n",
    "create table message_filtered (symbols array<struct<symbol:string>>, sentiment STRING, body STRING) STORED AS TEXTFILE;\n",
    "create table message_exploded (symbol string, sentiment STRING, body STRING) STORED AS TEXTFILE;\n",
    "\n",
    "insert overwrite table message_extracted \n",
    "select message.symbols, message.entities.sentiment, message.body from twits \n",
    "lateral view explode(messages) messages as message;\n",
    "\n",
    "insert overwrite table message_filtered \n",
    "select symbols, \n",
    "    case sentiment when 'Bearish' then -2 when 'Bullish' then 2 ELSE 0 END as sentiment, \n",
    "    body from message_extracted \n",
    "    where body is not null;\n",
    "\n",
    "insert overwrite table message_exploded \n",
    "select symbol.symbol, sentiment, body from message_filtered lateral view explode(symbols) symbols as symbol;\n",
    "\n",
    "\n",
    "DROP TABLE IF EXISTS sentiment_data;\n",
    "create table sentiment_data (sentiment int, body STRING) STORED AS TEXTFILE;\n",
    "\n",
    "insert overwrite table sentiment_data \n",
    "select sentiment, body from message_filtered;\n",
    "\n",
    "select * from twits;\n",
    "select * from message_extracted;\n",
    "select * from message_filtered;\n",
    "select * from message_exploded;\n",
    "select * from sentiment_data;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('json_message',), ('message_exploded',), ('message_extracted',), ('message_filtered',), ('sentiment_data',), ('twits',)]\n"
     ]
    }
   ],
   "source": [
    "from pyhive import hive\n",
    "from TCLIService.ttypes import TOperationState\n",
    "cursor = hive.connect('ip-10-0-0-55.ap-northeast-1.compute.internal', port=10000).cursor()\n",
    "cursor.execute('show tables', async=True)\n",
    "\n",
    "status = cursor.poll().operationState\n",
    "while status in (TOperationState.INITIALIZED_STATE, TOperationState.RUNNING_STATE):\n",
    "    logs = cursor.fetch_logs()\n",
    "    for message in logs:\n",
    "        print(message)\n",
    "    status = cursor.poll().operationState\n",
    "\n",
    "print(cursor.fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. データ変換"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add jar hdfs:/tmp/brickhouse-0.7.1-SNAPSHOT.jar;\n",
    "CREATE TEMPORARY FUNCTION to_json AS 'brickhouse.udf.json.ToJsonUDF';\n",
    "\n",
    "create table json_message (message STRING) STORED AS TEXTFILE;\n",
    "\n",
    "insert overwrite table json_message\n",
    "select to_json(named_struct('message_body', body, 'sentiment', sentiment)) from sentiment_data;\n",
    "\n",
    "select * from json_message;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hive://ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      " * hive://user1@ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      "Done.\n",
      "   hive://ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      " * hive://user1@ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql add jar hdfs:/tmp/brickhouse-0.7.1-SNAPSHOT.jar\n",
    "%sql CREATE TEMPORARY FUNCTION to_json AS 'brickhouse.udf.json.ToJsonUDF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hive://ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      " * hive://user1@ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      "Done.\n",
      "   hive://ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      " * hive://user1@ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql DROP TABLE IF EXISTS json_message\n",
    "%sql create table json_message (message STRING) STORED AS TEXTFILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hive://ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      " * hive://user1@ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "insert overwrite table json_message\n",
    "select to_json(named_struct('message_body', body, 'sentiment', sentiment)) from sentiment_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hive://ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      " * hive://user1@ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>message</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>{&quot;message_body&quot;:&quot;It’s ‘Star Trek’ vs. ‘Star Wars’ in European Streaming Battle  $CMCSA $VIAC $AAPL $AMZN $DIS &quot;,&quot;sentiment&quot;:0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>{&quot;message_body&quot;:&quot;$JD $AAPL&quot;,&quot;sentiment&quot;:0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>{&quot;message_body&quot;:&quot;$SPY $AAPL $CLVS $AMRN &quot;,&quot;sentiment&quot;:0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>{&quot;message_body&quot;:&quot;$AAPL Repo 8AM tomorrow&quot;,&quot;sentiment&quot;:2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>{&quot;message_body&quot;:&quot;$AAPL bought 320 puts at the hod should open nicely looking for 70-80%+ at open&quot;,&quot;sentiment&quot;:-2}</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[('{\"message_body\":\"It’s ‘Star Trek’ vs. ‘Star Wars’ in European Streaming Battle  $CMCSA $VIAC $AAPL $AMZN $DIS \",\"sentiment\":0}',),\n",
       " ('{\"message_body\":\"$JD $AAPL\",\"sentiment\":0}',),\n",
       " ('{\"message_body\":\"$SPY $AAPL $CLVS $AMRN \",\"sentiment\":0}',),\n",
       " ('{\"message_body\":\"$AAPL Repo 8AM tomorrow\",\"sentiment\":2}',),\n",
       " ('{\"message_body\":\"$AAPL bought 320 puts at the hod should open nicely looking for 70-80%+ at open\",\"sentiment\":-2}',)]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "select * from user1.json_message limit 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hive://ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      " * hive://user1@ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>message</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>{&quot;message_body&quot;:&quot;It’s ‘Star Trek’ vs. ‘Star Wars’ in European Streaming Battle  $CMCSA $VIAC $AAPL $AMZN $DIS &quot;,&quot;sentiment&quot;:0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>{&quot;message_body&quot;:&quot;$JD $AAPL&quot;,&quot;sentiment&quot;:0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>{&quot;message_body&quot;:&quot;$SPY $AAPL $CLVS $AMRN &quot;,&quot;sentiment&quot;:0}</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[('{\"message_body\":\"It’s ‘Star Trek’ vs. ‘Star Wars’ in European Streaming Battle  $CMCSA $VIAC $AAPL $AMZN $DIS \",\"sentiment\":0}',),\n",
       " ('{\"message_body\":\"$JD $AAPL\",\"sentiment\":0}',),\n",
       " ('{\"message_body\":\"$SPY $AAPL $CLVS $AMRN \",\"sentiment\":0}',)]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "select * from user1.json_message limit 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import sys\n",
    "from random import random\n",
    "from operator import add\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"JsonGen\")\\\n",
    "    .getOrCreate()\n",
    "    \n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "#json_list = spark.read.table(\"json_message\")\n",
    "json_list = spark.sql(\"select * from user1.json_message\")\n",
    "\n",
    "#json_list.show(5)\n",
    "\n",
    "path = \"./output.json\"\n",
    "\n",
    "with open(path, mode='w') as f:\n",
    "    f.write('{\"data\":[')\n",
    "    bool_first_line = True\n",
    "    for row in json_list.rdd.collect():\n",
    "        if bool_first_line:\n",
    "            bool_first_line = False\n",
    "            f.write(row.message)\n",
    "        else:\n",
    "            #print(row.message)\n",
    "            #f.write(row.message.encode(\"utf-8\"))\n",
    "            for i in range(100): # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "                f.write(\",\\n\")\n",
    "                f.write(row.message)\n",
    "    \n",
    "    f.write(\"]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 感情分析\n",
    "企業の価値を決定するときは、ニュースをフォローすることが重要です。たとえば、会社の製品チェーンにおける製品のリコールまたは自然災害。この情報を信号に変換できるようにしたいと考えています。現在、この仕事に最適なツールはニューラルネットワークです。\n",
    "\n",
    "このプロジェクトでは、ソーシャルメディアサイトStockTwitsの投稿を使用します。StockTwitsのコミュニティは、投資家、トレーダー、起業家により利用されています。投稿された各メッセージはTwitと呼ばれます。これはTwitterのツイートによく似ています。感情のスコアを生成するこれらのtwitを中心にモデルを構築します。\n",
    "\n",
    "多数のtwitsを収集し、それぞれの感情を手でラベル付けしました。センチメントの度合いを把握するために、非常にネガティブ、ネガティブ、ニュートラル、ポジティブ、非常にポジティブという5段階のスケールを使用します。各ツイットは、それぞれ非常に負から非常に正まで、1のステップで-2から2までラベル付けされます。このラベル付きデータを使用して、感情を自分でtwitsに割り当てることを学習する感情分析モデルを構築します。\n",
    "\n",
    "最初にすべきことは、データをロードすることです。\n",
    "\n",
    "## Import Twits \n",
    "### Load Twits Data \n",
    "This JSON file contains a list of objects for each twit in the `'data'` field:\n",
    "\n",
    "```\n",
    "{'data':\n",
    "  {'message_body': 'Neutral twit body text here',\n",
    "   'sentiment': 0},\n",
    "  {'message_body': 'Happy twit body text here',\n",
    "   'sentiment': 1},\n",
    "   ...\n",
    "}\n",
    "```\n",
    "\n",
    "The fields represent the following:\n",
    "\n",
    "* `'message_body'`: The text of the twit.\n",
    "* `'sentiment'`: センチメントスコアは、-2から2の範囲で1のステップで、0は中立です。\n",
    "\n",
    "\n",
    "データがどのように見えるかを確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'message_body': 'It’s ‘Star Trek’ vs. ‘Star Wars’ in European Streaming Battle  $CMCSA $VIAC $AAPL $AMZN $DIS ', 'sentiment': 0}, {'message_body': '$JD $AAPL', 'sentiment': 0}, {'message_body': '$JD $AAPL', 'sentiment': 0}, {'message_body': '$JD $AAPL', 'sentiment': 0}, {'message_body': '$JD $AAPL', 'sentiment': 0}, {'message_body': '$JD $AAPL', 'sentiment': 0}, {'message_body': '$JD $AAPL', 'sentiment': 0}, {'message_body': '$JD $AAPL', 'sentiment': 0}, {'message_body': '$JD $AAPL', 'sentiment': 0}, {'message_body': '$JD $AAPL', 'sentiment': 0}]\n"
     ]
    }
   ],
   "source": [
    "#with open(os.path.join('..', '..', 'data', 'project_6_stocktwits', 'twits.json'), 'r') as f:\n",
    "#with open('./twits_dumped.json', 'r') as f:\n",
    "with open('./output.json', 'r') as f:\n",
    "    twits = json.load(f)\n",
    "\n",
    "print(twits['data'][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length of Data\n",
    "Now let's look at the number of twits in dataset. Print the number of twits below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131901\n"
     ]
    }
   ],
   "source": [
    "\"\"\"print out the number of twits\"\"\"\n",
    "\n",
    "# TODO Implement \n",
    "\n",
    "print(len(twits['data']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Message Body and Sentiment Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [twit['message_body'] for twit in twits['data']]\n",
    "# Since the sentiment scores are discrete, we'll scale the sentiments to 0 to 4 for use in our network\n",
    "sentiments = [twit['sentiment'] + 2 for twit in twits['data']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの前処理\n",
    "データを入手したら、テキストを前処理する必要があります。これらのtwitは、twit自体でリーダー$シンボルで示されるティッカーシンボルでフィルタリングすることにより収集されます。例えば、\n",
    "\n",
    "{'message_body': 'RT @google Our annual look at the year in Google blogging (and beyond) http://t.co/sptHOAh8 $GOOG',\n",
    " 'sentiment': 0}\n",
    "\n",
    "ティッカーシンボルはセンチメントに関する情報を提供せず、すべてのツイットに含まれているため、削除する必要があります。このtwitには@googleユーザー名もあり、ここでもセンチメント情報は提供されないため、削除する必要があります。URLも表示されますhttp://t.co/sptHOAh8。これらも削除しましょう。\n",
    "\n",
    "特定の単語やフレーズを削除する最も簡単な方法は、reモジュールを使用して正規表現を使用することです。スペースを使用して特定のパターンをサブアウトできます。\n",
    "\n",
    "re.sub(pattern, ' ', text)\n",
    "これにより、テキスト内のパターンが一致する場所でスペースが置換されます。後でテキストをトークン化するときに、それらのスペースで適切に分割します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/cdsw/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "\n",
    "def preprocess(message):\n",
    "    \"\"\"\n",
    "    入力として文字列を受け取り、次の操作を実行する: \n",
    "        - 全てのアルファベットを小文字に変換\n",
    "        - URLを削除\n",
    "        - ティッカーシンボルを削除 \n",
    "        - 句読点を削除\n",
    "        - 文字列をスペースで分割しトークン化する\n",
    "        - シングル・キャラクターのトークンを削除\n",
    "    \n",
    "    パラメータ\n",
    "    ----------\n",
    "        message : 前処理の対象テキストメッセージ\n",
    "        \n",
    "    戻り値\n",
    "    -------\n",
    "        tokens: 前処理後のトークン配列\n",
    "    \"\"\" \n",
    "    #TODO: Implement \n",
    "    \n",
    "    # Lowercase the twit message\n",
    "    text = message.lower()\n",
    "    \n",
    "    # Replace URLs with a space in the message\n",
    "    text = re.sub(\"http(s)?://([\\w\\-]+\\.)+[\\w-]+(/[\\w\\- ./?%&=]*)?\",' ', text)\n",
    "    \n",
    "    # Replace ticker symbols with a space. The ticker symbols are any stock symbol that starts with $.\n",
    "    text = re.sub(\"\\$[^ \\t\\n\\r\\f]+\", ' ', text)\n",
    "    \n",
    "    # Replace StockTwits usernames with a space. The usernames are any word that starts with @.\n",
    "    text = re.sub(\"@[^ \\t\\n\\r\\f]+\", ' ', text)\n",
    "\n",
    "    # Replace everything not a letter with a space\n",
    "    text = re.sub(\"[^a-z]\", ' ', text)\n",
    "    \n",
    "    \n",
    "    # Tokenize by splitting the string on whitespace into a list of words\n",
    "    tokens = text.split()\n",
    "\n",
    "    # Lemmatize words using the WordNetLemmatizer. You can ignore any word that is not longer than one character.\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    tokens = [wnl.lemmatize(w, pos='v') for w in tokens if len(w) > 1]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitsメッセージ前処理\n",
    "Now we can preprocess each of the twits in our dataset. Apply the function `preprocess` to all the twit messages.\n",
    "\n",
    "※この処理には、データのサイズに応じて多少時間がかかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['it', 'star', 'trek', 'vs', 'star', 'war', 'in', 'european', 'stream', 'battle'], [], []]\n",
      "131901\n"
     ]
    }
   ],
   "source": [
    "# TODO Implement\n",
    "#print(messages[:3])\n",
    "\n",
    "tokenized = list(map(preprocess, messages))\n",
    "\n",
    "print(tokenized[:3])\n",
    "print(len(tokenized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words\n",
    "\n",
    "すべてのメッセージがトークン化されたので、語彙を作成し、コーパス全体で各単語が出現する頻度をカウントします。Counter関数を使用して、すべてのトークンをカウントアップします。\n",
    "[`Counter`](https://docs.python.org/3.1/library/collections.html#collections.Counter)\n",
    "\n",
    "※この処理には、データのサイズに応じて多少時間がかかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it', 'star', 'trek', 'vs', 'star', 'war', 'in', 'european', 'stream', 'battle', 'repo', 'be', 'tomorrow']\n",
      "1713110\n",
      "131901\n",
      "[[8, 237, 507, 76, 237, 324, 4, 325, 262, 508], [], []]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "#words = []\n",
    "#for tokens in tokenized:\n",
    "#    for token in tokens:\n",
    "#        words.append(token)\n",
    "out_list = tokenized\n",
    "words = [element for in_list in out_list for element in in_list]\n",
    "\n",
    "print(words[:13])\n",
    "print(len(words))\n",
    "\n",
    "\"\"\"\n",
    "Create a vocabulary by using Bag of words\n",
    "\"\"\"\n",
    "\n",
    "# TODO: Implement \n",
    "\n",
    "word_counts = Counter(words)\n",
    "sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "int_to_vocab = {ii: word for ii, word in enumerate(sorted_vocab)}\n",
    "vocab_to_int = {word:ii for ii, word in int_to_vocab.items()}\n",
    "\n",
    "bow = []\n",
    "for tokens in tokenized:\n",
    "    bow.append([vocab_to_int[token] for token in tokens])\n",
    "\n",
    "print(len(bow))\n",
    "print(bow[:3])\n",
    "\n",
    "# This BOW will not be used because it is not filtered to eliminate common words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### メッセージに現れる単語の頻度\n",
    "\n",
    "ボキャブラリーを使用して、「the」、「and」、「it」などの最も一般的な単語の一部を削除します。\n",
    "これらの単語は感情を特定するのに寄与せず、非常に一般的であるため、ニューラルネットワークの入力のノイズとなります。これらを除外することで、ネットワークの学習時間を短縮することができます。\n",
    "\n",
    "また、ほんの数回しか使われていない、非常にまれな単語も削除します。ここでは、各単語のカウントをメッセージの数で除算する必要があります。次に、メッセージのごく一部にしか表示されない単語を削除します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(sorted_vocab): 1300\n",
      "sorted_vocab - top: ['the', 'be', 'to']\n",
      "sorted_vocab - least: ['enterprise', 'profitable', 'recognizable', 'quality', 'brand', 'exceed', 'restaurants', 'cafes', 'cubans', 'largest', 'constructive', 'exposure', 'december', 'atom', 'eps']\n",
      "freqs[the]: 0.038759916175843935\n",
      "high_cutoff: 20\n",
      "low_cutoff: 2e-06\n",
      "K_most_common: ['the', 'be', 'to', 'and', 'in', 'of', 'for', 'this', 'it', 'will', 'at', 'amp', 'utm', 'on', 'have', 'up', 'get', 'go', 'stock', 'buy']\n",
      "len(filtered_words): 1280\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Set the following variables:\n",
    "    freqs\n",
    "    low_cutoff\n",
    "    high_cutoff\n",
    "    K_most_common\n",
    "\"\"\"\n",
    "\n",
    "# TODO Implement \n",
    "\n",
    "print(\"len(sorted_vocab):\",len(sorted_vocab))\n",
    "print(\"sorted_vocab - top:\", sorted_vocab[:3])\n",
    "print(\"sorted_vocab - least:\", sorted_vocab[-15:])\n",
    "\n",
    "# Dictionart that contains the Frequency of words appearing in messages.\n",
    "# The key is the token and the value is the frequency of that word in the corpus.\n",
    "total_count = len(words)\n",
    "freqs = {word: count/total_count for word, count in word_counts.items()}\n",
    "\n",
    "#print(\"freqs[supplication]:\",freqs[\"supplication\"] )\n",
    "print(\"freqs[the]:\",freqs[\"the\"] )\n",
    "\n",
    "\"\"\"\n",
    "This was the post by Ricardo:\n",
    "\n",
    "there's no exact value for low_cutoff and high_cutoff, \n",
    "however I'd recommend you to use \n",
    "a low_cutoff that's around 0.000002 and 0.000007 \n",
    "(This depends on the values you get from your freqs calculations) and \n",
    "a high_cutofffrom 5 to 20 (this depends on the most_common values from the bow).\n",
    "\"\"\"\n",
    "\n",
    "# Float that is the frequency cutoff. Drop words with a frequency that is lower or equal to this number.\n",
    "low_cutoff = 0.000002\n",
    "\n",
    "# Integer that is the cut off for most common words. Drop words that are the `high_cutoff` most common words.\n",
    "\"\"\"\n",
    "example_count = []\n",
    "example_count.append(sorted_vocab.index(\"the\"))\n",
    "example_count.append(sorted_vocab.index(\"for\"))\n",
    "example_count.append(sorted_vocab.index(\"of\"))\n",
    "print(example_count)\n",
    "high_cutoff = min(example_count)\n",
    "\"\"\"\n",
    "high_cutoff = 20\n",
    "print(\"high_cutoff:\",high_cutoff)\n",
    "print(\"low_cutoff:\",low_cutoff)\n",
    "\n",
    "# The k most common words in the corpus. Use `high_cutoff` as the k.\n",
    "#K_most_common = [word for word in sorted_vocab[:high_cutoff]]\n",
    "K_most_common = sorted_vocab[:high_cutoff]\n",
    "\n",
    "print(\"K_most_common:\",K_most_common)\n",
    "\n",
    "\n",
    "##  END of TODO Implement\n",
    "\n",
    "filtered_words = [word for word in freqs if (freqs[word] > low_cutoff and word not in K_most_common)]\n",
    "\n",
    "print(\"len(filtered_words):\",len(filtered_words)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### フィルターされた単語を削除して語彙を更新する¶\n",
    "ボキャブラリーに役立つ3つの変数を作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(tokenized): 131901\n",
      "len(filtered): 131901\n",
      "tokenized[:1] [['it', 'star', 'trek', 'vs', 'star', 'war', 'in', 'european', 'stream', 'battle']]\n",
      "filtered[:1] [['star', 'trek', 'vs', 'star', 'war', 'european', 'stream', 'battle']]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Set the following variables:\n",
    "    vocab\n",
    "    id2vocab\n",
    "    filtered\n",
    "\"\"\"\n",
    "\n",
    "#TODO Implement\n",
    "\n",
    "# A dictionary for the `filtered_words`. The key is the word and value is an id that represents the word. \n",
    "vocab =  {word:ii for ii, word in enumerate(filtered_words)}\n",
    "# Reverse of the `vocab` dictionary. The key is word id and value is the word. \n",
    "id2vocab = {ii:word for word, ii in vocab.items()}\n",
    "# tokenized with the words not in `filtered_words` removed.\n",
    "\n",
    "print(\"len(tokenized):\", len(tokenized))\n",
    "\n",
    "filtered = [[token for token in tokens if token in vocab] for tokens in tokenized]\n",
    "print(\"len(filtered):\", len(filtered))\n",
    "print(\"tokenized[:1]\", tokenized[:1])\n",
    "print(\"filtered[:1]\",filtered[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### クラスのバランス\n",
    "最後の前処理ステップをいくつか行いましょう。twitのラベル付けを見ると、twitの50％がニュートラルであることがわかります。これは、毎回0を推測するだけで、ネットワークの精度が50％になることを意味します。ネットワークが適切に学習できるように、クラスのバランスを取る必要があります。つまり、それぞれのセンチメントスコアがデータにほぼ同じ頻度で表示されることを確認します。\n",
    "\n",
    "ここでできることは、それぞれの例に目を通し、中立的な感情を持つtwitsをランダムにドロップすることです。50％のニュートラルから20％のニュートラルtwitを取得したい場合、これらのtwitをドロップする確率はどうなりますか？この機会に、長さ0のメッセージを削除する必要もあります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "balanced = {'messages': [], 'sentiments':[]}\n",
    "\n",
    "n_neutral = sum(1 for each in sentiments if each == 2)\n",
    "N_examples = len(sentiments)\n",
    "keep_prob = (N_examples - n_neutral)/4/n_neutral\n",
    "\n",
    "for idx, sentiment in enumerate(sentiments):\n",
    "    message = filtered[idx]\n",
    "    if len(message) == 0:\n",
    "        # skip this message because it has length zero\n",
    "        continue\n",
    "    elif sentiment != 2 or random.random() < keep_prob:\n",
    "        balanced['messages'].append(message)\n",
    "        balanced['sentiments'].append(sentiment) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you did it correctly, you should see the following result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19318607263197304"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_neutral = sum(1 for each in balanced['sentiments'] if each == 2)\n",
    "N_examples = len(balanced['sentiments'])\n",
    "n_neutral/N_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's convert our tokens into integer ids which we can pass to the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = [[vocab[word] for word in message] for message in balanced['messages']]\n",
    "sentiments = balanced['sentiments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#from singer import Singer\n",
    "\n",
    "#singer = Singer('Shanranran')\n",
    "\n",
    "with open('vocab.pickle', 'wb') as f:\n",
    "    pickle.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ニューラルネットワーク¶\n",
    "これでボキャブラリーができたので、トークンをIDに変換し、それをネットワークに渡すことができます。ネットワークを定義します\n",
    "\n",
    "下記は、ネットワークの概要です：\n",
    "\n",
    "#### Embed -> RNN -> Dense -> Softmax\n",
    "### Text classifier (テキスト分類器)実装\n",
    "テキスト分類器を作成する前に、「RNNを使用したセンチメント分析」演習で作成した他のネットワーク（ここでは「SentimentRNN」と呼ばれるネットワーク、ここでは「TextClassifer」と呼びます）を覚えている場合、3つの主要な部分で構成されています：: 1) init function `__init__` 2) forward pass `forward`  3) hidden state `init_hidden`. \n",
    "\n",
    "このネットワークは、forwardパスで期待して構築したネットワークに非常に似ています 。シグモイドの代わりにsoftmaxを使用します。シグモイドを使用しないのは、NNの出力がバイナリではないためです。このネットワークでは、センチメントスコアには5つの結果があります。最も高い確率の結果を探しているため、softmaxの方が適しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, lstm_size, output_size, lstm_layers=1, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "            vocab_size : The vocabulary size.\n",
    "            embed_size : The embedding layer size.\n",
    "            lstm_size : The LSTM layer size.\n",
    "            output_size : The output size.\n",
    "            lstm_layers : The number of LSTM layers.\n",
    "            dropout : The dropout probability.\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.lstm_size = lstm_size\n",
    "        self.output_size = output_size\n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # TODO Implement\n",
    "\n",
    "        # Setup embedding layer\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embed_size)\n",
    "        \n",
    "        # Setup additional layers\n",
    "        self.lstm = nn.LSTM(self.embed_size, self.lstm_size, self.lstm_layers, dropout=self.dropout)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(lstm_size, output_size)\n",
    "        \n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\" \n",
    "        Initializes hidden state\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "            batch_size : The size of batches.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "            hidden_state\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO Implement \n",
    "        \n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        \n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        hidden = (weight.new(self.lstm_layers, batch_size,self.lstm_size).zero_(),\n",
    "                         weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_())\n",
    "        return hidden\n",
    "\n",
    "\n",
    "    def forward(self, nn_input, hidden_state):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on nn_input.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "            nn_input : The batch of input to the NN.\n",
    "            hidden_state : The LSTM hidden state.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            logps: log softmax output\n",
    "            hidden_state: The new hidden state.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO Implement \n",
    "        batch_size = nn_input.size(0)\n",
    "        \n",
    "        embeds = self.embedding(nn_input)\n",
    "        lstm_out, hidden_state = self.lstm(embeds, hidden_state)\n",
    "        \n",
    "        #lstm_out = lstm_out.contiguous().view(-1, self.lstm_size)    \n",
    "        \"\"\"\n",
    "        remember here you do not have batch_first=True, \n",
    "        so accordingly shape your input. \n",
    "        Moreover, since now input is seq_length x batch you just need to transform lstm_out = lstm_out[-1,:,:].\n",
    "        you don't have to use batch_first=True in this case, \n",
    "        nor reshape the outputs with .view just transform your lstm_out as advised and you should be good to go.\n",
    "        \"\"\"\n",
    "        lstm_out = lstm_out[-1,:,:]\n",
    "        \n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        logps = self.softmax(out)\n",
    "        \n",
    "        \n",
    "        return logps, hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5444, -1.7328, -1.5370, -1.6911, -1.5585],\n",
      "        [-1.5448, -1.7439, -1.5324, -1.6698, -1.5726],\n",
      "        [-1.5466, -1.7685, -1.5253, -1.6312, -1.5936],\n",
      "        [-1.6101, -1.7089, -1.4966, -1.6616, -1.5831]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "model = TextClassifier(len(vocab), 10, 6, 5, dropout=0.1, lstm_layers=2)\n",
    "model.embedding.weight.data.uniform_(-1, 1)\n",
    "input = torch.randint(0, 1000, (5, 4), dtype=torch.int64)\n",
    "hidden = model.init_hidden(4)\n",
    "\n",
    "logps, _ = model.forward(input, hidden)\n",
    "print(logps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## トレーニング\n",
    "### DataLoaderとバッチ処理\n",
    "ここで、データをループするために使用できるジェネレーターを構築する必要があります。シーケンスをバッチとして渡すことができれば、より効率的です。入力テンソルは次のようになり(sequence_length, batch_size)ます。したがって、シーケンスが40トークンで、25シーケンスを渡す場合、入力サイズはになり(40, 25)ます。\n",
    "\n",
    "シーケンスの長さを40に設定した場合、40トークンより多いまたは少ないメッセージをどう処理しますか？40トークン未満のメッセージの場合、空のスポットにゼロを埋め込みます。データを処理する前にRNNが何も開始しないように、必ずパッドを残しておく必要があります。メッセージに20個のトークンがある場合、40個の長いシーケンスの最初の20個のスポットは0になります。メッセージに40個を超えるトークンがある場合、最初の40個のトークンを保持します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def dataloader(messages, labels, sequence_length=30, batch_size=32, shuffle=False):\n",
    "def dataloader(messages, labels, sequence_length=20, batch_size=32, shuffle=False):\n",
    "    \"\"\" \n",
    "    Build a dataloader.\n",
    "    \"\"\"\n",
    "    if shuffle:\n",
    "        indices = list(range(len(messages)))\n",
    "        random.shuffle(indices)\n",
    "        messages = [messages[idx] for idx in indices]\n",
    "        labels = [labels[idx] for idx in indices]\n",
    "\n",
    "    total_sequences = len(messages)\n",
    "\n",
    "    for ii in range(0, total_sequences, batch_size):\n",
    "        batch_messages = messages[ii: ii+batch_size]\n",
    "        \n",
    "        # First initialize a tensor of all zeros\n",
    "        batch = torch.zeros((sequence_length, len(batch_messages)), dtype=torch.int64)\n",
    "        for batch_num, tokens in enumerate(batch_messages):\n",
    "            token_tensor = torch.tensor(tokens)\n",
    "            # Left pad!\n",
    "            start_idx = max(sequence_length - len(token_tensor), 0)\n",
    "            batch[start_idx:, batch_num] = token_tensor[:sequence_length]\n",
    "        \n",
    "        label_tensor = torch.tensor(labels[ii: ii+len(batch_messages)])\n",
    "        \n",
    "        yield batch, label_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and  Validation\n",
    "With our data in nice shape, we'll split it into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Split data into training and validation datasets. Use an appropriate split size.\n",
    "The features are the `token_ids` and the labels are the `sentiments`.\n",
    "\"\"\"   \n",
    "\n",
    "# TODO Implement \n",
    "\n",
    "split_frac = 0.98 # for small data\n",
    "#split_frac = 0.8 # for big data\n",
    "\n",
    "## split data into training, validation, and test data (features and labels, x and y)\n",
    "\n",
    "split_idx = int(len(token_ids)*split_frac)\n",
    "train_features, remaining_features = token_ids[:split_idx], token_ids[split_idx:]\n",
    "train_labels, remaining_labels = sentiments[:split_idx], sentiments[split_idx:]\n",
    "\n",
    "test_idx = int(len(remaining_features)*0.5)\n",
    "valid_features, test_features = remaining_features[:test_idx], remaining_features[test_idx:]\n",
    "valid_labels, test_labels = remaining_labels[:test_idx], remaining_labels[test_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_batch, labels = next(iter(dataloader(train_features, train_labels, sequence_length=20, batch_size=64)))\n",
    "model = TextClassifier(len(vocab)+1, 200, 128, 5, dropout=0.)\n",
    "hidden = model.init_hidden(64)\n",
    "logps, hidden = model.forward(text_batch, hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "It's time to train the neural network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextClassifier(\n",
       "  (embedding): Embedding(1281, 1024)\n",
       "  (lstm): LSTM(1024, 512, num_layers=2, dropout=0.2)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (fc): Linear(in_features=512, out_features=5, bias=True)\n",
       "  (softmax): LogSoftmax()\n",
       ")"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = TextClassifier(len(vocab)+1, 1024, 512, 5, lstm_layers=2, dropout=0.2)\n",
    "model.embedding.weight.data.uniform_(-1, 1)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### トレーニング実施\n",
    "\n",
    "※この処理には、データのサイズに応じて、十分な時間が必要です。\n",
    "\n",
    "GPUを備えた環境で実行する場合、ターミナルで以下のコマンドを実行することで、GPUが利用されていることを確認することができます（ GPU実行中、コマンド実行により表示されるテーブルの右上のVolatile GPU-Utilのパーセンテージ値が増えます）\n",
    "```\n",
    "$ watch nvidia-smi\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "Epoch: 1/4... Step: 100... Loss: 0.000316... Val Loss: 0.134239 Collect Count: 512.0 Accuracy: 100.00\n",
      "Starting epoch 2\n",
      "Epoch: 2/4... Step: 100... Loss: 0.000114... Val Loss: 0.066218 Collect Count: 512.0 Accuracy: 100.00\n",
      "Starting epoch 3\n",
      "Epoch: 3/4... Step: 100... Loss: 0.000083... Val Loss: 0.043962 Collect Count: 512.0 Accuracy: 100.00\n",
      "Starting epoch 4\n",
      "Epoch: 4/4... Step: 100... Loss: 0.000063... Val Loss: 0.032908 Collect Count: 512.0 Accuracy: 100.00\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Train your model with dropout. Make sure to clip your gradients.\n",
    "Print the training loss, validation loss, and validation accuracy for every 100 steps.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "epochs = 4 #pass\n",
    "batch_size =  64#pass\n",
    "batch_size =  512#pass\n",
    "learning_rate = 0.001 #pass\n",
    "\n",
    "print_every = 100\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "model.train()\n",
    "\n",
    "val_losses = []\n",
    "accuracy = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Starting epoch {}'.format(epoch + 1))\n",
    "    \n",
    "    steps = 0\n",
    "    for text_batch, labels in dataloader(\n",
    "            train_features, train_labels, batch_size=batch_size, sequence_length=20, shuffle=True):\n",
    "        steps += 1\n",
    "        hidden = model.init_hidden(labels.shape[0]) #pass\n",
    "        \n",
    "        # Set Device\n",
    "        text_batch, labels = text_batch.to(device), labels.to(device)\n",
    "        for each in hidden:\n",
    "            each.to(device)\n",
    "        \n",
    "        # TODO Implement: Train Model\n",
    "        hidden = tuple([each.data for each in hidden])\n",
    "        model.zero_grad()\n",
    "        output, hidden = model(text_batch, hidden)\n",
    "        loss = criterion(output.squeeze(), labels)\n",
    "        loss.backward()\n",
    "        clip = 5\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate loss\n",
    "        val_losses.append(loss.item())\n",
    "        \n",
    "        correct_count = 0.0\n",
    "        if steps % print_every == 0:\n",
    "            model.eval()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            ps = torch.exp(output)\n",
    "            top_p, top_class = ps.topk(1, dim=1)\n",
    "            #?top_class = top_class.to(device)\n",
    "            #?labels = labels.to(device)\n",
    "\n",
    "            correct_count += torch.sum(top_class.squeeze()== labels)\n",
    "            accuracy.append(100*correct_count/len(labels))\n",
    "            \n",
    "            # TODO Implement: Print metrics\n",
    "            print(\"Epoch: {}/{}...\".format(epoch+1, epochs),\n",
    "                 \"Step: {}...\".format(steps),\n",
    "                 \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                 \"Val Loss: {:.6f}\".format(np.mean(val_losses)),\n",
    "                 \"Collect Count: {}\".format(correct_count),\n",
    "                 \"Accuracy: {:.2f}\".format((100*correct_count/len(labels))),\n",
    "                 # AttributeError: 'torch.dtype' object has no attribute 'type'\n",
    "                 #\"Accuracy Avg: {:.2f}\".format(np.mean(accuracy))\n",
    "                 )\n",
    "            \n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'state_dict': model.state_dict()}, 'checkpoint.pth.tar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 予測（Prediction）関数の作成\n",
    "### Prediction \n",
    "訓練されたモデルを手に入れたので、新しいツイットでそれを試して、それが適切に機能するかどうか確かめてください。新しいテキストについては、ネットワークに渡す前に最初に前処理する必要があることに注意してください。predictメッセージから予測ベクトルを生成する関数を実装します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/cdsw/checkpoint.pth.tar', '/home/cdsw/nlp_handson.ipynb', '/home/cdsw/twits_dumped.json', '/home/cdsw/test.py', '/home/cdsw/data', '/home/cdsw/lib', '/home/cdsw/nlp_solution.ipynb', '/home/cdsw/README.md', '/home/cdsw/model.torch', '/home/cdsw/nltk_data', '/home/cdsw/tables.hql', '/home/cdsw/ticker.txt', '/home/cdsw/vocab.pickle', '/home/cdsw/init.sh', '/home/cdsw/output.json']\n",
      "/home/cdsw\n",
      "[ 0.00204649  0.02024106  0.08254681  0.09021453  0.80495113]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/cdsw/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "print(glob.glob(\"/home/cdsw/*\"))\n",
    "\n",
    "import pickle\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import torch\n",
    "\n",
    "#from sentiment import TextClassifier\n",
    "\n",
    "import os\n",
    "import sys\n",
    "cur_dir = os.path.dirname(os.path.abspath('__file__'))\n",
    "print(cur_dir)\n",
    "sys.path.append(cur_dir)\n",
    "\n",
    "vocab_filename = 'vocab.pickle'\n",
    "vocab_path = cur_dir + \"/\" + vocab_filename\n",
    "vocab_l = pickle.load(open(vocab_path, 'rb'))\n",
    "\n",
    "#model_path = cur_dir + \"/\" + \"model.torch\"\n",
    "#model_l = torch.load(model_path, map_location='cpu')\n",
    "\n",
    "model_l = TextClassifier(len(vocab_l)+1, 1024, 512, 5, lstm_layers=2, dropout=0.2)\n",
    "checkpoint = torch.load('./checkpoint.pth.tar')\n",
    "model_l.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "\n",
    "def preprocess(message):\n",
    "    \"\"\"\n",
    "    This function takes a string as input, then performs these operations: \n",
    "        - lowercase\n",
    "        - remove URLs\n",
    "        - remove ticker symbols \n",
    "        - removes punctuation\n",
    "        - tokenize by splitting the string on whitespace \n",
    "        - removes any single character tokens\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        message : The text message to be preprocessed.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "        tokens: The preprocessed text into tokens.\n",
    "    \"\"\" \n",
    "    \n",
    "    # Lowercase the twit message\n",
    "    text = message.lower()\n",
    "    \n",
    "    # Replace URLs with a space in the message\n",
    "    text = re.sub(\"http(s)?://([\\w\\-]+\\.)+[\\w-]+(/[\\w\\- ./?%&=]*)?\",' ', text)\n",
    "    \n",
    "    # Replace ticker symbols with a space. The ticker symbols are any stock symbol that starts with $.\n",
    "    text = re.sub(\"\\$[^ \\t\\n\\r\\f]+\", ' ', text)\n",
    "    \n",
    "    # Replace StockTwits usernames with a space. The usernames are any word that starts with @.\n",
    "    text = re.sub(\"@[^ \\t\\n\\r\\f]+\", ' ', text)\n",
    "\n",
    "    # Replace everything not a letter with a space\n",
    "    text = re.sub(\"[^a-z]\", ' ', text)\n",
    "    \n",
    "    \n",
    "    # Tokenize by splitting the string on whitespace into a list of words\n",
    "    tokens = text.split()\n",
    "\n",
    "    # Lemmatize words using the WordNetLemmatizer. You can ignore any word that is not longer than one character.\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    tokens = [wnl.lemmatize(w, pos='v') for w in tokens if len(w) > 1]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "\n",
    "def predict_func(text, model, vocab):\n",
    "    \"\"\" \n",
    "    Make a prediction on a single sentence.\n",
    "    Parameters\n",
    "    ----------\n",
    "        text : The string to make a prediction on.\n",
    "        model : The model to use for making the prediction.\n",
    "        vocab : Dictionary for word to word ids. The key is the word and the value is the word id.\n",
    "    Returns\n",
    "    -------\n",
    "        pred : Prediction vector\n",
    "    \"\"\"\n",
    "\n",
    "    tokens = preprocess(text)    \n",
    "\n",
    "    # Filter non-vocab words\n",
    "    tokens = [token for token in tokens if token in vocab] #pass\n",
    "    # Convert words to ids\n",
    "    tokens = [vocab[token] for token in tokens] #pass\n",
    "\n",
    "    if len(tokens) == 0:\n",
    "      raise UnknownWordsError\n",
    "\n",
    "    # Adding a batch dimension\n",
    "    text_input = torch.from_numpy(np.asarray(torch.LongTensor(tokens).view(-1, 1)))\n",
    "\n",
    "    # Get the NN output       \n",
    "    batch_size = 1\n",
    "    hidden = model.init_hidden(batch_size) #pass\n",
    "    \n",
    "    logps, _ = model(text_input, hidden) #pass\n",
    "    # Take the exponent of the NN output to get a range of 0 to 1 for each label.\n",
    "    pred = torch.round(logps.squeeze())#pass\n",
    "    pred = torch.exp(logps) \n",
    "    \n",
    "    return pred\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predict_api(args):\n",
    "  text = args.get('text')\n",
    "  try:\n",
    "    result = predict_func(text, model_l, vocab_l)\n",
    "    return result.detach().numpy()[0]\n",
    "  except UnknownWordsError:\n",
    "    return [0,0,1,0,0]\n",
    "    \n",
    "\n",
    "#args = {\"text\": \"Google is working on self driving cars, I'm bullish on $goog\"}\n",
    "#args = {\"text\": \"I'm bullish on $goog\"}\n",
    "args = {\"text\": \"I'll strongly recommend to buy on $goog\"}\n",
    "#args = {\"text\": \"elyoq baoq pquq $goog\"}\n",
    "result = predict_api(args)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, model, vocab):\n",
    "    \"\"\" \n",
    "    Make a prediction on a single sentence.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        text : The string to make a prediction on.\n",
    "        model : The model to use for making the prediction.\n",
    "        vocab : Dictionary for word to word ids. The key is the word and the value is the word id.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        pred : Prediction vector\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO Implement\n",
    "    tokens = preprocess(text)\n",
    "\n",
    "    # Filter non-vocab words\n",
    "    tokens = [token for token in tokens if token in vocab] #pass\n",
    "    # Convert words to ids\n",
    "    tokens = [vocab[token] for token in tokens] #pass\n",
    "\n",
    "    # Adding a batch dimension\n",
    "    text_input = torch.from_numpy(np.asarray(torch.LongTensor(tokens).view(-1, 1)))\n",
    "\n",
    "    # Get the NN output       \n",
    "    batch_size = 1\n",
    "    hidden = model.init_hidden(batch_size) #pass\n",
    "    \n",
    "    logps, _ = model(text_input, hidden) #pass\n",
    "    # Take the exponent of the NN output to get a range of 0 to 1 for each label.\n",
    "    pred = torch.round(logps.squeeze())#pass\n",
    "    pred = torch.exp(logps) \n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1989, 0.1649, 0.1976, 0.1587, 0.2799]], grad_fn=<ExpBackward>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Good good good wonderful\"\n",
    "model.eval()\n",
    "model.to(\"cpu\")\n",
    "predict(text, model, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1995, 0.1990, 0.2000, 0.1916, 0.2100]], grad_fn=<ExpBackward>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Bad bad bad worst\"\n",
    "model.eval()\n",
    "model.to(\"cpu\")\n",
    "predict(text, model, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2138, 0.1600, 0.2078, 0.1529, 0.2655]], grad_fn=<ExpBackward>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Google is working on self driving cars, I'm bullish on $goog\"\n",
    "model.eval()\n",
    "model.to(\"cpu\")\n",
    "predict(text, model, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions: What is the prediction of the model? What is the uncertainty of the prediction?\n",
    "** TODO: Answer Question**\n",
    "\n",
    "#### What is the prediction of the model?\n",
    "The prediction to the text above is positive as the highest value is positive in the probability list - very negative, negative, neutral, positive, very positive.\n",
    "#### What is the uncertainty of the prediction?\n",
    "When considering the sum of the rest values except for the highest class, the uncertainty of the prediction is low and when taking into account the both positive and very positive, the uncertainty is very low. So, the prediction seems appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a trained model and we can make predictions. We can use this model to track the sentiments of various stocks by predicting the sentiments of twits as they are coming in. Now we have a stream of twits. For each of those twits, pull out the stocks mentioned in them and keep track of the sentiments. Remember that in the twits, ticker symbols are encoded with a dollar sign as the first character, all caps, and 2-4 letters, like $AAPL. Ideally, you'd want to track the sentiments of the stocks in your universe and use this as a signal in your larger model(s).\n",
    "\n",
    "## Testing\n",
    "### Load the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('..', '..', 'data', 'project_6_stocktwits', 'test_twits.json'), 'r') as f:\n",
    "    test_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twit Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message_body': '$JWN has moved -1.69% on 10-31. Check out the movement and peers at  https://dividendbot.com?s=JWN',\n",
       " 'timestamp': '2018-11-01T00:00:05Z'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def twit_stream():\n",
    "    for twit in test_data['data']:\n",
    "        yield twit\n",
    "\n",
    "next(twit_stream())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `prediction` function, let's apply it to a stream of twits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_twits(stream, model, vocab, universe):\n",
    "    \"\"\" \n",
    "    Given a stream of twits and a universe of tickers, return sentiment scores for tickers in the universe.\n",
    "    \"\"\"\n",
    "    for twit in stream:\n",
    "\n",
    "        # Get the message text\n",
    "        text = twit['message_body']\n",
    "        symbols = re.findall('\\$[A-Z]{2,4}', text)\n",
    "        score = predict(text, model, vocab)\n",
    "\n",
    "        for symbol in symbols:\n",
    "            if symbol in universe:\n",
    "                yield {'symbol': symbol, 'score': score, 'timestamp': twit['timestamp']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'symbol': '$AAPL',\n",
       " 'score': tensor([[ 0.1006,  0.1506,  0.2158,  0.2898,  0.2432]]),\n",
       " 'timestamp': '2018-11-01T00:00:18Z'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "universe = {'$BBRY', '$AAPL', '$AMZN', '$BABA', '$YHOO', '$LQMT', '$FB', '$GOOG', '$BBBY', '$JNUG', '$SBUX', '$MU'}\n",
    "score_stream = score_twits(twit_stream(), model, vocab, universe)\n",
    "\n",
    "next(score_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it. You have successfully built a model for sentiment analysis! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
