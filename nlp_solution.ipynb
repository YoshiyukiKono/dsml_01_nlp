{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science / Machine Learning Meetup #1 Deep Learning Hands-on\n",
    "# オルタナティブ・データと自然言語処理\n",
    "\n",
    "## はじめに\n",
    "\n",
    "演習の概略は以下の通りです。\n",
    "1. 環境準備\n",
    "1. Web Scraping\n",
    "1. データ変換\n",
    "1. 感情分析\n",
    "    1. 前処理\n",
    "    1. ニューラル・ネットワーク構築\n",
    "    1. トレーニング\n",
    "    1. 予測\n",
    "\n",
    "以下の点にご注意ください。\n",
    "- 実行するコードの中に、ご利用中のユーザー名に合わせて、変更していただく部分があります。\n",
    "\n",
    "## 1. 環境準備\n",
    "\n",
    "### パッケージのインストールとインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk==3.4.5\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5MB 11.0MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk==3.4.5) (1.12.0)\n",
      "Building wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/cdsw/.cache/pip/wheels/96/86/f6/68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n",
      "Successfully built nltk\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.4.5\n",
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting torch==1.4.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/19/4804aea17cd136f1705a5e98a00618cb8f6ccc375ad8bfa437408e09d058/torch-1.4.0-cp36-cp36m-manylinux1_x86_64.whl (753.4MB)\n",
      "\u001b[K     |████████████████████████████████| 753.4MB 52kB/s s eta 0:00:01     |███████                         | 164.6MB 79.3MB/s eta 0:00:08     |███████████▌                    | 270.6MB 63.2MB/s eta 0:00:08     |███████████████████▍            | 457.1MB 65.2MB/s eta 0:00:05     |███████████████████████████     | 637.2MB 60.5MB/s eta 0:00:02     |█████████████████████████████▉  | 701.2MB 44.1MB/s eta 0:00:02     |██████████████████████████████▌ | 716.8MB 44.1MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: torch\n",
      "Successfully installed torch-1.4.0\n",
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting ipython-sql==0.3.9\n",
      "  Downloading https://files.pythonhosted.org/packages/ab/df/427e7cf05ffc67e78672ad57dce2436c1e825129033effe6fcaf804d0c60/ipython_sql-0.3.9-py2.py3-none-any.whl\n",
      "Collecting sqlalchemy>=0.6.7 (from ipython-sql==0.3.9)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/47/35edeb0f86c0b44934c05d961c893e223ef27e79e1f53b5e6f14820ff553/SQLAlchemy-1.3.13.tar.gz (6.0MB)\n",
      "\u001b[K     |████████████████████████████████| 6.0MB 6.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting prettytable (from ipython-sql==0.3.9)\n",
      "  Downloading https://files.pythonhosted.org/packages/ef/30/4b0746848746ed5941f052479e7c23d2b56d174b82f4fd34a25e389831f5/prettytable-0.7.2.tar.bz2\n",
      "Requirement already satisfied: ipython>=1.0 in /usr/local/lib/python3.6/dist-packages (from ipython-sql==0.3.9) (5.1.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from ipython-sql==0.3.9) (1.12.0)\n",
      "Requirement already satisfied: ipython-genutils>=0.1.0 in /usr/local/lib/python3.6/dist-packages (from ipython-sql==0.3.9) (0.2.0)\n",
      "Collecting sqlparse (from ipython-sql==0.3.9)\n",
      "  Downloading https://files.pythonhosted.org/packages/ef/53/900f7d2a54557c6a37886585a91336520e5539e3ae2423ff1102daf4f3a7/sqlparse-0.3.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython>=1.0->ipython-sql==0.3.9) (2.4.2)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython>=1.0->ipython-sql==0.3.9) (4.4.0)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=1.0->ipython-sql==0.3.9) (0.7.5)\n",
      "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython>=1.0->ipython-sql==0.3.9) (4.3.2)\n",
      "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.3 in /usr/local/lib/python3.6/dist-packages (from ipython>=1.0->ipython-sql==0.3.9) (1.0.15)\n",
      "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=1.0->ipython-sql==0.3.9) (0.8.1)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=1.0->ipython-sql==0.3.9) (4.7.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython>=1.0->ipython-sql==0.3.9) (41.0.1)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.3->ipython>=1.0->ipython-sql==0.3.9) (0.1.7)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=1.0->ipython-sql==0.3.9) (0.6.0)\n",
      "Building wheels for collected packages: sqlalchemy, prettytable\n",
      "  Building wheel for sqlalchemy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/cdsw/.cache/pip/wheels/b3/35/98/4c9cb3fd63d21d5606b972dd70643769745adf60e622467b71\n",
      "  Building wheel for prettytable (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/cdsw/.cache/pip/wheels/80/34/1c/3967380d9676d162cb59513bd9dc862d0584e045a162095606\n",
      "Successfully built sqlalchemy prettytable\n",
      "Installing collected packages: sqlalchemy, prettytable, sqlparse, ipython-sql\n",
      "Successfully installed ipython-sql-0.3.9 prettytable-0.7.2 sqlalchemy-1.3.13 sqlparse-0.3.0\n",
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting PyHive==0.6.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4e/26/de91125c0d9e8947d48f387f4d1f2e7a22aa92a30771ad02f63a5653361b/PyHive-0.6.1.tar.gz (41kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 2.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting future (from PyHive==0.6.1)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
      "\u001b[K     |████████████████████████████████| 829kB 11.6MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from PyHive==0.6.1) (2.8.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil->PyHive==0.6.1) (1.12.0)\n",
      "Building wheels for collected packages: PyHive, future\n",
      "  Building wheel for PyHive (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/cdsw/.cache/pip/wheels/00/61/fb/77a0e77deb4c900276f689e62628a5ca7ba9df600f9ad7ba6a\n",
      "  Building wheel for future (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/cdsw/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
      "Successfully built PyHive future\n",
      "Installing collected packages: future, PyHive\n",
      "Successfully installed PyHive-0.6.1 future-0.18.2\n",
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: SQLAlchemy==1.3.13 in ./.local/lib/python3.6/site-packages (1.3.13)\n",
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting thrift==0.13.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/97/1e/3284d19d7be99305eda145b8aa46b0c33244e4a496ec66440dac19f8274d/thrift-0.13.0.tar.gz (59kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 3.0MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.7.2 in /usr/local/lib/python3.6/dist-packages (from thrift==0.13.0) (1.12.0)\n",
      "Building wheels for collected packages: thrift\n",
      "  Building wheel for thrift (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/cdsw/.cache/pip/wheels/02/a2/46/689ccfcf40155c23edc7cdbd9de488611c8fdf49ff34b1706e\n",
      "Successfully built thrift\n",
      "Installing collected packages: thrift\n",
      "Successfully installed thrift-0.13.0\n",
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting sasl==0.2.1\n",
      "  Downloading https://files.pythonhosted.org/packages/8e/2c/45dae93d666aea8492678499e0999269b4e55f1829b1e4de5b8204706ad9/sasl-0.2.1.tar.gz\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sasl==0.2.1) (1.12.0)\n",
      "Building wheels for collected packages: sasl\n",
      "  Building wheel for sasl (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/cdsw/.cache/pip/wheels/56/20/21/ff481fd0f4ae09d5d94c76d089f550204580b1703e44f27dd5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully built sasl\n",
      "Installing collected packages: sasl\n",
      "Successfully installed sasl-0.2.1\n",
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting thrift_sasl==0.3.0\n",
      "  Downloading https://files.pythonhosted.org/packages/50/fe/89cbc910809e3757c762f56ee190ca39e0f28b7ea451835232c0c988d706/thrift_sasl-0.3.0.tar.gz\n",
      "Requirement already satisfied: thrift>=0.10.0 in ./.local/lib/python3.6/site-packages (from thrift_sasl==0.3.0) (0.13.0)\n",
      "Requirement already satisfied: sasl>=0.2.1 in ./.local/lib/python3.6/site-packages (from thrift_sasl==0.3.0) (0.2.1)\n",
      "Requirement already satisfied: six>=1.7.2 in /usr/local/lib/python3.6/dist-packages (from thrift>=0.10.0->thrift_sasl==0.3.0) (1.12.0)\n",
      "Building wheels for collected packages: thrift-sasl\n",
      "  Building wheel for thrift-sasl (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/cdsw/.cache/pip/wheels/c8/3a/34/1d82df3d652788fc211c245d51dde857a58e603695ea41d93d\n",
      "Successfully built thrift-sasl\n",
      "Installing collected packages: thrift-sasl\n",
      "Successfully installed thrift-sasl-0.3.0\n",
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install ipython-sql==0.3.9\n",
    "!pip3 install PyHive==0.6.1\n",
    "!pip3 install SQLAlchemy==1.3.13\n",
    "!pip3 install thrift==0.13.0\n",
    "!pip3 install sasl==0.2.1\n",
    "!pip3 install thrift_sasl==0.3.0\n",
    "\n",
    "!pip3 install nltk==3.4.5\n",
    "!pip3 install torch==1.4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記でインストールしたPyHiveは、Pythonコードの中でimportして使われるのではなく、Hiveへの接続の際の接続文字列`sqlalchemy.create_engine('hive://<host>:<port>')`の中でdialectsとして指定された際に必要になります。そのため、インストール後に利用するためには、新しくプロセスを始める必要があります。**インストールした後に一度、KernelをRestartしてください。**インストールしたプロセスでは、接続時に下記のようなエラーが発生します。\n",
    "`NoSuchModuleError: Can't load plugin: sqlalchemy.dialects:hive`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import subprocess\n",
    "import glob\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "\n",
    "from pyhive import hive\n",
    "import sqlalchemy\n",
    "\n",
    "import sys\n",
    "#from random import random\n",
    "from operator import add\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import torch\n",
    "import nltk\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Web Scraping\n",
    "\n",
    "無償で利用できるAPIを用いて演習を行います。そのため、利用に一定の制限が課せられることにご留意ください。\n",
    "例えば、ご利用状況に応じて、下記のようなエラーメッセージを受け取ることがあります。\n",
    "\n",
    "```\n",
    "{\"response\":{\"status\":429},\"errors\":[{\"message\":\"Rate limit exceeded. Client may not make more than 200 requests an hour.\"}]}\n",
    "```\n",
    "まず、APIで取得したデータをCDSWプロジェクト内のファイルとして保存します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘./data’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2882\n",
      "['A', 'AA', 'AAL', 'AAN', 'AAOI', 'AAON', 'AAP', 'AAPL', 'AAWW', 'AAXN', 'ABBV', 'ABC', 'ABCB', 'ABEO', 'ABG', 'ABM', 'ABMD', 'ABT', 'ABTX', 'ACA', 'ACAD', 'ACCO', 'ACEL', 'ACGL', 'ACHC', 'ACHN', 'ACHV', 'ACIA', 'ACIW', 'ACLS', 'ACM', 'ACN', 'ACNB', 'ACOR', 'ACRS', 'ACRX', 'ACTG', 'ADBE', 'ADES', 'ADI', 'ADM', 'ADMA', 'ADMP', 'ADMS', 'ADP', 'ADPT', 'ADRO', 'ADS', 'ADSK', 'ADSW', 'ADT', 'ADTN', 'ADUS', 'ADVM', 'ADXS', 'AE', 'AEE', 'AEGN', 'AEIS', 'AEL', 'AEM', 'AEMD', 'AEO', 'AEP', 'AERI', 'AES', 'AFG', 'AFI', 'AFL', 'AG', 'AGCO', 'AGEN', 'AGFS', 'AGI', 'AGIO', 'AGLE', 'AGM', 'AGN', 'AGO', 'AGR', 'AGRX', 'AGS', 'AGTC', 'AGX', 'AGYS', 'AHC', 'AHCO', 'AIG', 'AIMC', 'AIMT', 'AIN', 'AIR', 'AIRG', 'AIRT', 'AIT', 'AIZ', 'AJG', 'AJRD', 'AKAM', 'AKBA', 'AKCA', 'AKRO', 'AKRX', 'AKS', 'AL', 'ALB', 'ALCO', 'ALDX', 'ALE', 'ALEC', 'ALG', 'ALGN', 'ALGT', 'ALIM', 'ALK', 'ALKS', 'ALL', 'ALLK', 'ALLO', 'ALLY', 'ALNY', 'ALOT', 'ALPN', 'ALRM', 'ALRN', 'ALSK', 'ALSN', 'ALT', 'ALTR', 'ALV', 'ALXN', 'AM', 'AMAG', 'AMAL', 'AMAT', 'AMBA', 'AMBC', 'AMC', 'AMCX', 'AMD', 'AME', 'AMED', 'AMEH', 'AMG', 'AMGN', 'AMK', 'AMKR', 'AMN', 'AMNB', 'AMOT', 'AMP', 'AMPE', 'AMPH', 'AMRC', 'AMRS', 'AMRX', 'AMSC', 'AMSF', 'AMSWA', 'AMTB', 'AMTD', 'AMTX', 'AMWD', 'AMZN', 'AN', 'ANAB', 'ANAT', 'ANDA', 'ANDE', 'ANET', 'ANF', 'ANGI', 'ANGO', 'ANIK', 'ANIP', 'ANIX', 'ANSS', 'ANTM', 'AOBC', 'AON', 'AOS', 'AP', 'APA', 'APD', 'APDN', 'APEI', 'APEN', 'APH', 'APLS', 'APLT', 'APOG', 'APPF', 'APPN', 'APPS', 'APRE', 'APRN', 'APT', 'APTV', 'APY', 'AQB', 'AQN', 'AQUA', 'AR', 'ARA', 'ARAV', 'ARAY', 'ARCB', 'ARCH', 'ARCO', 'ARCT', 'ARDS', 'ARDX', 'ARES', 'ARGO', 'ARKR', 'ARLO', 'ARMK', 'ARMP', 'ARNA', 'ARNC', 'AROC', 'AROW', 'ARTNA', 'ARVN', 'ARW', 'ARWR', 'ASB', 'ASFI', 'ASGN', 'ASH', 'ASIX', 'ASMB', 'ASNA', 'ASPS', 'ASPU', 'ASRT', 'ASTE', 'ASYS', 'ATEC', 'ATEN', 'ATEX', 'ATGE', 'ATH', 'ATHX', 'ATI', 'ATKR', 'ATLO', 'ATNI', 'ATNX', 'ATO', 'ATR', 'ATRA', 'ATRC', 'ATRI', 'ATRO', 'ATRS', 'ATSG', 'ATUS', 'ATVI', 'AUB', 'AUMN', 'AUPH', 'AUTO', 'AUY', 'AVA', 'AVAV', 'AVD', 'AVGO', 'AVID', 'AVLR', 'AVNS', 'AVNW', 'AVRO', 'AVT', 'AVTR', 'AVX', 'AVXL', 'AVY', 'AVYA', 'AWI', 'AWK', 'AWR', 'AWRE', 'AX', 'AXAS', 'AXDX', 'AXGN', 'AXGT', 'AXL', 'AXLA', 'AXNX', 'AXP', 'AXS', 'AXSM', 'AXTA', 'AXTI', 'AYI', 'AYX', 'AZO', 'AZPN', 'AZZ', 'B', 'BA', 'BAC', 'BAH', 'BAM', 'BANC', 'BAND', 'BANF', 'BANR', 'BAP', 'BAX', 'BB', 'BBBY', 'BBI', 'BBIO', 'BBQ', 'BBW', 'BBY', 'BC', 'BCBP', 'BCC', 'BCE', 'BCEI', 'BCEL', 'BCLI', 'BCO', 'BCOR', 'BCOV', 'BCPC', 'BCRX', 'BDC', 'BDGE', 'BDSI', 'BDX', 'BE', 'BEAT', 'BECN', 'BELFB', 'BEN', 'BERY', 'BFAM', 'BFC', 'BFIN', 'BG', 'BGCP', 'BGFV', 'BGG', 'BGS', 'BH', 'BHB', 'BHC', 'BHE', 'BHF', 'BHLB', 'BIG', 'BIIB', 'BIMI', 'BIO', 'BIOS', 'BJ', 'BJRI', 'BK', 'BKD', 'BKE', 'BKH', 'BKI', 'BKNG', 'BKR', 'BKU', 'BL', 'BLBD', 'BLCM', 'BLD', 'BLDP', 'BLDR', 'BLFS', 'BLK', 'BLKB', 'BLL', 'BLMN', 'BLNK', 'BLUE', 'BLX', 'BMCH', 'BMI', 'BMO', 'BMRC', 'BMRN', 'BMTC', 'BMY', 'BNED', 'BNFT', 'BNGO', 'BNS', 'BOH', 'BOKF', 'BOMN', 'BOOM', 'BOOT', 'BOX', 'BPFH', 'BPMC', 'BPTH', 'BR', 'BRC', 'BRKB', 'BRKL', 'BRKR', 'BRKS', 'BRMK', 'BRO', 'BRY', 'BSET', 'BSIG', 'BSQR', 'BSRR', 'BSTC', 'BSX', 'BTG', 'BURL', 'BUSE', 'BV', 'BWA', 'BWEN', 'BWXT', 'BX', 'BXC', 'BXG', 'BXS', 'BY', 'BYD', 'BYND', 'BZH', 'C', 'CABO', 'CAC', 'CACC', 'CACI', 'CADE', 'CAG', 'CAH', 'CAI', 'CAKE', 'CAL', 'CALA', 'CALM', 'CALX', 'CAMP', 'CAPR', 'CAR', 'CARA', 'CARE', 'CARG', 'CARS', 'CASA', 'CASH', 'CASI', 'CASS', 'CASY', 'CAT', 'CATB', 'CATM', 'CATO', 'CATY', 'CBIO', 'CBMG', 'CBPO', 'CBPX', 'CBRE', 'CBRL', 'CBSH', 'CBT', 'CBTX', 'CBU', 'CBZ', 'CC', 'CCBG', 'CCEP', 'CCF', 'CCJ', 'CCK', 'CCL', 'CCMP', 'CCNE', 'CCO', 'CCOI', 'CCRN', 'CCS', 'CCXI', 'CDAY', 'CDE', 'CDK', 'CDLX', 'CDMO', 'CDNA', 'CDNS', 'CDW', 'CDXS', 'CE', 'CECE', 'CEIX', 'CEL', 'CELH', 'CEMI', 'CENT', 'CENTA', 'CENX', 'CERN', 'CERS', 'CEVA', 'CF', 'CFB', 'CFFI', 'CFFN', 'CFG', 'CFMS', 'CFR', 'CFX', 'CGC', 'CGEN', 'CGNX', 'CHCO', 'CHD', 'CHDN', 'CHE', 'CHEF', 'CHGG', 'CHH', 'CHK', 'CHKP', 'CHMA', 'CHMG', 'CHNG', 'CHRS', 'CHRW', 'CHS', 'CHTR', 'CHUY', 'CHWY', 'CI', 'CIEN', 'CINF', 'CIR', 'CIT', 'CKH', 'CKPT', 'CL', 'CLAR', 'CLBK', 'CLBS', 'CLCT', 'CLDR', 'CLDX', 'CLF', 'CLFD', 'CLGX', 'CLH', 'CLNE', 'CLPS', 'CLR', 'CLSD', 'CLSN', 'CLVS', 'CLW', 'CLX', 'CLXT', 'CM', 'CMA', 'CMBM', 'CMC', 'CMCO', 'CMCSA', 'CMD', 'CME', 'CMG', 'CMI', 'CMLS', 'CMP', 'CMS', 'CMTL', 'CNA', 'CNBKA', 'CNC', 'CNCE', 'CNDT', 'CNI', 'CNK', 'CNMD', 'CNNE', 'CNO', 'CNOB', 'CNP', 'CNQ', 'CNR', 'CNS', 'CNSL', 'CNST', 'CNX', 'CNXN', 'CO', 'COF', 'COG', 'COHR', 'COHU', 'COKE', 'COLB', 'COLL', 'COLM', 'COMM', 'CONN', 'COO', 'COOP', 'COP', 'CORE', 'CORT', 'COST', 'COT', 'COTY', 'COUP', 'CP', 'CPA', 'CPB', 'CPE', 'CPF', 'CPG', 'CPIX', 'CPK', 'CPRI', 'CPRT', 'CPRX', 'CPS', 'CPSI', 'CPST', 'CR', 'CRAI', 'CRBP', 'CRC', 'CRCM', 'CREE', 'CRI', 'CRIS', 'CRK', 'CRL', 'CRM', 'CRMD', 'CRMT', 'CRNC', 'CRNT', 'CRNX', 'CRON', 'CROX', 'CRS', 'CRTX', 'CRUS', 'CRVL', 'CRVS', 'CRWD', 'CRWS', 'CRY', 'CSBR', 'CSCO', 'CSFL', 'CSGP', 'CSGS', 'CSII', 'CSIQ', 'CSL', 'CSOD', 'CSPI', 'CSS', 'CSSE', 'CSTL', 'CSU', 'CSV', 'CSWI', 'CSX', 'CTAS', 'CTB', 'CTBI', 'CTG', 'CTIC', 'CTL', 'CTLT', 'CTMX', 'CTO', 'CTRA', 'CTRN', 'CTS', 'CTSH', 'CTSO', 'CTVA', 'CTXS', 'CUB', 'CUBI', 'CULP', 'CURO', 'CUTR', 'CVA', 'CVBF', 'CVCO', 'CVE', 'CVET', 'CVGW', 'CVI', 'CVLT', 'CVLY', 'CVM', 'CVNA', 'CVS', 'CVTI', 'CVU', 'CVX', 'CW', 'CWBC', 'CWCO', 'CWEN', 'CWH', 'CWK', 'CWST', 'CWT', 'CXDC', 'CXO', 'CY', 'CYAN', 'CYBE', 'CYBR', 'CYCN', 'CYD', 'CYH', 'CYRX', 'CYTK', 'CZNC', 'CZR', 'CZWI', 'CZZ', 'D', 'DAIO', 'DAKT', 'DAL', 'DAN', 'DAR', 'DARE', 'DB', 'DBD', 'DBI', 'DBX', 'DCI', 'DCO', 'DCOM', 'DCPH', 'DD', 'DDD', 'DDOG', 'DDS', 'DE', 'DECK', 'DELL', 'DENN', 'DERM', 'DFIN', 'DFS', 'DG', 'DGICA', 'DGII', 'DGLY', 'DGX', 'DHI', 'DHIL', 'DHR', 'DHT', 'DIN', 'DIOD', 'DIS', 'DISCA', 'DISCK', 'DISH', 'DJCO', 'DK', 'DKS', 'DLA', 'DLB', 'DLTR', 'DLX', 'DMRC', 'DNKN', 'DNLI', 'DNOW', 'DNR', 'DO', 'DOCU', 'DOMO', 'DOOR', 'DORM', 'DOV', 'DOW', 'DPLO', 'DPZ', 'DRAD', 'DRI', 'DRNA', 'DRQ', 'DSGX', 'DSKE', 'DSPG', 'DT', 'DTE', 'DTIL', 'DUK', 'DVA', 'DVAX', 'DVD', 'DVN', 'DXC', 'DXCM', 'DXPE', 'DXR', 'DY', 'DZSI', 'EA', 'EAF', 'EAT', 'EB', 'EBAY', 'EBF', 'EBIX', 'EBS', 'EBSB', 'EBTC', 'ECHO', 'ECL', 'ECOL', 'ECOM', 'ECPG', 'ED', 'EDIT', 'EDSA', 'EDTX', 'EDUC', 'EE', 'EEFT', 'EEX', 'EFC', 'EFSC', 'EFX', 'EGAN', 'EGBN', 'EGHT', 'EGO', 'EGOV', 'EGRX', 'EGY', 'EHC', 'EHTH', 'EIDX', 'EIG', 'EIGI', 'EIGR', 'EIX', 'EKSO', 'EL', 'ELAN', 'ELF', 'ELGX', 'ELY', 'EMCF', 'EME', 'EMKR', 'EML', 'EMMS', 'EMN', 'EMR', 'ENB', 'ENDP', 'ENLV', 'ENPH', 'ENR', 'ENS', 'ENSG', 'ENTA', 'ENTG', 'ENV', 'ENVA', 'EOG', 'EOLS', 'EPAC', 'EPAM', 'EPAY', 'EPC', 'EPIX', 'EPM', 'EPZM', 'EQH', 'EQT', 'ERA', 'ERF', 'ERI', 'ERIE', 'ERII', 'ES', 'ESCA', 'ESE', 'ESGR', 'ESLT', 'ESNT', 'ESPR', 'ESSA', 'ETFC', 'ETH', 'ETM', 'ETN', 'ETNB', 'ETR', 'ETSY', 'EV', 'EVBG', 'EVC', 'EVER', 'EVFM', 'EVH', 'EVOP', 'EVR', 'EVRG', 'EVRI', 'EW', 'EWBC', 'EXAS', 'EXC', 'EXEL', 'EXK', 'EXLS', 'EXP', 'EXPD', 'EXPE', 'EXPI', 'EXPO', 'EXPR', 'EXTR', 'EYE', 'EYPT', 'EZPW', 'F', 'FAF', 'FANG', 'FARM', 'FARO', 'FAST', 'FAT', 'FATE', 'FB', 'FBC', 'FBHS', 'FBIZ', 'FBK', 'FBM', 'FBMS', 'FBNC', 'FC', 'FCBC', 'FCEL', 'FCF', 'FCFS', 'FCN', 'FCNCA', 'FCX', 'FDEF', 'FDP', 'FDS', 'FDX', 'FE', 'FEIM', 'FELE', 'FEYE', 'FF', 'FFBC', 'FFG', 'FFIC', 'FFIN', 'FFIV', 'FFWM', 'FG', 'FGEN', 'FHB', 'FHN', 'FIBK', 'FICO', 'FII', 'FIS', 'FISI', 'FISV', 'FIT', 'FITB', 'FIVE', 'FIVN', 'FIX', 'FIXX', 'FIZZ', 'FL', 'FLDM', 'FLEX', 'FLGT', 'FLIC', 'FLIR', 'FLMN', 'FLNT', 'FLO', 'FLOW', 'FLR', 'FLS', 'FLT', 'FLWS', 'FLXN', 'FLXS', 'FMBH', 'FMBI', 'FMC', 'FMNB', 'FN', 'FNB', 'FND', 'FNF', 'FNHC', 'FNJN', 'FNKO', 'FNLC', 'FNV', 'FOCS', 'FOE', 'FOLD', 'FOMX', 'FONR', 'FOR', 'FORM', 'FORR', 'FOSL', 'FOX', 'FOXA', 'FOXF', 'FPRX', 'FRAN', 'FRC', 'FREQ', 'FRGI', 'FRHC', 'FRME', 'FRO', 'FRPH', 'FRPT', 'FRTA', 'FSB', 'FSBW', 'FSCT', 'FSFG', 'FSI', 'FSLR', 'FSLY', 'FSM', 'FSS', 'FSTR', 'FTCH', 'FTDR', 'FTEK', 'FTI', 'FTNT', 'FTR', 'FTS', 'FTSV', 'FTV', 'FUL', 'FULC', 'FULT', 'FVE', 'FVRR', 'FWONA', 'FWRD', 'GABC', 'GALT', 'GATX', 'GBCI', 'GBL', 'GBLI', 'GBT', 'GBX', 'GCAP', 'GCBC', 'GCI', 'GCO', 'GCP', 'GD', 'GDDY', 'GDEN', 'GDI', 'GDOT', 'GE', 'GEC', 'GEF', 'GEOS', 'GERN', 'GES', 'GEVO', 'GFF', 'GGG', 'GH', 'GHC', 'GHL', 'GHM', 'GIB', 'GIFI', 'GIII', 'GIL', 'GILD', 'GIS', 'GKOS', 'GL', 'GLDD', 'GLMD', 'GLNG', 'GLRE', 'GLT', 'GLUU', 'GLW', 'GLYC', 'GM', 'GME', 'GMED', 'GMS', 'GNC', 'GNCA', 'GNE', 'GNMK', 'GNMX', 'GNRC', 'GNTX', 'GNW', 'GO', 'GOGO', 'GOLD', 'GOLF', 'GOOG', 'GOOGL', 'GOOS', 'GORO', 'GOSS', 'GPC', 'GPI', 'GPK', 'GPN', 'GPOR', 'GPRE', 'GPRK', 'GPRO', 'GPS', 'GPX', 'GRA', 'GRBK', 'GRC', 'GRIF', 'GRPN', 'GRTS', 'GRUB', 'GS', 'GSB', 'GSBC', 'GSHD', 'GSKY', 'GT', 'GTES', 'GTHX', 'GTLS', 'GTN', 'GTT', 'GTX', 'GVA', 'GWB', 'GWRE', 'GWRS', 'GWW', 'H', 'HA', 'HABT', 'HAE', 'HAFC', 'HAIN', 'HAL', 'HALO', 'HARP', 'HAS', 'HAYN', 'HBAN', 'HBB', 'HBCP', 'HBI', 'HBIO', 'HBM', 'HBNC', 'HBT', 'HCA', 'HCAT', 'HCC', 'HCCI', 'HCI', 'HCKT', 'HCSG', 'HD', 'HDS', 'HDSN', 'HE', 'HEAR', 'HEES', 'HEI', 'HELE', 'HEPA', 'HES', 'HFBL', 'HFC', 'HFFG', 'HFWA', 'HGV', 'HHC', 'HHS', 'HI', 'HIBB', 'HIFS', 'HIG', 'HII', 'HIIQ', 'HJLI', 'HL', 'HLF', 'HLI', 'HLIO', 'HLIT', 'HLT', 'HLX', 'HMHC', 'HMN', 'HMST', 'HMSY', 'HMTV', 'HNGR', 'HNI', 'HOFT', 'HOG', 'HOLX', 'HOMB', 'HOME', 'HON', 'HONE', 'HOPE', 'HOV', 'HP', 'HPE', 'HPQ', 'HQY', 'HRB', 'HRC', 'HRI', 'HRL', 'HROW', 'HRTG', 'HRTX', 'HSC', 'HSIC', 'HSII', 'HSKA', 'HSTM', 'HSY', 'HTBI', 'HTBK', 'HTGM', 'HTH', 'HTLD', 'HTLF', 'HUBB', 'HUBG', 'HUBS', 'HUD', 'HUM', 'HUN', 'HURC', 'HURN', 'HVT', 'HWC', 'HWCC', 'HWKN', 'HXL', 'HY', 'HYRE', 'HZO', 'IAA', 'IAC', 'IAG', 'IART', 'IBCP', 'IBKC', 'IBKR', 'IBM', 'IBOC', 'IBP', 'IBTX', 'ICBK', 'ICE', 'ICFI', 'ICHR', 'ICON', 'ICPT', 'ICUI', 'IDA', 'IDCC', 'IDRA', 'IDT', 'IDXX', 'IESC', 'IEX', 'IFF', 'IGMS', 'IGT', 'IHC', 'IIIN', 'IIIV', 'IIVI', 'ILMN', 'IMAX', 'IMGN', 'IMH', 'IMKTA', 'IMMR', 'IMMU', 'IMUX', 'IMXI', 'INAP', 'INBK', 'INCY', 'INDB', 'INFN', 'INFO', 'INGN', 'INGR', 'INMD', 'INO', 'INOD', 'INOV', 'INS', 'INSG', 'INSM', 'INSP', 'INST', 'INT', 'INTC', 'INTL', 'INTU', 'INVA', 'INVE', 'IO', 'IONS', 'IOSP', 'IOTS', 'IOVA', 'IP', 'IPAR', 'IPG', 'IPGP', 'IPHI', 'IPHS', 'IPI', 'IPWR', 'IQV', 'IR', 'IRBT', 'IRDM', 'IRMD', 'IRTC', 'IRWD', 'ISBC', 'ISEE', 'ISNS', 'ISRG', 'IT', 'ITCI', 'ITGR', 'ITIC', 'ITRI', 'ITT', 'ITW', 'IVC', 'IVZ', 'J', 'JACK', 'JAZZ', 'JBHT', 'JBL', 'JBLU', 'JBSS', 'JBT', 'JCI', 'JCOM', 'JCP', 'JEF', 'JELD', 'JILL', 'JJSF', 'JKHY', 'JLL', 'JNCE', 'JNJ', 'JNPR', 'JOE', 'JOUT', 'JPM', 'JRVR', 'JVA', 'JWN', 'JYNT', 'K', 'KAI', 'KALA', 'KALU', 'KALV', 'KAMN', 'KAR', 'KBH', 'KBR', 'KDMN', 'KDP', 'KE', 'KELYA', 'KEM', 'KEX', 'KEY', 'KEYS', 'KFRC', 'KFS', 'KFY', 'KGC', 'KHC', 'KIDS', 'KIRK', 'KL', 'KLAC', 'KLIC', 'KMB', 'KMI', 'KMPR', 'KMT', 'KMX', 'KN', 'KNDI', 'KNL', 'KNSA', 'KNSL', 'KNX', 'KO', 'KOD', 'KODK', 'KOP', 'KOPN', 'KOS', 'KPTI', 'KR', 'KRA', 'KRNT', 'KRNY', 'KRO', 'KRTX', 'KRUS', 'KRYS', 'KSS', 'KSU', 'KTB', 'KTCC', 'KTOS', 'KURA', 'KVHI', 'KW', 'KWR', 'L', 'LAD', 'LAKE', 'LANC', 'LASR', 'LAUR', 'LAWS', 'LB', 'LBAI', 'LBC', 'LBRDA', 'LBRDK', 'LBRT', 'LBTYA', 'LBTYK', 'LBY', 'LC', 'LCI', 'LCII', 'LCNB', 'LCUT', 'LDL', 'LDOS', 'LE', 'LEA', 'LEAF', 'LECO', 'LEG', 'LEGH', 'LEN', 'LEVI', 'LFUS', 'LFVN', 'LGIH', 'LGND', 'LH', 'LHCG', 'LHX', 'LII', 'LILA', 'LILAK', 'LIN', 'LINC', 'LIND', 'LITE', 'LIVE', 'LIVN', 'LJPC', 'LKFN', 'LKQ', 'LL', 'LLNW', 'LLY', 'LM', 'LMAT', 'LMNR', 'LMNX', 'LMT', 'LNC', 'LNDC', 'LNG', 'LNN', 'LNT', 'LNTH', 'LOB', 'LOCO', 'LOGC', 'LOGM', 'LOOP', 'LOPE', 'LORL', 'LOVE', 'LOW', 'LPCN', 'LPI', 'LPLA', 'LPSN', 'LPX', 'LQDA', 'LQDT', 'LRCX', 'LRN', 'LSCC', 'LSTR', 'LTHM', 'LTRPA', 'LTS', 'LULU', 'LUNA', 'LUV', 'LVGO', 'LVS', 'LW', 'LWAY', 'LXRX', 'LXU', 'LYFT', 'LYTS', 'LYV', 'LZB', 'M', 'MA', 'MACK', 'MAGS', 'MAN', 'MANH', 'MANT', 'MANU', 'MAR', 'MARA', 'MAS', 'MASI', 'MAT', 'MATW', 'MATX', 'MAXR', 'MBI', 'MBIN', 'MBIO', 'MBOT', 'MBUU', 'MBWM', 'MC', 'MCD', 'MCF', 'MCHP', 'MCHX', 'MCK', 'MCO', 'MCRB', 'MCRI', 'MCS', 'MCY', 'MD', 'MDB', 'MDC', 'MDGL', 'MDLA', 'MDLZ', 'MDP', 'MDRX', 'MDT', 'MDU', 'MDWD', 'MEC', 'MED', 'MEDP', 'MEET', 'MEI', 'MELI', 'MEOH', 'MERC', 'MESA', 'MET', 'MFC', 'MFIN', 'MFSF', 'MG', 'MGA', 'MGEE', 'MGI', 'MGIC', 'MGLN', 'MGM', 'MGNX', 'MGPI', 'MGRC', 'MGTA', 'MGTX', 'MGY', 'MHK', 'MHO', 'MIC', 'MIDD', 'MIK', 'MIME', 'MINI', 'MIRM', 'MIST', 'MITK', 'MKC', 'MKL', 'MKSI', 'MKTX', 'MLAB', 'MLHR', 'MLI', 'MLM', 'MLND', 'MLNT', 'MLNX', 'MLR', 'MMC', 'MMM', 'MMS', 'MMSI', 'MMYT', 'MNI', 'MNK', 'MNKD', 'MNLO', 'MNOV', 'MNRO', 'MNST', 'MNTA', 'MO', 'MOBL', 'MOD', 'MODN', 'MOFG', 'MOH', 'MORF', 'MORN', 'MOS', 'MOV', 'MPAA', 'MPC', 'MPWR', 'MPX', 'MRAM', 'MRC', 'MRCY', 'MRIN', 'MRK', 'MRKR', 'MRLN', 'MRNA', 'MRNS', 'MRO', 'MRTN', 'MRTX', 'MRVL', 'MS', 'MSA', 'MSBI', 'MSCI', 'MSEX', 'MSFT', 'MSG', 'MSGN', 'MSI', 'MSM', 'MSON', 'MSTR', 'MTB', 'MTBC', 'MTCH', 'MTD', 'MTDR', 'MTEM', 'MTEX', 'MTG', 'MTH', 'MTN', 'MTOR', 'MTRN', 'MTRX', 'MTSC', 'MTSI', 'MTW', 'MTX', 'MTZ', 'MU', 'MUR', 'MUSA', 'MUX', 'MVIS', 'MWA', 'MXIM', 'MXL', 'MYE', 'MYGN', 'MYL', 'MYOK', 'MYOV', 'MYRG', 'NAII', 'NAT', 'NATH', 'NATI', 'NATR', 'NAV', 'NAVI', 'NBEV', 'NBHC', 'NBIX', 'NBL', 'NBR', 'NBSE', 'NBTB', 'NC', 'NCBS', 'NCLH', 'NCMI', 'NCR', 'NDAQ', 'NDLS', 'NDSN', 'NEE', 'NEM', 'NEO', 'NEOG', 'NEON', 'NEP', 'NERV', 'NET', 'NETE', 'NEU', 'NEWR', 'NEXT', 'NFBK', 'NFE', 'NFG', 'NFLX', 'NG', 'NGHC', 'NGM', 'NGS', 'NGVC', 'NGVT', 'NHC', 'NHTC', 'NI', 'NJR', 'NK', 'NKE', 'NKSH', 'NKTR', 'NL', 'NLNK', 'NLOK', 'NLSN', 'NLTX', 'NMIH', 'NMRK', 'NNBR', 'NNI', 'NOC', 'NOV', 'NOVA', 'NOVN', 'NOVT', 'NOW', 'NP', 'NPK', 'NPO', 'NPTN', 'NR', 'NRC', 'NRG', 'NRIM', 'NSC', 'NSIT', 'NSP', 'NSSC', 'NSTG', 'NTAP', 'NTB', 'NTCT', 'NTGR', 'NTLA', 'NTNX', 'NTR', 'NTRA', 'NTRS', 'NTUS', 'NTWK', 'NUAN', 'NUE', 'NUS', 'NUVA', 'NVAX', 'NVCN', 'NVCR', 'NVDA', 'NVEC', 'NVEE', 'NVMI', 'NVR', 'NVRO', 'NVST', 'NVTA', 'NWBI', 'NWE', 'NWFL', 'NWHM', 'NWL', 'NWLI', 'NWN', 'NWPX', 'NWS', 'NWSA', 'NX', 'NXGN', 'NXPI', 'NXST', 'NXTC', 'NYCB', 'NYT', 'OAS', 'OBCI', 'OBNK', 'OC', 'OCFC', 'OCGN', 'OCN', 'OCUL', 'ODC', 'ODFL', 'ODP', 'ODT', 'OFIX', 'OFLX', 'OGE', 'OGS', 'OI', 'OII', 'OIS', 'OKE', 'OKTA', 'OLED', 'OLLI', 'OLN', 'OMC', 'OMCL', 'OMER', 'OMEX', 'OMF', 'OMI', 'ON', 'ONB', 'ONCS', 'ONCT', 'ONDK', 'ONTO', 'ONTX', 'OOMA', 'OPB', 'OPES', 'OPGN', 'OPK', 'OPRT', 'OPTN', 'OPTT', 'OPY', 'ORA', 'ORBC', 'ORCC', 'ORCL', 'ORI', 'ORLY', 'ORMP', 'ORRF', 'OSIS', 'OSK', 'OSPN', 'OSTK', 'OSUR', 'OTEX', 'OTIC', 'OTTR', 'OVV', 'OXM', 'OXY', 'OZK', 'P', 'PAAS', 'PACB', 'PACQ', 'PACW', 'PAG', 'PAGP', 'PAGS', 'PAH', 'PAHC', 'PANW', 'PAR', 'PARR', 'PATK', 'PAYC', 'PAYS', 'PAYX', 'PB', 'PBCT', 'PBF', 'PBH', 'PBI', 'PBPB', 'PBYI', 'PCAR', 'PCG', 'PCOM', 'PCRX', 'PCTI', 'PCTY', 'PCYG', 'PD', 'PDCE', 'PDCO', 'PDFS', 'PDLI', 'PE', 'PEBO', 'PEG', 'PEGA', 'PEGI', 'PEIX', 'PEN', 'PENN', 'PEP', 'PERI', 'PETQ', 'PETS', 'PFBC', 'PFBI', 'PFE', 'PFG', 'PFGC', 'PFIS', 'PFNX', 'PFPT', 'PFS', 'PFSI', 'PFSW', 'PG', 'PGC', 'PGNX', 'PGNY', 'PGR', 'PGTI', 'PH', 'PHAS', 'PHAT', 'PHM', 'PHR', 'PHX', 'PI', 'PICO', 'PII', 'PINC', 'PING', 'PINS', 'PIR', 'PIRS', 'PJT', 'PKE', 'PKG', 'PKI', 'PKOH', 'PLAB', 'PLAN', 'PLAY', 'PLCE', 'PLIN', 'PLMR', 'PLNT', 'PLOW', 'PLPC', 'PLSE', 'PLT', 'PLUG', 'PLUS', 'PLXP', 'PLXS', 'PLYA', 'PM', 'PMD', 'PME', 'PNC', 'PNFP', 'PNM', 'PNR', 'PNRG', 'PNTG', 'PNW', 'PODD', 'POL', 'POOL', 'POR', 'POST', 'POWI', 'POWL', 'PPBI', 'PPC', 'PPG', 'PPIH', 'PPL', 'PPSI', 'PQG', 'PRA', 'PRAA', 'PRAH', 'PRCP', 'PRFT', 'PRGO', 'PRGS', 'PRI', 'PRIM', 'PRK', 'PRLB', 'PRMW', 'PRNB', 'PRO', 'PROS', 'PROV', 'PRPL', 'PRSC', 'PRSP', 'PRTA', 'PRTK', 'PRTY', 'PRU', 'PRVB', 'PRVL', 'PS', 'PSMT', 'PSN', 'PSNL', 'PSTG', 'PSTI', 'PSTV', 'PSX', 'PTC', 'PTCT', 'PTEN', 'PTGX', 'PTI', 'PTLA', 'PTON', 'PTSI', 'PTVCB', 'PUB', 'PUMP', 'PVG', 'PVH', 'PWOD', 'PWR', 'PXD', 'PXLW', 'PYPL', 'PZN', 'PZZA', 'QADA', 'QCOM', 'QCRH', 'QDEL', 'QEP', 'QLYS', 'QNST', 'QRTEA', 'QRVO', 'QSR', 'QTRX', 'QTWO', 'QUAD', 'QUIK', 'QUMU', 'QUOT', 'R', 'RACE', 'RAD', 'RADA', 'RAIL', 'RAMP', 'RAPT', 'RARE', 'RAVE', 'RAVN', 'RBA', 'RBBN', 'RBC', 'RBCAA', 'RBCN', 'RCI', 'RCII', 'RCKT', 'RCKY', 'RCL', 'RCM', 'RCUS', 'RDFN', 'RDI', 'RDN', 'RDNT', 'RDUS', 'RDWR', 'RE', 'REAL', 'RECN', 'REGI', 'REGN', 'RELL', 'REPH', 'REPL', 'RES', 'RESN', 'RETA', 'REV', 'REVG', 'REX', 'REZI', 'RF', 'RFIL', 'RFL', 'RFP', 'RGA', 'RGEN', 'RGLD', 'RGNX', 'RGR', 'RGS', 'RH', 'RHI', 'RICK', 'RILY', 'RIOT', 'RJF', 'RL', 'RLGY', 'RLH', 'RLI', 'RM', 'RMAX', 'RMBS', 'RMCF', 'RMD', 'RMNI', 'RMR', 'RMTI', 'RNET', 'RNG', 'RNR', 'RNST', 'RNWK', 'ROCK', 'ROG', 'ROK', 'ROKU', 'ROL', 'ROLL', 'ROP', 'ROST', 'RP', 'RPAY', 'RPD', 'RPM', 'RRC', 'RRD', 'RRGB', 'RRR', 'RRTS', 'RS', 'RSG', 'RST', 'RTIX', 'RTN', 'RTRX', 'RUBI', 'RUBY', 'RUN', 'RUSHA', 'RUTH', 'RVLV', 'RVNC', 'RWLK', 'RXN', 'RY', 'RYI', 'RYTM', 'S', 'SA', 'SABR', 'SAFM', 'SAFT', 'SAGE', 'SAH', 'SAIA', 'SAIC', 'SAIL', 'SAM', 'SANM', 'SANW', 'SASR', 'SATS', 'SAVE', 'SBCF', 'SBGI', 'SBH', 'SBNY', 'SBPH', 'SBSI', 'SBT', 'SBUX', 'SC', 'SCCO', 'SCHL', 'SCHN', 'SCHW', 'SCI', 'SCKT', 'SCL', 'SCOR', 'SCPL', 'SCS', 'SCSC', 'SCU', 'SCVL', 'SCWX', 'SCX', 'SDC', 'SDRL', 'SEAS', 'SEB', 'SEDG', 'SEE', 'SEIC', 'SEM', 'SENEA', 'SERV', 'SF', 'SFBS', 'SFE', 'SFIX', 'SFM', 'SFNC', 'SG', 'SGA', 'SGBX', 'SGC', 'SGEN', 'SGH', 'SGMO', 'SGMS', 'SGRY', 'SGU', 'SHAK', 'SHEN', 'SHLD', 'SHLO', 'SHOO', 'SHOP', 'SHW', 'SIBN', 'SIEN', 'SIF', 'SIG', 'SIGA', 'SIGI', 'SILK', 'SINA', 'SINT', 'SIRI', 'SITE', 'SIVB', 'SIX', 'SJI', 'SJM', 'SJR', 'SJW', 'SKX', 'SKY', 'SKYW', 'SLAB', 'SLB', 'SLCA', 'SLCT', 'SLDB', 'SLF', 'SLGG', 'SLGN', 'SLM', 'SLP', 'SLRX', 'SM', 'SMAR', 'SMBC', 'SMED', 'SMG', 'SMMF', 'SMP', 'SMPL', 'SMTC', 'SMTX', 'SNA', 'SNAP', 'SNBR', 'SNCR', 'SND', 'SNDX', 'SNOA', 'SNPS', 'SNSS', 'SNV', 'SNX', 'SO', 'SOI', 'SON', 'SONM', 'SONO', 'SORL', 'SP', 'SPAR', 'SPB', 'SPCE', 'SPFI', 'SPGI', 'SPKE', 'SPLK', 'SPN', 'SPNE', 'SPNS', 'SPOK', 'SPOT', 'SPPI', 'SPR', 'SPSC', 'SPTN', 'SPWR', 'SPXC', 'SQ', 'SR', 'SRAX', 'SRCE', 'SRCL', 'SRDX', 'SRE', 'SRI', 'SRL', 'SRPT', 'SRRK', 'SSB', 'SSD', 'SSI', 'SSNC', 'SSP', 'SSRM', 'SSTI', 'SSTK', 'SSYS', 'ST', 'STAA', 'STBA', 'STC', 'STE', 'STFC', 'STIM', 'STL', 'STLD', 'STMP', 'STNE', 'STOK', 'STRA', 'STRL', 'STRO', 'STRS', 'STRT', 'STSA', 'STT', 'STX', 'STZ', 'SU', 'SUM', 'SUP', 'SUPN', 'SVMK', 'SWAV', 'SWCH', 'SWIR', 'SWK', 'SWKS', 'SWM', 'SWN', 'SWTX', 'SWX', 'SXC', 'SXI', 'SXT', 'SYBT', 'SYBX', 'SYF', 'SYK', 'SYKE', 'SYNA', 'SYNC', 'SYNH', 'SYRS', 'SYX', 'SYY', 'T', 'TA', 'TACO', 'TACT', 'TALO', 'TAP', 'TARA', 'TARO', 'TAST', 'TBBK', 'TBI', 'TBIO', 'TBK', 'TBNK', 'TBPH', 'TCBI', 'TCBK', 'TCDA', 'TCMD', 'TCON', 'TCRR', 'TCS', 'TCX', 'TD', 'TDC', 'TDG', 'TDOC', 'TDS', 'TDW', 'TDY', 'TEAM', 'TECD', 'TECH', 'TECK', 'TEL', 'TELL', 'TEN', 'TENB', 'TER', 'TERP', 'TESS', 'TEX', 'TFC', 'TFSL', 'TFX', 'TG', 'TGE', 'TGEN', 'TGH', 'TGI', 'TGLS', 'TGNA', 'TGP', 'TGT', 'TGTX', 'THC', 'THFF', 'THG', 'THMO', 'THO', 'THR', 'THRM', 'THS', 'TIF', 'TILE', 'TISI', 'TIVO', 'TJX', 'TKKS', 'TKR', 'TLRA', 'TLRD', 'TLRY', 'TLYS', 'TMHC', 'TMO', 'TMP', 'TMST', 'TMUS', 'TNAV', 'TNC', 'TNDM', 'TNET', 'TNP', 'TOL', 'TORC', 'TOWN', 'TPC', 'TPCO', 'TPH', 'TPIC', 'TPR', 'TPRE', 'TPTX', 'TPX', 'TR', 'TRC', 'TREE', 'TREX', 'TRGP', 'TRHC', 'TRI', 'TRIP', 'TRMB', 'TRMK', 'TRN', 'TROW', 'TROX', 'TRP', 'TRQ', 'TRS', 'TRST', 'TRT', 'TRTN', 'TRU', 'TRUE', 'TRUP', 'TRV', 'TRWH', 'TRXC', 'TSC', 'TSCO', 'TSEM', 'TSG', 'TSLA', 'TSN', 'TSQ', 'TSRI', 'TTC', 'TTD', 'TTEC', 'TTEK', 'TTGT', 'TTMI', 'TTOO', 'TTPH', 'TTWO', 'TUFN', 'TUP', 'TUSK', 'TVTY', 'TW', 'TWIN', 'TWLO', 'TWNK', 'TWOU', 'TWST', 'TWTR', 'TXG', 'TXMD', 'TXN', 'TXRH', 'TXT', 'TYL', 'TZOO', 'UAA', 'UAL', 'UBER', 'UBSI', 'UBX', 'UCBI', 'UCTT', 'UEIC', 'UFCS', 'UFI', 'UFPI', 'UFPT', 'UFS', 'UG', 'UGI', 'UHAL', 'UHS', 'UI', 'UIHC', 'UIS', 'ULBI', 'ULH', 'ULTA', 'UMBF', 'UMPQ', 'UNB', 'UNF', 'UNFI', 'UNH', 'UNM', 'UNP', 'UNT', 'UNVR', 'UPLD', 'UPS', 'UPWK', 'URBN', 'URGN', 'URI', 'UROV', 'USAK', 'USAP', 'USAS', 'USB', 'USCR', 'USFD', 'USLM', 'USM', 'USNA', 'USPH', 'USX', 'UTHR', 'UTI', 'UTL', 'UTMD', 'UTSI', 'UTX', 'UVE', 'UVSP', 'UVV', 'V', 'VAC', 'VAL', 'VALU', 'VAPO', 'VAR', 'VBTX', 'VC', 'VCEL', 'VCNX', 'VCRA', 'VCYT', 'VEC', 'VECO', 'VEEV', 'VERI', 'VERU', 'VFC', 'VGR', 'VHI', 'VIAC', 'VIAV', 'VICR', 'VIE', 'VIR', 'VIRT', 'VIVE', 'VIVO', 'VKTX', 'VLGEA', 'VLO', 'VLY', 'VMC', 'VMI', 'VMW', 'VNCE', 'VNDA', 'VNE', 'VOXX', 'VOYA', 'VPG', 'VRA', 'VRAY', 'VRCA', 'VREX', 'VRNS', 'VRNT', 'VRRM', 'VRS', 'VRSK', 'VRSN', 'VRTS', 'VRTU', 'VRTV', 'VRTX', 'VSAT', 'VSEC', 'VSH', 'VSLR', 'VST', 'VSTO', 'VUZI', 'VVI', 'VVUS', 'VVV', 'VYGR', 'VZ', 'W', 'WAB', 'WABC', 'WAFD', 'WAL', 'WASH', 'WAT', 'WATT', 'WBA', 'WBS', 'WBT', 'WCC', 'WCN', 'WD', 'WDAY', 'WDC', 'WDFC', 'WDR', 'WEC', 'WEN', 'WERN', 'WETF', 'WEX', 'WEYS', 'WFC', 'WGO', 'WH', 'WHD', 'WHG', 'WHR', 'WIFI', 'WINA', 'WING', 'WINS', 'WIRE', 'WIX', 'WK', 'WLDN', 'WLFC', 'WLH', 'WLK', 'WLL', 'WLTW', 'WM', 'WMB', 'WMK', 'WMS', 'WMT', 'WNC', 'WNEB', 'WOR', 'WORK', 'WORX', 'WOW', 'WPM', 'WPRT', 'WPX', 'WRB', 'WRK', 'WRLD', 'WRTC', 'WSBC', 'WSBF', 'WSC', 'WSFS', 'WSM', 'WSO', 'WST', 'WTBA', 'WTFC', 'WTI', 'WTM', 'WTR', 'WTRE', 'WTS', 'WU', 'WVE', 'WVFC', 'WVVI', 'WW', 'WWD', 'WWE', 'WWR', 'WWW', 'WYND', 'WYNN', 'X', 'XAIR', 'XBIT', 'XEC', 'XEL', 'XENE', 'XENT', 'XLNX', 'XLRN', 'XNCR', 'XOG', 'XOM', 'XOMA', 'XON', 'XONE', 'XPEL', 'XPER', 'XPO', 'XRAY', 'XRX', 'XYL', 'Y', 'YELP', 'YETI', 'YEXT', 'YMAB', 'YNDX', 'YORW', 'YRCW', 'YUM', 'YUMA', 'YUMC', 'ZAGG', 'ZBH', 'ZBRA', 'ZEN', 'ZFGN', 'ZG', 'ZGNX', 'ZION', 'ZIOP', 'ZIXI', 'ZM', 'ZNGA', 'ZS', 'ZTS', 'ZUMZ', 'ZUO', 'ZVO', 'ZYME', 'ZYNE']\n"
     ]
    }
   ],
   "source": [
    "myfile = open(\"ticker.txt\")\n",
    "data = myfile.readlines()\n",
    "myfile.close()\n",
    "#print(data)\n",
    "ticker_list = [i.rstrip('\\n') for i in data]\n",
    "\n",
    "print(len(ticker_list))\n",
    "print(ticker_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.stocktwits.com/api/2/streams/symbol/BBRY.json\n",
      "./data/BBRY_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/AAPL.json\n",
      "./data/AAPL_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/AMZN.json\n",
      "./data/AMZN_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/BABA.json\n",
      "./data/BABA_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/YHOO.json\n",
      "./data/YHOO_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/LQMT.json\n",
      "./data/LQMT_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/FB.json\n",
      "./data/FB_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/GOOG.json\n",
      "./data/GOOG_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/BBBY.json\n",
      "./data/BBBY_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/JNUG.json\n",
      "./data/JNUG_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/SBUX.json\n",
      "./data/SBUX_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/MU.json\n",
      "./data/MU_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/AHCO.json\n",
      "./data/AHCO_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/EMR.json\n",
      "./data/EMR_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/LL.json\n",
      "./data/LL_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/AXLA.json\n",
      "./data/AXLA_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ZYME.json\n",
      "./data/ZYME_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/BHLB.json\n",
      "./data/BHLB_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/NCLH.json\n",
      "./data/NCLH_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/AEL.json\n",
      "./data/AEL_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/WBA.json\n",
      "./data/WBA_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/LOGM.json\n",
      "./data/LOGM_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/NHC.json\n",
      "./data/NHC_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/LMAT.json\n",
      "./data/LMAT_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/CCF.json\n",
      "./data/CCF_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/CMS.json\n",
      "./data/CMS_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/STMP.json\n",
      "./data/STMP_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/GTN.json\n",
      "./data/GTN_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/EVFM.json\n",
      "./data/EVFM_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/MMC.json\n",
      "./data/MMC_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/HFWA.json\n",
      "./data/HFWA_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/CDLX.json\n",
      "./data/CDLX_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/IRWD.json\n",
      "./data/IRWD_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/KMPR.json\n",
      "./data/KMPR_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ZNGA.json\n",
      "./data/ZNGA_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/MRTX.json\n",
      "./data/MRTX_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/CLCT.json\n",
      "./data/CLCT_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/SHW.json\n",
      "./data/SHW_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/SM.json\n",
      "./data/SM_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/V.json\n",
      "./data/V_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/NBHC.json\n",
      "./data/NBHC_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/BIIB.json\n",
      "./data/BIIB_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/IPG.json\n",
      "./data/IPG_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/PLMR.json\n",
      "./data/PLMR_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/NG.json\n",
      "./data/NG_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/SSP.json\n",
      "./data/SSP_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ESSA.json\n",
      "./data/ESSA_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/CHH.json\n",
      "./data/CHH_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/BIO.json\n",
      "./data/BIO_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/GPRK.json\n",
      "./data/GPRK_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/TLRD.json\n",
      "./data/TLRD_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/HMSY.json\n",
      "./data/HMSY_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/BKI.json\n",
      "./data/BKI_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/SPFI.json\n",
      "./data/SPFI_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/DNR.json\n",
      "./data/DNR_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/LEVI.json\n",
      "./data/LEVI_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/GLNG.json\n",
      "./data/GLNG_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/CDNA.json\n",
      "./data/CDNA_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/HUN.json\n",
      "./data/HUN_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/NATH.json\n",
      "./data/NATH_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/TRN.json\n",
      "./data/TRN_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/PBPB.json\n",
      "./data/PBPB_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ALT.json\n",
      "./data/ALT_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/WIFI.json\n",
      "./data/WIFI_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/IOTS.json\n",
      "./data/IOTS_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/KVHI.json\n",
      "./data/KVHI_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/DLB.json\n",
      "./data/DLB_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/RFL.json\n",
      "./data/RFL_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/IAG.json\n",
      "./data/IAG_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/UNF.json\n",
      "./data/UNF_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/MANT.json\n",
      "./data/MANT_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/SNSS.json\n",
      "./data/SNSS_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/NEO.json\n",
      "./data/NEO_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/PI.json\n",
      "./data/PI_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/INDB.json\n",
      "./data/INDB_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/MAR.json\n",
      "./data/MAR_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/CRVL.json\n",
      "./data/CRVL_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/GT.json\n",
      "./data/GT_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/K.json\n",
      "./data/K_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/MOV.json\n",
      "./data/MOV_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/FFIC.json\n",
      "./data/FFIC_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/HNGR.json\n",
      "./data/HNGR_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/MERC.json\n",
      "./data/MERC_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/SNPS.json\n",
      "./data/SNPS_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/JRVR.json\n",
      "./data/JRVR_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/HTLF.json\n",
      "./data/HTLF_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/PRAH.json\n",
      "./data/PRAH_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/AG.json\n",
      "./data/AG_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/PZZA.json\n",
      "./data/PZZA_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/CSS.json\n",
      "./data/CSS_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/TRS.json\n",
      "./data/TRS_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/OFIX.json\n",
      "./data/OFIX_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/AIT.json\n",
      "./data/AIT_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ARNC.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/ARNC_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/GORO.json\n",
      "./data/GORO_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/SLGN.json\n",
      "./data/SLGN_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/AMRS.json\n",
      "./data/AMRS_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/MDB.json\n",
      "./data/MDB_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/PGNY.json\n",
      "./data/PGNY_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ACOR.json\n",
      "./data/ACOR_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ANIX.json\n",
      "./data/ANIX_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/FOXF.json\n",
      "./data/FOXF_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/FIT.json\n",
      "./data/FIT_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/FE.json\n",
      "./data/FE_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/IMAX.json\n",
      "./data/IMAX_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/CGNX.json\n",
      "./data/CGNX_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/BBIO.json\n",
      "./data/BBIO_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/WPRT.json\n",
      "./data/WPRT_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ASPU.json\n",
      "./data/ASPU_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/QRVO.json\n",
      "./data/QRVO_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/IBTX.json\n",
      "./data/IBTX_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/EZPW.json\n",
      "./data/EZPW_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/MIC.json\n",
      "./data/MIC_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/GLUU.json\n",
      "./data/GLUU_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ALKS.json\n",
      "./data/ALKS_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/HRC.json\n",
      "./data/HRC_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/HSC.json\n",
      "./data/HSC_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/SLCA.json\n",
      "./data/SLCA_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/RAMP.json\n",
      "./data/RAMP_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/CNI.json\n",
      "./data/CNI_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/VLY.json\n",
      "./data/VLY_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/CLW.json\n",
      "./data/CLW_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/PRGS.json\n",
      "./data/PRGS_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/OLN.json\n",
      "./data/OLN_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/UTHR.json\n",
      "./data/UTHR_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ADP.json\n",
      "./data/ADP_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/IPGP.json\n",
      "./data/IPGP_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/DTE.json\n",
      "./data/DTE_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/MSCI.json\n",
      "./data/MSCI_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/CWH.json\n",
      "./data/CWH_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/JNCE.json\n",
      "./data/JNCE_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/AMG.json\n",
      "./data/AMG_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/DOV.json\n",
      "./data/DOV_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/WSC.json\n",
      "./data/WSC_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/YORW.json\n",
      "./data/YORW_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/CARG.json\n",
      "./data/CARG_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/EVRI.json\n",
      "./data/EVRI_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/CEIX.json\n",
      "./data/CEIX_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/WING.json\n",
      "./data/WING_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/NWFL.json\n",
      "./data/NWFL_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/WTI.json\n",
      "./data/WTI_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ICUI.json\n",
      "./data/ICUI_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/CAI.json\n",
      "./data/CAI_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ALRN.json\n",
      "./data/ALRN_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/PING.json\n",
      "./data/PING_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/SOI.json\n",
      "./data/SOI_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/RCM.json\n",
      "./data/RCM_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/EIGI.json\n",
      "./data/EIGI_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/MXIM.json\n",
      "./data/MXIM_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/OKTA.json\n",
      "./data/OKTA_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/VZ.json\n",
      "./data/VZ_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/DOMO.json\n",
      "./data/DOMO_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/SEAS.json\n",
      "./data/SEAS_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/OAS.json\n",
      "./data/OAS_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ARMK.json\n",
      "./data/ARMK_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/HRI.json\n",
      "./data/HRI_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/INSP.json\n",
      "./data/INSP_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/NWN.json\n",
      "./data/NWN_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/NVDA.json\n",
      "./data/NVDA_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/IMGN.json\n",
      "./data/IMGN_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/AGFS.json\n",
      "./data/AGFS_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/VIAC.json\n",
      "./data/VIAC_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ICHR.json\n",
      "./data/ICHR_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/VRSN.json\n",
      "./data/VRSN_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/MMSI.json\n",
      "./data/MMSI_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/AXS.json\n",
      "./data/AXS_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/PRSC.json\n",
      "./data/PRSC_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/SWKS.json\n",
      "./data/SWKS_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/BIMI.json\n",
      "./data/BIMI_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/AE.json\n",
      "./data/AE_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/MORF.json\n",
      "./data/MORF_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/VMC.json\n",
      "./data/VMC_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/GPX.json\n",
      "./data/GPX_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/PFGC.json\n",
      "./data/PFGC_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/AMKR.json\n",
      "./data/AMKR_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/SSTI.json\n",
      "./data/SSTI_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/QUOT.json\n",
      "./data/QUOT_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/HFC.json\n",
      "./data/HFC_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/PRO.json\n",
      "./data/PRO_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/CALM.json\n",
      "./data/CALM_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ANIP.json\n",
      "./data/ANIP_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/SNA.json\n",
      "./data/SNA_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/EGRX.json\n",
      "./data/EGRX_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/CHD.json\n",
      "./data/CHD_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/OC.json\n",
      "./data/OC_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/PANW.json\n",
      "./data/PANW_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/OBNK.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/OBNK_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/NTAP.json\n",
      "./data/NTAP_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/SBSI.json\n",
      "./data/SBSI_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/VRS.json\n",
      "./data/VRS_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/MUX.json\n",
      "./data/MUX_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/TCX.json\n",
      "./data/TCX_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/MGPI.json\n",
      "./data/MGPI_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/SVMK.json\n",
      "./data/SVMK_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/PRPL.json\n",
      "./data/PRPL_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/CHE.json\n",
      "./data/CHE_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/MDLZ.json\n",
      "./data/MDLZ_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/NVST.json\n",
      "./data/NVST_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ABCB.json\n",
      "./data/ABCB_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/CACC.json\n",
      "./data/CACC_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/OTTR.json\n",
      "./data/OTTR_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/PRK.json\n",
      "./data/PRK_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/MTBC.json\n",
      "./data/MTBC_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/VRA.json\n",
      "./data/VRA_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/TD.json\n",
      "./data/TD_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/OSTK.json\n",
      "./data/OSTK_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/NATI.json\n",
      "./data/NATI_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/YETI.json\n",
      "./data/YETI_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/CULP.json\n",
      "./data/CULP_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/VIVO.json\n",
      "./data/VIVO_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/GDI.json\n",
      "./data/GDI_20200129_0214.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/MGM.json\n",
      "./data/MGM_20200129_0214.json\n"
     ]
    }
   ],
   "source": [
    "symbols = ['BBRY', 'AAPL', 'AMZN', 'BABA', 'YHOO', 'LQMT', 'FB', 'GOOG', 'BBBY', 'JNUG', 'SBUX', 'MU']\n",
    "\n",
    "NUM_REQUEST = 200\n",
    "#symbols.extend(ticker_list[0:50])\n",
    "symbols.extend(random.sample(ticker_list, NUM_REQUEST))\n",
    "\n",
    "args = ['curl', '-X', 'GET', '']\n",
    "URL = \"https://api.stocktwits.com/api/2/streams/symbol/\"\n",
    "\n",
    "FILE_PATH = \"./data/\"\n",
    "\n",
    "start_datetime = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "for symbol in symbols:\n",
    "    try:\n",
    "        args[3] = URL + symbol + \".json\"\n",
    "        print(args[3])\n",
    "        proc = subprocess.run(args,stdout = subprocess.PIPE, stderr = subprocess.PIPE)\n",
    "\n",
    "        path = FILE_PATH + symbol + \"_\" + start_datetime + \".json\"\n",
    "        print(path)\n",
    "        with open(path, mode='w') as f:\n",
    "            f.write(proc.stdout.decode(\"utf8\"))\n",
    "    except:\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "!grep -rl error data | xargs rm\n",
    "!grep -rlv '{\"response\":{\"status\":200}' | xargs rm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、保存したファイルを、分散処理環境（クラスター）を使って加工するためにHDFSへコピーします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hdfs', 'dfs', '-put', './data/UTHR_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/ICUI_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/HRI_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/MAR_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/NVST_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/GTN_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/V_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/ANIP_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/HFC_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/MMSI_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/SWKS_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/NWFL_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/OC_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/HTLF_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/EMR_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/BBIO_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/BIO_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/MUX_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/HSC_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/QUOT_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/GORO_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/HRC_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/LMAT_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/NBHC_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/UNF_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/IOTS_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/DOV_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/HMSY_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/TRS_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/MRTX_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/ZNGA_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/IPG_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/ADP_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/SM_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/GLUU_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/ESSA_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/CDLX_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/LL_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/WSC_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/CALM_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/ARMK_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/CLCT_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/AAPL_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/CWH_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/INSP_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/PANW_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/SBUX_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/SPFI_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/DLB_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/STMP_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/CHD_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/MGPI_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/SBSI_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/IMGN_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/WIFI_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/AMG_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/NTAP_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/CAI_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/JNUG_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/HUN_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/AG_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/WING_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/YORW_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/INDB_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/AIT_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/GPRK_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/MMC_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/BIIB_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/SOI_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/OLN_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/BKI_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/SNA_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/CHE_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/EGRX_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/QRVO_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/AMKR_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/MSCI_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/PGNY_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/CCF_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/DNR_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/KVHI_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/FFIC_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/IAG_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/SNSS_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/MDB_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/FIT_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/AXS_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/ALRN_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/MU_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/FE_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/FOXF_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/HNGR_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/ASPU_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/ABCB_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/ARNC_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/PING_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/VZ_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/PRO_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/VRS_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/LEVI_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/NEO_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/OKTA_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/DOMO_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/IMAX_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/CNI_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/AMRS_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/BBRY_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/EVRI_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/SLCA_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/CLW_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/MXIM_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/EZPW_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/ICHR_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/BABA_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/AGFS_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/MOV_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/PZZA_20200129_0214.json', './twits/']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['hdfs', 'dfs', '-put', './data/OBNK_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/AMZN_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/CEIX_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/NWN_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/MDLZ_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/SNPS_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/RAMP_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/SSP_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/CDNA_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/NHC_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/PFGC_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/RCM_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/CGNX_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/CACC_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/DTE_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/PLMR_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/YHOO_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/FB_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/GLNG_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/GPX_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/IBTX_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/SSTI_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/SEAS_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/PRPL_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/CARG_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/MORF_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/NCLH_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/PBPB_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/BBBY_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/SHW_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/VRSN_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/TCX_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/BHLB_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/GT_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/HFWA_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/NVDA_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/ZYME_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/WBA_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/EIGI_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/RFL_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/MIC_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/OAS_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/ALT_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/VIAC_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/AXLA_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/AEL_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/ANIX_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/CHH_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/MANT_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/K_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/TRN_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/VLY_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/CSS_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/WPRT_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/IPGP_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/VMC_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/MERC_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/JNCE_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/PRAH_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/SLGN_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/PRGS_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/OFIX_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/IRWD_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/PI_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/EVFM_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/AE_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/CRVL_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/GOOG_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/ACOR_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/NATH_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/PRSC_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/BIMI_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/SVMK_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/KMPR_20200129_0214.json', './twits/']\n",
      "\n",
      "['hdfs', 'dfs', '-put', './data/ALKS_20200129_0214.json', './twits/']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "os.environ['HADOOP_CONF_DIR'] = \"/etc/spark/conf/yarn-conf\"\n",
    "\n",
    "HDFS_PATH_DIR = './twits/'\n",
    "\n",
    "args = ['hdfs', 'dfs', '-put', '', HDFS_PATH_DIR]\n",
    "\n",
    "\n",
    "try:\n",
    "    args_mkdir = ['hdfs', 'dfs', '-mkdir', HDFS_PATH_DIR]\n",
    "    proc = subprocess.run(args_mkdir,stdout = subprocess.PIPE, stderr = subprocess.PIPE)\n",
    "except:\n",
    "    traceback.print_exc()\n",
    "\n",
    "file_list = glob.glob(\"./data/*\")\n",
    "\n",
    "\n",
    "for file in file_list:\n",
    "    try:\n",
    "        args[3] = file\n",
    "        print(file)\n",
    "\n",
    "        proc = subprocess.run(args,stdout = subprocess.PIPE, stderr = subprocess.PIPE)\n",
    "\n",
    "    except:\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. データ変換\n",
    "\n",
    "クラスターでデータを変換します。CDSW上では、ユーザーごとに別のプロジェクトを使っていましたが。\n",
    "クラスター環境では、自分が利用しているユーザーとデータを意識して取り扱う必要があります。\n",
    "\n",
    "\n",
    "あなたのユーザ名は以下で確認できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Engine(hive://user2@master.ykono.work:10000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlalchemy.create_engine('hive://user2@master.ykono.work:10000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#$ beeline -u 'jdbc:hive2://10.0.0.55:10000' -f tables.hql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hdfs', 'dfs', '-put', './lib/json-1.3.7.3.jar', './']\n",
      "stdout: \n",
      "['hdfs', 'dfs', '-put', './lib/README.md', './']\n",
      "stdout: \n",
      "['hdfs', 'dfs', '-put', './lib/brickhouse-0.7.1-SNAPSHOT.jar', './']\n",
      "stdout: \n",
      "['hdfs', 'dfs', '-put', './lib/json-serde-cdh5-shim-1.3.7.3.jar', './']\n",
      "stdout: \n",
      "['hdfs', 'dfs', '-put', './lib/json-serde-1.3.7.3.jar', './']\n",
      "stdout: \n"
     ]
    }
   ],
   "source": [
    "HDFS_PATH_DIR = '/tmp/'\n",
    "HDFS_PATH_DIR = './'\n",
    "\n",
    "args = ['hdfs', 'dfs', '-put', '', HDFS_PATH_DIR]\n",
    "\n",
    "file_list = glob.glob(\"./lib/*\")\n",
    "\n",
    "for file in file_list:\n",
    "    try:\n",
    "        args[3] = file\n",
    "        print(args)\n",
    "\n",
    "        proc = subprocess.run(args,stdout = subprocess.PIPE, stderr = subprocess.PIPE)\n",
    "  \n",
    "    except:\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sql extension is already loaded. To reload it, use:\n",
      "  %reload_ext sql\n"
     ]
    }
   ],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**下記のセルの中を適切なユーザ名とURL（Hiveサーバー）に置換してください。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Connected: user2@None'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql hive://user2@ip-10-0-0-55.ap-northeast-1.compute.internal:10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**あなたのユーザ名でデータベースを作成・利用してください**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * hive://user2@ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      "Done.\n",
      " * hive://user2@ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      "Done.\n",
      " * hive://user2@ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>tab_name</th>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql CREATE DATABASE user2\n",
    "%sql USE user2\n",
    "%sql SHOW TABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * hive://user2@ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      "Done.\n",
      " * hive://user2@ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      "Done.\n",
      " * hive://user2@ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql add jar hdfs:/tmp/json-1.3.7.3.jar\n",
    "%sql add jar hdfs:/tmp/json-serde-1.3.7.3.jar\n",
    "%sql add jar hdfs:/tmp/json-serde-cdh5-shim-1.3.7.3.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * hive://user2@ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      "Done.\n",
      " * hive://user2@ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      "Done.\n",
      " * hive://user2@ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      "Done.\n",
      " * hive://user2@ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      "Done.\n",
      " * hive://user2@ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql DROP TABLE IF EXISTS twits\n",
    "%sql DROP TABLE IF EXISTS message_extracted\n",
    "%sql DROP TABLE IF EXISTS message_filtered\n",
    "%sql DROP TABLE IF EXISTS message_exploded\n",
    "%sql DROP TABLE IF EXISTS sentiment_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`LOCATION`にあなたがファイルをアップロードしたパスを指定してください**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * hive://user2@ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "CREATE EXTERNAL TABLE twits (\n",
    "\tmessages \n",
    "\tARRAY<\n",
    "\t    STRUCT<body: STRING,\n",
    "\t        symbols:ARRAY<STRUCT<symbol:STRING>>,\n",
    "\t        entities:STRUCT<sentiment:STRUCT<basic:STRING>>\n",
    "\t    >\n",
    "\t>\n",
    ")\n",
    "ROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe' \n",
    "STORED AS TEXTFILE\n",
    "LOCATION '/user/user2/twits'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * hive://user2@ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>messages</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>[{&quot;body&quot;:&quot;$SPY $AMZN $MSFT $AAPL $TSLA currently using RH but wanting to switch to either think or swim or Webull. Anyone have preference?&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;AMZN&quot;},{&quot;symbol&quot;:&quot;MSFT&quot;},{&quot;symbol&quot;:&quot;SPY&quot;},{&quot;symbol&quot;:&quot;TSLA&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$AAPL \\n\\nApple Reports 1Q 2020 Results: $22.2B Profit on $91.8B Revenue, Best Quarter Ever.\\n\\nHow is it even fathomable that some of you are still rationalizing.\\nSome are saying 300 tomorrow.\\nEither you’re a bear, placed puts, want to annoy some of the winners, or plain stupid!\\nAt least 330 is a given.\\nProbably between 335 - 360 if had to bet.\\n400 is definitely a possibility.\\nNo one knows for sure!&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$AAPL Congrats, longs. It&amp;#39;s been hard not booking some gains on looooong held shares. I felt a beat coming on, and strong guidance, but you never really know. Are we back to sandbagging guidance? Def. back to growth.&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$AAPL ambulance for the bears. \\nTik tok:westtt80&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$AAPL goes up 4 dollars and bulls Cole out of the cave&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bearish&quot;}}},{&quot;body&quot;:&quot;$AAPL Lackluster services # wasn&amp;#39;t that supposed to be a primary growth driver.&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bearish&quot;}}},{&quot;body&quot;:&quot;$SPY $AAPL $AMZN $TSLA still early but why not have some fun 🤷‍♂️&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;AMZN&quot;},{&quot;symbol&quot;:&quot;SPY&quot;},{&quot;symbol&quot;:&quot;TSLA&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$AAPL futures going very green right now with the info that the epidemic is possibly slowing down. Less new cases today than yesterday. Gonna help push Apple higher.&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$SPY when I think of bears I always see them still using them flip phones 😂😂🤦‍♂️ $AAPL&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;SPY&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$AAPL can’t wait to more\\nOpen flush&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bearish&quot;}}},{&quot;body&quot;:&quot;$BABA 9988-HK, BABA HK chart, perfect double zig-zag correction, a breach of 215.2 in HK will confirm the long side, we may have to wait till earnings for that. $AAPL just confirmed new ATH on earnings which tells me the lows are in and next leg up started. Happy trading.&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;BABA&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$AAPL 350 tomorrow!&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$SPCE i think a lot of money is going to go to aapl and tsla tomorrow, i think my baby space dips tomorrow and possibly until friday, i would ride the $AAPL and $TSLA wave and get back into spce on friday around noon... imho&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;TSLA&quot;},{&quot;symbol&quot;:&quot;SPCE&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$AAPL \\nTHIS ARTICLE WILL MAKE YOU MONEY!!\\nhttps://stocktalks.substack.com/p/earning-report-plays-jan-27-31&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;Apple announces earnings. $4.99 EPS. Beats estimates. $91.80b revenue.  https://www.marketbeat.com/s/440450 $AAPL&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;Apple announces earnings. $4.99 EPS. Beats estimates. $91.80b revenue.  https://www.marketbeat.com/s/440450 $AAPL&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$AAPL what happened to all the bears? They get the corona virus?&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$AAPL Blow off top inbound&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bearish&quot;}}},{&quot;body&quot;:&quot;$AAPL The dummy parade of gloating bears looks silly, in hindsight.&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$FB $MSFT futures showing green. 👀 $AAPL ER blew it out of the water.  Might be a big money day for bulls. More ERs to come&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;MSFT&quot;},{&quot;symbol&quot;:&quot;FB&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$AAPL So we opening at 320 or 330??&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$AAPL i live in a retirement comunity in boca raton.  I had s friend that held 400 shares for a few years.  He rode it to 700 and it split 7 form1  Then he had 2800 shares at 100.  Now he has 2800 shares at 300 and it is closer to going to  to 700 again than going back to 100.  I believe in this.  I see it at 700 in a year or so.  These earnings will just keep growing.  Then it will split again.  And it pays dividends and there a stock buy backs.&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$AAPL  \\nThe company made more money than the combined Money of ALL OF EASTERN CANADA in 1 q \\n \\nand these bear morons are drawing lines and think AH “sell off” is going to bring us to 300 \\n \\nYOU are going to get burned like Tim Cook taking it up the ass as he celebrates the amazing numbers !! \\n \\ndeal with it STOP POSTING  \\nyou are looking moronic !&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$SPY $AAPL Friends.&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;SPY&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$AAPL CREAM! GET THE DIVIDEND, DOLLAR DOLLAR BILL YALL \\nWU TANG AINT NOTHING TO INVEST WITH.&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$AAPL NEW ARTICLE : Apple&amp;#39;s fastest-growing business segment, which includes AirPods and Watch, is now bigger than the Mac https://dashboard.stck.pro/news.php?ticker=AAPL&amp;amp;rowid=3423747 Get all the latest $AAPL related news here : https://dashboard.stck.pro/news.php?ticker=AAPL&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$SPY  bears will be chocking on them 🍏s tmrw $AAPL&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;SPY&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$TSLA $AAPL $UBER&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;TSLA&quot;},{&quot;symbol&quot;:&quot;UBER&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$AAPL right now estimates say earnings grow 27% end of 2021... and PE is around 27X...  seems like PE multiple expansion based on more confidence on future growth with a two year window $SPY $QQQ $DIA&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;DIA&quot;},{&quot;symbol&quot;:&quot;SPY&quot;},{&quot;symbol&quot;:&quot;QQQ&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$AAPL Apple earnings and sales surge to record, sending stock toward new highs\\n\\nhttps://www.marketwatch.com/story/apple-stock-gains-after-record-earnings-upbeat-forecast-2020-01-28?mod=newsviewer_click&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>[{&quot;body&quot;:&quot;Piper Sandler Lowers Ameris Bancorp FY2020 Earnings Estimates to $4.10 EPS (Previously $4.18). https://www.marketbeat.com/x/776220 $ABCB&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABCB&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;SunTrust Banks Lowers Ameris Bancorp Q1 2020 Earnings Estimates to $0.99 EPS (Previously $1.03). https://www.marketbeat.com/x/776221 $ABCB&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABCB&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ABCB Ameris Bancorp Expected to Earn Q1 2020 Earnings of $0.99 Per Share \\n\\nhttps://newsfilter.io/a/77ad8ac1c1b5a3f81991e1453d4fa887&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABCB&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;Be aware! A lot of indicators predict a fall for Ameris Bancorp ($ABCB) during next few days. Full story HERE https://stockinvest.us/l/zCSsJmOe9v&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABCB&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bearish&quot;}}},{&quot;body&quot;:&quot;$ABCB Ameris Bancorp Issues Earnings Results \\n\\nhttps://newsfilter.io/a/d9bcba462224c2e882a89dbcfe37d9d9&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABCB&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;Ameris Bancorp (ABCB) announces earnings. $0.96 EPS. Meets estimates. 66.81M earnings. $ABCB https://www.tipranks.com/stocks/ABCB/earnings-calendar?ref=TREarnings&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABCB&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;Ameris Bancorp just filed its Current report, items 2.02, 7.01, and 9. http://www.conferencecalltranscripts.org/include?location=http://www.sec.gov/Archives/edgar/data/351569/000119312520013278/0001193125-20-013278-index.htm $ABCB&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABCB&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ABCB / Ameris Bancorp files form 8-K - Regulation FD Disclosure, Financial Statements and Exhibits, Results of Operations and Financial Condition https://fintel.io/s/us/abcb?utm_source=stocktwits.com&amp;amp;utm_medium=Social&amp;amp;utm_campaign=filing&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABCB&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ABCB just filed a Earnings Release, a Regulated Disclosure and a Financial Exhibit https://last10k.com/sec-filings/abcb/0001193125-20-013278.htm?utm_source=stocktwits&amp;amp;utm_medium=forum&amp;amp;utm_campaign=8K&amp;amp;utm_term=abcb&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABCB&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;Ameris Bancorp ($ABCB) 4Q19 Investor Call On 24th January 2020 At 9:30 AM Eastern Time\\n\\nhttps://www.stockmarketintellects.com/ameris-bancorp-nasdaqabcb-4q19-investor-call-on-24th-january-2020-at-930-am-eastern-time/&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABCB&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ABCB Earnings Call Tomorrow: 9:30 AM EST \\nAnalyst Rating: Strong Buy \\nPhone: 412-317-0088 \\nPIN: 10138227 \\nWebcast: http://mmm.wallstreethorizon.com/u.asp?u=121865&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABCB&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ABCB beats on revenue too as per earningswhisper&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABCB&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$ABCB: Issued Press Release on January 23, 17:13:00: Ameris Bancorp Announces Fourth Quarter And Full Year 2019 Financial Results https://s.flashalert.me/qB9L8y&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABCB&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ABCB reported earnings of $0.96, consensus was $0.96, Earnings Whisper was $0.95 via @eWhispers #whisperbeat http://eps.sh/d/abcb&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABCB&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ABCB NEW ARTICLE : Ameris Bancorp EPS in-line, misses on revenue https://dashboard.stck.pro/news.php?ticker=ABCB&amp;amp;rowid=3365091 Get all the latest $ABCB related news here : https://dashboard.stck.pro/news.php?ticker=ABCB&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABCB&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ABCB Ameris Bancorp EPS in-line, misses on revenue \\n\\nhttps://newsfilter.io/a/adc7d73ce6a8e088e301d23681b67e3a&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABCB&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ABCB Ameris Bancorp Announces Fourth Quarter And Full Year 2019 Financial Results \\n\\nhttps://newsfilter.io/a/4baf4f255b0bbc07599b072b70bd0e79&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABCB&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ABCB is scheduled to report #earnings after the market closes today via @eWhispers http://eps.sh/s/abcb&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABCB&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ABCB NEW ARTICLE : Notable earnings after Thursday&amp;#39;s close https://dashboard.stck.pro/news.php?ticker=ABCB&amp;amp;rowid=3355428&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABCB&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;Ameris Bancorp to release earnings after the market closes on Thursday. Analysts expect 0.96 EPS.  $ABCB https://www.marketbeat.com/p/44115&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABCB&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ABCB Peregrine Capital Management Decreases Holdings in Ameris Bancorp \\n\\nhttps://newsfilter.io/a/adfb732351c913358e2f2aaecabdf7e7&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABCB&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;Wow this is a big change! $ABCB MACD Histogram just turned positive. View odds of uptrend. https://tickeron.com/go/1133393&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABCB&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;Piper Sandler Sets Ameris Bancorp FY2020 Earnings Estimates at $4.18 EPS. https://www.marketbeat.com/x/770045 $ABCB&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABCB&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;Summit Creek Advisors LLC,has filed Form 13F for Q4 2019.Opened NEW positions in $ABCB $GO $TREX $WEX&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;WEX&quot;},{&quot;symbol&quot;:&quot;ABCB&quot;},{&quot;symbol&quot;:&quot;TREX&quot;},{&quot;symbol&quot;:&quot;GO&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;Ameris Bancorp (ABCB) to release earnings after the market closes on Thursday, January 23. Expected EPS: 0.96. $ABCB https://www.tipranks.com/stocks/ABCB/earnings-calendar?ref=TREarnings&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABCB&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ABCB: Issued Press Release on January 10, 13:30:00: Ameris Bancorp Announces Date Of Fourth Quarter 2019 Earnings Release And Conference  https://s.flashalert.me/mvHR2&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABCB&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ABCB  Ameris Bancorp Announces Date Of Fourth Quarter 2019 Earnings Release And Conference Call: Ameris Bancorp announced that it intends to release its fourth quarter and full year 2019 financial res.. https://newsfilter.io/a/9541b6c3c7855bb46ded4ac930a1b94e&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABCB&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ABCB 8 sec ago: Ameris Bancorp Lowered to Strong Sell at BidaskClub: BidaskClub downgraded shares of Ameris Bancorp from a sell rating to a strong sell rating in a research note issued to investors on Saturday .. https://newsfilter.io/articles/ameris-bancorp-nasdaqabcb-lowered-to-strong-sell-at-bidaskclub-a3661e0916a89b867d46b8c8b7886d4e&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABCB&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ABCB - Should I keep both Ameris Bancorp and Eagle Bancorp… http://dlvr.it/RMYD3F #portfolio_prospective #better_portfolio #diversify&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABCB&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ABCB - Earnings call in two weeks. 81% chance to finish above $42 in February http://dlvr.it/RMQrr7&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABCB&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>[{&quot;body&quot;:&quot;Acorda Therapeutics $ACOR Trading Report http://tinyurl.com/s7e9gw4&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ACOR&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ADAP \\nHere&amp;#39;s why I bought in today:\\n1. Bull flag holding a strong level of support on the daily AND weekly\\n2. Overall trend is upward on the daily and weekly\\n3. RSI is curling upwards on the daily and weekly\\n4. Expecting the 20/50 sma cross on the daily and weekly\\n5. MACD curling on the weekly\\n6. Bullish engulfing on the daily\\nWe can see on the daily it somewhat broke out of the bull flag with but pulled back to that resistance. On the weekly, it used to be in a downward trend but had a nice gap up and pulled back to support where we can see it was touched twice before. Right now the candlestick is a doji which imo is a good sign because the week before it was a red candle and this is telling us the buyers are trying to stop the bears. My SL for this is 3.5 and my PT are 4.5, 5.0, 5.5 and 6.38(200 sma on the weekly). I also found out there was some insider buys today AH which means we might see a move up soon just like $ACOR. I&amp;#39;ll post the weekly chart below&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ACOR&quot;},{&quot;symbol&quot;:&quot;ADAP&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$ACOR&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ACOR&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ACOR Don’t forget the Bill and Melinda Gates Foundation has had an association with the ARCUS technology&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ACOR&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ACOR lots of churning in the T/R .... but ... needs to see weekly chart break above 2.75 to get me in ...&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ACOR&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ACOR very low volume&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ACOR&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ACOR adding here 2.09&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ACOR&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;When I compare the Beta value, $ACOR does not outperform its peers. Check for yourself https://wallmine.com/nasdaq/acor/peers/beta?utm_source=stocktwits&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ACOR&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ACOR and with that dip, I’m in!&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ACOR&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;Wedbush Sets Acorda Therapeutics FY2024 Earnings Estimates at ($0.99) EPS. https://www.marketbeat.com/x/776227 $ACOR&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ACOR&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ACOR Watching for entry below 2.15 for a bounce. Target 2.50.&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ACOR&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$ACOR I expected this below $2 today and what a surprise.... sombody clearly accumulating.....&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ACOR&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ACOR Crazy 2 trading days in a row I could have sold on top end and rebought low to accumulate BUT I don’t want to sell and suddenly it keeps going up and out of range.  Thursday high of $2.5, Friday $2.44 and today $2.37.  Oh well I can’t predict the future&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ACOR&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ACOR bullish tomorrow gap up&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ACOR&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$ACOR&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ACOR&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bearish&quot;}}},{&quot;body&quot;:&quot;$ACOR that was a technical bounce off the technical bounce which just took place @ 2.55. This one simply under the active trigger 2.40-2.50 we hit 2.54 1st last week then 2.55. Meaning 1 deviation and now we hit 2.37 3 of the 2.40 bottom trigger generating polarity to attempt to balance out and eat the spread that took place from the 2.54/2.55 manoeuvre&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ACOR&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$acor pullback near VWAP&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ACOR&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$acor holy grail - volume build&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ACOR&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ACOR Stock 2 shares at $2.29 does this mean halt coming on pending news?&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ACOR&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ACOR has one of two drugs for Parkinson’s  off periods . Inbrija’s sales will continue to grow.I expect $3 by early summer.&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ACOR&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ACOR Is Biogen still a partner with Acorda?&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ACOR&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ACOR Bought 50% more at 2.20. High risk reward setting&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ACOR&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$ACOR Was dabbling with Tradestation&amp;#39;s Option set up and accidentally picked up $3 Calls.  Looks like I&amp;#39;m doing a few options too now...&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ACOR&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ACOR Undervalued and ignored by the market ... but cooking something strong, we will soon see a rise to more than $3&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ACOR&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$ACOR I think it’s funny the other day this goes up and everyone is saying this is on scanners but today no one has said anything like that.&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ACOR&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ACOR Looks like go time is getting close.&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ACOR&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ACOR - Will Acorda Therapeutics price increase in February 2020? Upcoming quarterly earning… https://www.macroaxis.com/invest/market/ACOR--valuation--Acorda-Therapeutics #stocks #earnings&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ACOR&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ACOR IMO only reason shares are being sold is because someone shorted them and trying to give appearance of something that is not.&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ACOR&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$ACOR  $2.6 high of the day?&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ACOR&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ACOR  With securities sold at $2.42/share this is a bargain right?&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ACOR&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}}]</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[('[{\"body\":\"$SPY $AMZN $MSFT $AAPL $TSLA currently using RH but wanting to switch to either think or swim or Webull. Anyone have preference?\",\"symbols\" ... (6666 characters truncated) ... pple-stock-gains-after-record-earnings-upbeat-forecast-2020-01-28?mod=newsviewer_click\",\"symbols\":[{\"symbol\":\"AAPL\"}],\"entities\":{\"sentiment\":null}}]',),\n",
       " ('[{\"body\":\"Piper Sandler Lowers Ameris Bancorp FY2020 Earnings Estimates to $4.10 EPS (Previously $4.18). https://www.marketbeat.com/x/776220 $ABCB\",\" ... (6729 characters truncated) ... gs call in two weeks. 81% chance to finish above $42 in February http://dlvr.it/RMQrr7\",\"symbols\":[{\"symbol\":\"ABCB\"}],\"entities\":{\"sentiment\":null}}]',),\n",
       " ('[{\"body\":\"Acorda Therapeutics $ACOR Trading Report http://tinyurl.com/s7e9gw4\",\"symbols\":[{\"symbol\":\"ACOR\"}],\"entities\":{\"sentiment\":null}},{\"body\":\" ... (5651 characters truncated) ... t\":null}},{\"body\":\"$ACOR  With securities sold at $2.42/share this is a bargain right?\",\"symbols\":[{\"symbol\":\"ACOR\"}],\"entities\":{\"sentiment\":null}}]',)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "select * from twits limit 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * hive://user2@ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      "Done.\n",
      " * hive://user2@ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      "Done.\n",
      " * hive://user2@ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      "Done.\n",
      " * hive://user2@ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql create table message_extracted (symbols array<struct<symbol:string>>, sentiment STRING, body STRING) STORED AS TEXTFILE\n",
    "%sql create table message_filtered (symbols array<struct<symbol:string>>, sentiment STRING, body STRING) STORED AS TEXTFILE\n",
    "%sql create table message_exploded (symbol string, sentiment STRING, body STRING) STORED AS TEXTFILE\n",
    "%sql create table sentiment_data (sentiment int, body STRING) STORED AS TEXTFILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * hive://user2@ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "insert overwrite table message_extracted \n",
    "select message.symbols, message.entities.sentiment, message.body from twits \n",
    "lateral view explode(messages) messages as message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * hive://user2@ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>symbols</th>\n",
       "        <th>sentiment</th>\n",
       "        <th>body</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;AMZN&quot;},{&quot;symbol&quot;:&quot;MSFT&quot;},{&quot;symbol&quot;:&quot;SPY&quot;},{&quot;symbol&quot;:&quot;TSLA&quot;}]</td>\n",
       "        <td>None</td>\n",
       "        <td>$SPY $AMZN $MSFT $AAPL $TSLA currently using RH but wanting to switch to either think or swim or Webull. Anyone have preference?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>[{&quot;symbol&quot;:&quot;AAPL&quot;}]</td>\n",
       "        <td>Bullish</td>\n",
       "        <td>$AAPL </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>[]</td>\n",
       "        <td>None</td>\n",
       "        <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>[{&quot;symbol&quot;:&quot;Apple Reports 1Q 2020 Results: $22.2B Profit on $91.8B Revenue, Best Quarter Ever.&quot;}]</td>\n",
       "        <td>None</td>\n",
       "        <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>[]</td>\n",
       "        <td>None</td>\n",
       "        <td>None</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[('[{\"symbol\":\"AAPL\"},{\"symbol\":\"AMZN\"},{\"symbol\":\"MSFT\"},{\"symbol\":\"SPY\"},{\"symbol\":\"TSLA\"}]', None, '$SPY $AMZN $MSFT $AAPL $TSLA currently using RH but wanting to switch to either think or swim or Webull. Anyone have preference?'),\n",
       " ('[{\"symbol\":\"AAPL\"}]', 'Bullish', '$AAPL '),\n",
       " ('[]', None, None),\n",
       " ('[{\"symbol\":\"Apple Reports 1Q 2020 Results: $22.2B Profit on $91.8B Revenue, Best Quarter Ever.\"}]', None, None),\n",
       " ('[]', None, None)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "select * from message_extracted limit 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * hive://user2@ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "insert overwrite table message_filtered \n",
    "select symbols, \n",
    "    case sentiment when 'Bearish' then -2 when 'Bullish' then 2 ELSE 0 END as sentiment, \n",
    "    body from message_extracted \n",
    "    where body is not null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * hive://user2@ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>symbols</th>\n",
       "        <th>sentiment</th>\n",
       "        <th>body</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;AMZN&quot;},{&quot;symbol&quot;:&quot;MSFT&quot;},{&quot;symbol&quot;:&quot;SPY&quot;},{&quot;symbol&quot;:&quot;TSLA&quot;}]</td>\n",
       "        <td>0</td>\n",
       "        <td>$SPY $AMZN $MSFT $AAPL $TSLA currently using RH but wanting to switch to either think or swim or Webull. Anyone have preference?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>[{&quot;symbol&quot;:&quot;AAPL&quot;}]</td>\n",
       "        <td>2</td>\n",
       "        <td>$AAPL </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>[{&quot;symbol&quot;:&quot;AAPL&quot;}]</td>\n",
       "        <td>2</td>\n",
       "        <td>$AAPL Congrats, longs. It&amp;#39;s been hard not booking some gains on looooong held shares. I felt a beat coming on, and strong guidance, but you never really know. Are we back to sandbagging guidance? Def. back to growth.</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[('[{\"symbol\":\"AAPL\"},{\"symbol\":\"AMZN\"},{\"symbol\":\"MSFT\"},{\"symbol\":\"SPY\"},{\"symbol\":\"TSLA\"}]', '0', '$SPY $AMZN $MSFT $AAPL $TSLA currently using RH but wanting to switch to either think or swim or Webull. Anyone have preference?'),\n",
       " ('[{\"symbol\":\"AAPL\"}]', '2', '$AAPL '),\n",
       " ('[{\"symbol\":\"AAPL\"}]', '2', '$AAPL Congrats, longs. It&#39;s been hard not booking some gains on looooong held shares. I felt a beat coming on, and strong guidance, but you never really know. Are we back to sandbagging guidance? Def. back to growth.')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "select * from message_filtered limit 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * hive://user2@ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "insert overwrite table message_exploded \n",
    "select symbol.symbol, sentiment, body from message_filtered lateral view explode(symbols) symbols as symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * hive://user2@ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>symbol</th>\n",
       "        <th>sentiment</th>\n",
       "        <th>body</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>AAPL</td>\n",
       "        <td>0</td>\n",
       "        <td>$SPY $AMZN $MSFT $AAPL $TSLA currently using RH but wanting to switch to either think or swim or Webull. Anyone have preference?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>AMZN</td>\n",
       "        <td>0</td>\n",
       "        <td>$SPY $AMZN $MSFT $AAPL $TSLA currently using RH but wanting to switch to either think or swim or Webull. Anyone have preference?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>MSFT</td>\n",
       "        <td>0</td>\n",
       "        <td>$SPY $AMZN $MSFT $AAPL $TSLA currently using RH but wanting to switch to either think or swim or Webull. Anyone have preference?</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[('AAPL', '0', '$SPY $AMZN $MSFT $AAPL $TSLA currently using RH but wanting to switch to either think or swim or Webull. Anyone have preference?'),\n",
       " ('AMZN', '0', '$SPY $AMZN $MSFT $AAPL $TSLA currently using RH but wanting to switch to either think or swim or Webull. Anyone have preference?'),\n",
       " ('MSFT', '0', '$SPY $AMZN $MSFT $AAPL $TSLA currently using RH but wanting to switch to either think or swim or Webull. Anyone have preference?')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "select * from message_exploded limit 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * hive://user2@ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "insert overwrite table sentiment_data \n",
    "select sentiment, body from message_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * hive://user2@ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>sentiment</th>\n",
       "        <th>body</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>0</td>\n",
       "        <td>$SPY $AMZN $MSFT $AAPL $TSLA currently using RH but wanting to switch to either think or swim or Webull. Anyone have preference?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2</td>\n",
       "        <td>$AAPL </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2</td>\n",
       "        <td>$AAPL Congrats, longs. It&amp;#39;s been hard not booking some gains on looooong held shares. I felt a beat coming on, and strong guidance, but you never really know. Are we back to sandbagging guidance? Def. back to growth.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>0</td>\n",
       "        <td>$AAPL ambulance for the bears. </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>-2</td>\n",
       "        <td>$AAPL goes up 4 dollars and bulls Cole out of the cave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>-2</td>\n",
       "        <td>$AAPL Lackluster services # wasn&amp;#39;t that supposed to be a primary growth driver.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>0</td>\n",
       "        <td>$SPY $AAPL $AMZN $TSLA still early but why not have some fun 🤷‍♂️</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>0</td>\n",
       "        <td>$AAPL futures going very green right now with the info that the epidemic is possibly slowing down. Less new cases today than yesterday. Gonna help push Apple higher.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2</td>\n",
       "        <td>$SPY when I think of bears I always see them still using them flip phones 😂😂🤦‍♂️ $AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>-2</td>\n",
       "        <td>$AAPL can’t wait to more</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(0, '$SPY $AMZN $MSFT $AAPL $TSLA currently using RH but wanting to switch to either think or swim or Webull. Anyone have preference?'),\n",
       " (2, '$AAPL '),\n",
       " (2, '$AAPL Congrats, longs. It&#39;s been hard not booking some gains on looooong held shares. I felt a beat coming on, and strong guidance, but you never really know. Are we back to sandbagging guidance? Def. back to growth.'),\n",
       " (0, '$AAPL ambulance for the bears. '),\n",
       " (-2, '$AAPL goes up 4 dollars and bulls Cole out of the cave'),\n",
       " (-2, '$AAPL Lackluster services # wasn&#39;t that supposed to be a primary growth driver.'),\n",
       " (0, '$SPY $AAPL $AMZN $TSLA still early but why not have some fun 🤷\\u200d♂️'),\n",
       " (0, '$AAPL futures going very green right now with the info that the epidemic is possibly slowing down. Less new cases today than yesterday. Gonna help push Apple higher.'),\n",
       " (2, '$SPY when I think of bears I always see them still using them flip phones 😂😂🤦\\u200d♂️ $AAPL'),\n",
       " (-2, '$AAPL can’t wait to more')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "select * from sentiment_data limit 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSONファイルの作成\n",
    "\n",
    "加工したデータをJSONファイルとして出力します。\n",
    "\n",
    "感情分析を担当するデータサイエンティスト・機械学習エンジニアは、このJSONファイルを使います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add jar hdfs:/tmp/brickhouse-0.7.1-SNAPSHOT.jar;\n",
    "CREATE TEMPORARY FUNCTION to_json AS 'brickhouse.udf.json.ToJsonUDF';\n",
    "\n",
    "create table json_message (message STRING) STORED AS TEXTFILE;\n",
    "\n",
    "insert overwrite table json_message\n",
    "select to_json(named_struct('message_body', body, 'sentiment', sentiment)) from sentiment_data;\n",
    "\n",
    "select * from json_message;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * hive://user2@ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      "Done.\n",
      " * hive://user2@ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql add jar hdfs:/tmp/brickhouse-0.7.1-SNAPSHOT.jar\n",
    "%sql CREATE TEMPORARY FUNCTION to_json AS 'brickhouse.udf.json.ToJsonUDF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * hive://user2@ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      "Done.\n",
      " * hive://user2@ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql DROP TABLE IF EXISTS json_message\n",
    "%sql create table json_message (message STRING) STORED AS TEXTFILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * hive://user2@ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "insert overwrite table json_message\n",
    "select to_json(named_struct('message_body', body, 'sentiment', sentiment)) from sentiment_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * hive://user2@ip-10-0-0-55.ap-northeast-1.compute.internal:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>message</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>{&quot;message_body&quot;:&quot;$SPY $AMZN $MSFT $AAPL $TSLA currently using RH but wanting to switch to either think or swim or Webull. Anyone have preference?&quot;,&quot;sentiment&quot;:0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>{&quot;message_body&quot;:&quot;$AAPL &quot;,&quot;sentiment&quot;:2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>{&quot;message_body&quot;:&quot;$AAPL Congrats, longs. It&amp;#39;s been hard not booking some gains on looooong held shares. I felt a beat coming on, and strong guidance, but you never really know. Are we back to sandbagging guidance? Def. back to growth.&quot;,&quot;sentiment&quot;:2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>{&quot;message_body&quot;:&quot;$AAPL ambulance for the bears. &quot;,&quot;sentiment&quot;:0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>{&quot;message_body&quot;:&quot;$AAPL goes up 4 dollars and bulls Cole out of the cave&quot;,&quot;sentiment&quot;:-2}</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[('{\"message_body\":\"$SPY $AMZN $MSFT $AAPL $TSLA currently using RH but wanting to switch to either think or swim or Webull. Anyone have preference?\",\"sentiment\":0}',),\n",
       " ('{\"message_body\":\"$AAPL \",\"sentiment\":2}',),\n",
       " ('{\"message_body\":\"$AAPL Congrats, longs. It&#39;s been hard not booking some gains on looooong held shares. I felt a beat coming on, and strong guidance, but you never really know. Are we back to sandbagging guidance? Def. back to growth.\",\"sentiment\":2}',),\n",
       " ('{\"message_body\":\"$AAPL ambulance for the bears. \",\"sentiment\":0}',),\n",
       " ('{\"message_body\":\"$AAPL goes up 4 dollars and bulls Cole out of the cave\",\"sentiment\":-2}',)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "select * from json_message limit 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from __future__ import print_function\n",
    "\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"JsonGen\")\\\n",
    "    .getOrCreate()\n",
    "    \n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "#json_list = spark.read.table(\"json_message\")\n",
    "json_list = spark.sql(\"select * from user2.json_message\")\n",
    "\n",
    "#json_list.show(5)\n",
    "\n",
    "path = \"./output.json\"\n",
    "\n",
    "with open(path, mode='w') as f:\n",
    "    f.write('{\"data\":[')\n",
    "    bool_first_line = True\n",
    "    for row in json_list.rdd.collect():\n",
    "        if bool_first_line:\n",
    "            bool_first_line = False\n",
    "            f.write(row.message)\n",
    "        else:\n",
    "            #print(row.message)\n",
    "            #f.write(row.message.encode(\"utf-8\"))\n",
    "            for i in range(100): # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "                f.write(\",\\n\")\n",
    "                f.write(row.message)\n",
    "    \n",
    "    f.write(\"]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 感情分析\n",
    "企業の価値を決定するときは、ニュースをフォローすることが重要です。たとえば、会社の製品チェーンにおける製品のリコールまたは自然災害。この情報を信号に変換できるようにしたいと考えています。現在、この仕事に最適なツールはニューラルネットワークです。\n",
    "\n",
    "このプロジェクトでは、ソーシャルメディアサイトStockTwitsの投稿を使用します。StockTwitsのコミュニティは、投資家、トレーダー、起業家により利用されています。投稿された各メッセージはTwitと呼ばれます。これはTwitterのツイートによく似ています。感情のスコアを生成するこれらのtwitを中心にモデルを構築します。\n",
    "\n",
    "多数のtwitsを収集し、それぞれの感情を手でラベル付けしました。センチメントの度合いを把握するために、非常にネガティブ、ネガティブ、ニュートラル、ポジティブ、非常にポジティブという5段階のスケールを使用します。各ツイットは、それぞれ非常に負から非常に正まで、1のステップで-2から2までラベル付けされます。このラベル付きデータを使用して、感情を自分でtwitsに割り当てることを学習する感情分析モデルを構築します。\n",
    "\n",
    "最初にすべきことは、データをロードすることです。\n",
    "\n",
    "### データの確認\n",
    "\n",
    "This JSON file contains a list of objects for each twit in the `'data'` field:\n",
    "\n",
    "```\n",
    "{'data':\n",
    "  {'message_body': 'Neutral twit body text here',\n",
    "   'sentiment': 0},\n",
    "  {'message_body': 'Happy twit body text here',\n",
    "   'sentiment': 1},\n",
    "   ...\n",
    "}\n",
    "```\n",
    "\n",
    "The fields represent the following:\n",
    "\n",
    "* `'message_body'`: The text of the twit.\n",
    "* `'sentiment'`: センチメントスコアは、-2から2の範囲で1のステップで、0は中立です。\n",
    "\n",
    "\n",
    "データがどのように見えるかを確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'message_body': '$SPY $AMZN $MSFT $AAPL $TSLA currently using RH but wanting to switch to either think or swim or Webull. Anyone have preference?', 'sentiment': 0}, {'message_body': '$AAPL ', 'sentiment': 2}, {'message_body': '$AAPL ', 'sentiment': 2}, {'message_body': '$AAPL ', 'sentiment': 2}, {'message_body': '$AAPL ', 'sentiment': 2}, {'message_body': '$AAPL ', 'sentiment': 2}, {'message_body': '$AAPL ', 'sentiment': 2}, {'message_body': '$AAPL ', 'sentiment': 2}, {'message_body': '$AAPL ', 'sentiment': 2}, {'message_body': '$AAPL ', 'sentiment': 2}]\n"
     ]
    }
   ],
   "source": [
    "#with open(os.path.join('..', '..', 'data', 'project_6_stocktwits', 'twits.json'), 'r') as f:\n",
    "#with open('./twits_dumped.json', 'r') as f:\n",
    "with open('./output.json', 'r') as f:\n",
    "    twits = json.load(f)\n",
    "\n",
    "print(twits['data'][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データ長の確認\n",
    "Now let's look at the number of twits in dataset. Print the number of twits below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "575901\n"
     ]
    }
   ],
   "source": [
    "\"\"\"print out the number of twits\"\"\"\n",
    "\n",
    "# TODO Implement \n",
    "\n",
    "print(len(twits['data']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データの前処理\n",
    "データを入手したら、テキストを前処理する必要があります。これらのtwitは、twit自体でリーダー$シンボルで示されるティッカーシンボルでフィルタリングすることにより収集されます。例えば、\n",
    "\n",
    "{'message_body': 'RT @google Our annual look at the year in Google blogging (and beyond) http://t.co/sptHOAh8 $GOOG',\n",
    " 'sentiment': 0}\n",
    "\n",
    "ティッカーシンボルはセンチメントに関する情報を提供せず、すべてのツイットに含まれているため、削除する必要があります。このtwitには@googleユーザー名もあり、ここでもセンチメント情報は提供されないため、削除する必要があります。URLも表示されますhttp://t.co/sptHOAh8。これらも削除しましょう。\n",
    "\n",
    "特定の単語やフレーズを削除する最も簡単な方法は、reモジュールを使用して正規表現を使用することです。スペースを使用して特定のパターンをサブアウトできます。\n",
    "\n",
    "re.sub(pattern, ' ', text)\n",
    "これにより、テキスト内のパターンが一致する場所でスペースが置換されます。後でテキストをトークン化するときに、それらのスペースで適切に分割します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Message Body and Sentiment Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [twit['message_body'] for twit in twits['data']]\n",
    "# Since the sentiment scores are discrete, we'll scale the sentiments to 0 to 4 for use in our network\n",
    "sentiments = [twit['sentiment'] + 2 for twit in twits['data']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/cdsw/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "\n",
    "def preprocess(message):\n",
    "    \"\"\"\n",
    "    入力として文字列を受け取り、次の操作を実行する: \n",
    "        - 全てのアルファベットを小文字に変換\n",
    "        - URLを削除\n",
    "        - ティッカーシンボルを削除 \n",
    "        - 句読点を削除\n",
    "        - 文字列をスペースで分割しトークン化する\n",
    "        - シングル・キャラクターのトークンを削除\n",
    "    \n",
    "    パラメータ\n",
    "    ----------\n",
    "        message : 前処理の対象テキストメッセージ\n",
    "        \n",
    "    戻り値\n",
    "    -------\n",
    "        tokens: 前処理後のトークン配列\n",
    "    \"\"\" \n",
    "    #TODO: Implement \n",
    "    \n",
    "    # Lowercase the twit message\n",
    "    text = message.lower()\n",
    "    \n",
    "    # Replace URLs with a space in the message\n",
    "    text = re.sub(\"http(s)?://([\\w\\-]+\\.)+[\\w-]+(/[\\w\\- ./?%&=]*)?\",' ', text)\n",
    "    \n",
    "    # Replace ticker symbols with a space. The ticker symbols are any stock symbol that starts with $.\n",
    "    text = re.sub(\"\\$[^ \\t\\n\\r\\f]+\", ' ', text)\n",
    "    \n",
    "    # Replace StockTwits usernames with a space. The usernames are any word that starts with @.\n",
    "    text = re.sub(\"@[^ \\t\\n\\r\\f]+\", ' ', text)\n",
    "\n",
    "    # Replace everything not a letter with a space\n",
    "    text = re.sub(\"[^a-z]\", ' ', text)\n",
    "    \n",
    "    \n",
    "    # Tokenize by splitting the string on whitespace into a list of words\n",
    "    tokens = text.split()\n",
    "\n",
    "    # Lemmatize words using the WordNetLemmatizer. You can ignore any word that is not longer than one character.\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    tokens = [wnl.lemmatize(w, pos='v') for w in tokens if len(w) > 1]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitsメッセージ前処理\n",
    "Now we can preprocess each of the twits in our dataset. Apply the function `preprocess` to all the twit messages.\n",
    "\n",
    "※この処理には、データのサイズに応じて多少時間がかかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['currently', 'use', 'rh', 'but', 'want', 'to', 'switch', 'to', 'either', 'think', 'or', 'swim', 'or', 'webull', 'anyone', 'have', 'preference'], [], []]\n",
      "575901\n"
     ]
    }
   ],
   "source": [
    "tokenized = list(map(preprocess, messages))\n",
    "\n",
    "print(tokenized[:3])\n",
    "print(len(tokenized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words\n",
    "\n",
    "すべてのメッセージがトークン化されたので、語彙を作成し、コーパス全体で各単語が出現する頻度をカウントします。Counter関数を使用して、すべてのトークンをカウントアップします。\n",
    "[`Counter`](https://docs.python.org/3.1/library/collections.html#collections.Counter)\n",
    "\n",
    "※この処理には、データのサイズに応じて多少時間がかかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['currently', 'use', 'rh', 'but', 'want', 'to', 'switch', 'to', 'either', 'think', 'or', 'swim', 'or']\n",
      "7449017\n",
      "575901\n",
      "[[712, 561, 3243, 89, 295, 3, 2268, 3, 791, 135, 67, 1801, 67, 3244, 675, 25, 3245], [], []]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "#words = []\n",
    "#for tokens in tokenized:\n",
    "#    for token in tokens:\n",
    "#        words.append(token)\n",
    "out_list = tokenized\n",
    "words = [element for in_list in out_list for element in in_list]\n",
    "\n",
    "print(words[:13])\n",
    "print(len(words))\n",
    "\n",
    "\"\"\"\n",
    "Create a vocabulary by using Bag of words\n",
    "\"\"\"\n",
    "\n",
    "# TODO: Implement \n",
    "\n",
    "word_counts = Counter(words)\n",
    "sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "int_to_vocab = {ii: word for ii, word in enumerate(sorted_vocab)}\n",
    "vocab_to_int = {word:ii for ii, word in int_to_vocab.items()}\n",
    "\n",
    "bow = []\n",
    "for tokens in tokenized:\n",
    "    bow.append([vocab_to_int[token] for token in tokens])\n",
    "\n",
    "print(len(bow))\n",
    "print(bow[:3])\n",
    "\n",
    "# This BOW will not be used because it is not filtered to eliminate common words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### メッセージに現れる単語の頻度\n",
    "\n",
    "ボキャブラリーを使用して、「the」、「and」、「it」などの最も一般的な単語の一部を削除します。\n",
    "これらの単語は感情を特定するのに寄与せず、非常に一般的であるため、ニューラルネットワークの入力のノイズとなります。これらを除外することで、ネットワークの学習時間を短縮することができます。\n",
    "\n",
    "また、ほんの数回しか使われていない、非常にまれな単語も削除します。ここでは、各単語のカウントをメッセージの数で除算する必要があります。次に、メッセージのごく一部にしか表示されない単語を削除します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(sorted_vocab): 5849\n",
      "sorted_vocab - top: ['the', 'be', 'of']\n",
      "sorted_vocab - least: ['reek', 'hahahahaha', 'god', 'rumor', 'otl', 'unless', 'spec', 'mouse', 'supplement', 'issuance', 'institutions', 'ponied', 'emotionally', 'knee', 'jerk']\n",
      "freqs[the]: 0.027184794987043258\n",
      "high_cutoff: 20\n",
      "low_cutoff: 2e-06\n",
      "K_most_common: ['the', 'be', 'of', 'to', 'amp', 'utm', 'in', 'for', 'and', 'on', 'file', 'form', 'stock', 'share', 'by', 'sec', 'at', 'earn', 'report', 'this']\n",
      "len(filtered_words): 5829\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Set the following variables:\n",
    "    freqs\n",
    "    low_cutoff\n",
    "    high_cutoff\n",
    "    K_most_common\n",
    "\"\"\"\n",
    "\n",
    "# TODO Implement \n",
    "\n",
    "print(\"len(sorted_vocab):\",len(sorted_vocab))\n",
    "print(\"sorted_vocab - top:\", sorted_vocab[:3])\n",
    "print(\"sorted_vocab - least:\", sorted_vocab[-15:])\n",
    "\n",
    "# Dictionart that contains the Frequency of words appearing in messages.\n",
    "# The key is the token and the value is the frequency of that word in the corpus.\n",
    "total_count = len(words)\n",
    "freqs = {word: count/total_count for word, count in word_counts.items()}\n",
    "\n",
    "#print(\"freqs[supplication]:\",freqs[\"supplication\"] )\n",
    "print(\"freqs[the]:\",freqs[\"the\"] )\n",
    "\n",
    "\"\"\"\n",
    "This was the post by Ricardo:\n",
    "\n",
    "there's no exact value for low_cutoff and high_cutoff, \n",
    "however I'd recommend you to use \n",
    "a low_cutoff that's around 0.000002 and 0.000007 \n",
    "(This depends on the values you get from your freqs calculations) and \n",
    "a high_cutofffrom 5 to 20 (this depends on the most_common values from the bow).\n",
    "\"\"\"\n",
    "\n",
    "# Float that is the frequency cutoff. Drop words with a frequency that is lower or equal to this number.\n",
    "low_cutoff = 0.000002\n",
    "\n",
    "# Integer that is the cut off for most common words. Drop words that are the `high_cutoff` most common words.\n",
    "\"\"\"\n",
    "example_count = []\n",
    "example_count.append(sorted_vocab.index(\"the\"))\n",
    "example_count.append(sorted_vocab.index(\"for\"))\n",
    "example_count.append(sorted_vocab.index(\"of\"))\n",
    "print(example_count)\n",
    "high_cutoff = min(example_count)\n",
    "\"\"\"\n",
    "high_cutoff = 20\n",
    "print(\"high_cutoff:\",high_cutoff)\n",
    "print(\"low_cutoff:\",low_cutoff)\n",
    "\n",
    "# The k most common words in the corpus. Use `high_cutoff` as the k.\n",
    "#K_most_common = [word for word in sorted_vocab[:high_cutoff]]\n",
    "K_most_common = sorted_vocab[:high_cutoff]\n",
    "\n",
    "print(\"K_most_common:\",K_most_common)\n",
    "\n",
    "\n",
    "##  END of TODO Implement\n",
    "\n",
    "filtered_words = [word for word in freqs if (freqs[word] > low_cutoff and word not in K_most_common)]\n",
    "\n",
    "print(\"len(filtered_words):\",len(filtered_words)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### フィルターされた単語を削除して語彙を更新する¶\n",
    "ボキャブラリーに役立つ3つの変数を作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(tokenized): 575901\n",
      "len(filtered): 575901\n",
      "tokenized[:1] [['currently', 'use', 'rh', 'but', 'want', 'to', 'switch', 'to', 'either', 'think', 'or', 'swim', 'or', 'webull', 'anyone', 'have', 'preference']]\n",
      "filtered[:1] [['currently', 'use', 'rh', 'but', 'want', 'switch', 'either', 'think', 'or', 'swim', 'or', 'webull', 'anyone', 'have', 'preference']]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Set the following variables:\n",
    "    vocab\n",
    "    id2vocab\n",
    "    filtered\n",
    "\"\"\"\n",
    "\n",
    "#TODO Implement\n",
    "\n",
    "# A dictionary for the `filtered_words`. The key is the word and value is an id that represents the word. \n",
    "vocab =  {word:ii for ii, word in enumerate(filtered_words)}\n",
    "# Reverse of the `vocab` dictionary. The key is word id and value is the word. \n",
    "id2vocab = {ii:word for word, ii in vocab.items()}\n",
    "# tokenized with the words not in `filtered_words` removed.\n",
    "\n",
    "print(\"len(tokenized):\", len(tokenized))\n",
    "\n",
    "filtered = [[token for token in tokens if token in vocab] for tokens in tokenized]\n",
    "print(\"len(filtered):\", len(filtered))\n",
    "print(\"tokenized[:1]\", tokenized[:1])\n",
    "print(\"filtered[:1]\",filtered[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### クラスのバランス\n",
    "最後の前処理ステップをいくつか行いましょう。twitのラベル付けを見ると、twitの50％がニュートラルであることがわかります。これは、毎回0を推測するだけで、ネットワークの精度が50％になることを意味します。ネットワークが適切に学習できるように、クラスのバランスを取る必要があります。つまり、それぞれのセンチメントスコアがデータにほぼ同じ頻度で表示されることを確認します。\n",
    "\n",
    "ここでできることは、それぞれの例に目を通し、中立的な感情を持つtwitsをランダムにドロップすることです。50％のニュートラルから20％のニュートラルtwitを取得したい場合、これらのtwitをドロップする確率はどうなりますか？この機会に、長さ0のメッセージを削除する必要もあります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "balanced = {'messages': [], 'sentiments':[]}\n",
    "\n",
    "n_neutral = sum(1 for each in sentiments if each == 2)\n",
    "N_examples = len(sentiments)\n",
    "keep_prob = (N_examples - n_neutral)/4/n_neutral\n",
    "\n",
    "for idx, sentiment in enumerate(sentiments):\n",
    "    message = filtered[idx]\n",
    "    if len(message) == 0:\n",
    "        # skip this message because it has length zero\n",
    "        continue\n",
    "    elif sentiment != 2 or random.random() < keep_prob:\n",
    "        balanced['messages'].append(message)\n",
    "        balanced['sentiments'].append(sentiment) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you did it correctly, you should see the following result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21319965473740124"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_neutral = sum(1 for each in balanced['sentiments'] if each == 2)\n",
    "N_examples = len(balanced['sentiments'])\n",
    "n_neutral/N_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's convert our tokens into integer ids which we can pass to the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = [[vocab[word] for word in message] for message in balanced['messages']]\n",
    "sentiments = balanced['sentiments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#from singer import Singer\n",
    "\n",
    "#singer = Singer('Shanranran')\n",
    "\n",
    "with open('vocab.pickle', 'wb') as f:\n",
    "    pickle.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ニューラルネットワーク\n",
    "これでボキャブラリーができたので、トークンをIDに変換し、それをネットワークに渡すことができます。ネットワークを定義します\n",
    "\n",
    "下記は、ネットワークの概要です：\n",
    "\n",
    "#### Embed -> RNN -> Dense -> Softmax\n",
    "### Text classifier (テキスト分類器)実装\n",
    "テキスト分類器を作成する前に、「RNNを使用したセンチメント分析」演習で作成した他のネットワーク（ここでは「SentimentRNN」と呼ばれるネットワーク、ここでは「TextClassifer」と呼びます）を覚えている場合、3つの主要な部分で構成されています：: 1) init function `__init__` 2) forward pass `forward`  3) hidden state `init_hidden`. \n",
    "\n",
    "このネットワークは、forwardパスで期待して構築したネットワークに非常に似ています 。シグモイドの代わりにsoftmaxを使用します。シグモイドを使用しないのは、NNの出力がバイナリではないためです。このネットワークでは、センチメントスコアには5つの結果があります。最も高い確率の結果を探しているため、softmaxの方が適しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, lstm_size, output_size, lstm_layers=1, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "            vocab_size : The vocabulary size.\n",
    "            embed_size : The embedding layer size.\n",
    "            lstm_size : The LSTM layer size.\n",
    "            output_size : The output size.\n",
    "            lstm_layers : The number of LSTM layers.\n",
    "            dropout : The dropout probability.\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.lstm_size = lstm_size\n",
    "        self.output_size = output_size\n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # TODO Implement\n",
    "\n",
    "        # Setup embedding layer\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embed_size)\n",
    "        \n",
    "        # Setup additional layers\n",
    "        self.lstm = nn.LSTM(self.embed_size, self.lstm_size, self.lstm_layers, dropout=self.dropout)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(lstm_size, output_size)\n",
    "        \n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\" \n",
    "        Initializes hidden state\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "            batch_size : The size of batches.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "            hidden_state\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO Implement \n",
    "        \n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        \n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        hidden = (weight.new(self.lstm_layers, batch_size,self.lstm_size).zero_(),\n",
    "                         weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_())\n",
    "        return hidden\n",
    "\n",
    "\n",
    "    def forward(self, nn_input, hidden_state):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on nn_input.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "            nn_input : The batch of input to the NN.\n",
    "            hidden_state : The LSTM hidden state.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            logps: log softmax output\n",
    "            hidden_state: The new hidden state.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO Implement \n",
    "        batch_size = nn_input.size(0)\n",
    "        \n",
    "        embeds = self.embedding(nn_input)\n",
    "        lstm_out, hidden_state = self.lstm(embeds, hidden_state)\n",
    "        \n",
    "        #lstm_out = lstm_out.contiguous().view(-1, self.lstm_size)    \n",
    "        \"\"\"\n",
    "        remember here you do not have batch_first=True, \n",
    "        so accordingly shape your input. \n",
    "        Moreover, since now input is seq_length x batch you just need to transform lstm_out = lstm_out[-1,:,:].\n",
    "        you don't have to use batch_first=True in this case, \n",
    "        nor reshape the outputs with .view just transform your lstm_out as advised and you should be good to go.\n",
    "        \"\"\"\n",
    "        lstm_out = lstm_out[-1,:,:]\n",
    "        \n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        logps = self.softmax(out)\n",
    "        \n",
    "        \n",
    "        return logps, hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5954, -1.7832, -1.7476, -1.4568, -1.5055],\n",
      "        [-1.5747, -1.8081, -1.7448, -1.4557, -1.5095],\n",
      "        [-1.5760, -1.8013, -1.7465, -1.4589, -1.5085],\n",
      "        [-1.6398, -1.8044, -1.7004, -1.4428, -1.5027]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "model = TextClassifier(len(vocab), 10, 6, 5, dropout=0.1, lstm_layers=2)\n",
    "model.embedding.weight.data.uniform_(-1, 1)\n",
    "input = torch.randint(0, 1000, (5, 4), dtype=torch.int64)\n",
    "hidden = model.init_hidden(4)\n",
    "\n",
    "logps, _ = model.forward(input, hidden)\n",
    "print(logps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### トレーニング\n",
    "### DataLoaderとバッチ処理\n",
    "ここで、データをループするために使用できるジェネレーターを構築する必要があります。シーケンスをバッチとして渡すことができれば、より効率的です。入力テンソルは次のようになり(sequence_length, batch_size)ます。したがって、シーケンスが40トークンで、25シーケンスを渡す場合、入力サイズはになり(40, 25)ます。\n",
    "\n",
    "シーケンスの長さを40に設定した場合、40トークンより多いまたは少ないメッセージをどう処理しますか？40トークン未満のメッセージの場合、空のスポットにゼロを埋め込みます。データを処理する前にRNNが何も開始しないように、必ずパッドを残しておく必要があります。メッセージに20個のトークンがある場合、40個の長いシーケンスの最初の20個のスポットは0になります。メッセージに40個を超えるトークンがある場合、最初の40個のトークンを保持します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def dataloader(messages, labels, sequence_length=30, batch_size=32, shuffle=False):\n",
    "def dataloader(messages, labels, sequence_length=20, batch_size=32, shuffle=False):\n",
    "    \"\"\" \n",
    "    Build a dataloader.\n",
    "    \"\"\"\n",
    "    if shuffle:\n",
    "        indices = list(range(len(messages)))\n",
    "        random.shuffle(indices)\n",
    "        messages = [messages[idx] for idx in indices]\n",
    "        labels = [labels[idx] for idx in indices]\n",
    "\n",
    "    total_sequences = len(messages)\n",
    "\n",
    "    for ii in range(0, total_sequences, batch_size):\n",
    "        batch_messages = messages[ii: ii+batch_size]\n",
    "        \n",
    "        # First initialize a tensor of all zeros\n",
    "        batch = torch.zeros((sequence_length, len(batch_messages)), dtype=torch.int64)\n",
    "        for batch_num, tokens in enumerate(batch_messages):\n",
    "            token_tensor = torch.tensor(tokens)\n",
    "            # Left pad!\n",
    "            start_idx = max(sequence_length - len(token_tensor), 0)\n",
    "            batch[start_idx:, batch_num] = token_tensor[:sequence_length]\n",
    "        \n",
    "        label_tensor = torch.tensor(labels[ii: ii+len(batch_messages)])\n",
    "        \n",
    "        yield batch, label_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and  Validation\n",
    "With our data in nice shape, we'll split it into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Split data into training and validation datasets. Use an appropriate split size.\n",
    "The features are the `token_ids` and the labels are the `sentiments`.\n",
    "\"\"\"   \n",
    "\n",
    "# TODO Implement \n",
    "\n",
    "split_frac = 0.98 # for small data\n",
    "#split_frac = 0.8 # for big data\n",
    "\n",
    "## split data into training, validation, and test data (features and labels, x and y)\n",
    "\n",
    "split_idx = int(len(token_ids)*split_frac)\n",
    "train_features, remaining_features = token_ids[:split_idx], token_ids[split_idx:]\n",
    "train_labels, remaining_labels = sentiments[:split_idx], sentiments[split_idx:]\n",
    "\n",
    "test_idx = int(len(remaining_features)*0.5)\n",
    "valid_features, test_features = remaining_features[:test_idx], remaining_features[test_idx:]\n",
    "valid_labels, test_labels = remaining_labels[:test_idx], remaining_labels[test_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_batch, labels = next(iter(dataloader(train_features, train_labels, sequence_length=20, batch_size=64)))\n",
    "model = TextClassifier(len(vocab)+1, 200, 128, 5, dropout=0.)\n",
    "hidden = model.init_hidden(64)\n",
    "logps, hidden = model.forward(text_batch, hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "It's time to train the neural network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextClassifier(\n",
       "  (embedding): Embedding(5830, 1024)\n",
       "  (lstm): LSTM(1024, 512, num_layers=2, dropout=0.2)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (fc): Linear(in_features=512, out_features=5, bias=True)\n",
       "  (softmax): LogSoftmax()\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = TextClassifier(len(vocab)+1, 1024, 512, 5, lstm_layers=2, dropout=0.2)\n",
    "model.embedding.weight.data.uniform_(-1, 1)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### トレーニング実施\n",
    "\n",
    "※この処理には、データのサイズに応じて、十分な時間が必要です。\n",
    "\n",
    "GPUを備えた環境で実行する場合、ターミナルで以下のコマンドを実行することで、GPUが利用されていることを確認することができます（ GPU実行中、コマンド実行により表示されるテーブルの右上のVolatile GPU-Utilのパーセンテージ値が増えます）\n",
    "```\n",
    "$ watch nvidia-smi\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Train your model with dropout. Make sure to clip your gradients.\n",
    "Print the training loss, validation loss, and validation accuracy for every 100 steps.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "epochs = 4 #pass\n",
    "batch_size =  64#pass\n",
    "batch_size =  512#pass\n",
    "learning_rate = 0.001 #pass\n",
    "\n",
    "print_every = 100\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "model.train()\n",
    "\n",
    "val_losses = []\n",
    "accuracy = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Starting epoch {}'.format(epoch + 1))\n",
    "    \n",
    "    steps = 0\n",
    "    for text_batch, labels in dataloader(\n",
    "            train_features, train_labels, batch_size=batch_size, sequence_length=20, shuffle=True):\n",
    "        steps += 1\n",
    "        hidden = model.init_hidden(labels.shape[0]) #pass\n",
    "        \n",
    "        # Set Device\n",
    "        text_batch, labels = text_batch.to(device), labels.to(device)\n",
    "        for each in hidden:\n",
    "            each.to(device)\n",
    "        \n",
    "        # TODO Implement: Train Model\n",
    "        hidden = tuple([each.data for each in hidden])\n",
    "        model.zero_grad()\n",
    "        output, hidden = model(text_batch, hidden)\n",
    "        loss = criterion(output.squeeze(), labels)\n",
    "        loss.backward()\n",
    "        clip = 5\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate loss\n",
    "        val_losses.append(loss.item())\n",
    "        \n",
    "        correct_count = 0.0\n",
    "        if steps % print_every == 0:\n",
    "            model.eval()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            ps = torch.exp(output)\n",
    "            top_p, top_class = ps.topk(1, dim=1)\n",
    "            #?top_class = top_class.to(device)\n",
    "            #?labels = labels.to(device)\n",
    "\n",
    "            correct_count += torch.sum(top_class.squeeze()== labels)\n",
    "            accuracy.append(100*correct_count/len(labels))\n",
    "            \n",
    "            # TODO Implement: Print metrics\n",
    "            print(\"Epoch: {}/{}...\".format(epoch+1, epochs),\n",
    "                 \"Step: {}...\".format(steps),\n",
    "                 \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                 \"Val Loss: {:.6f}\".format(np.mean(val_losses)),\n",
    "                 \"Collect Count: {}\".format(correct_count),\n",
    "                 \"Accuracy: {:.2f}\".format((100*correct_count/len(labels))),\n",
    "                 # AttributeError: 'torch.dtype' object has no attribute 'type'\n",
    "                 #\"Accuracy Avg: {:.2f}\".format(np.mean(accuracy))\n",
    "                 )\n",
    "            \n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'state_dict': model.state_dict()}, 'checkpoint.pth.tar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 予測（Prediction）関数の作成\n",
    "### Prediction \n",
    "訓練されたモデルを手に入れたので、新しいツイットでそれを試して、それが適切に機能するかどうか確かめてください。新しいテキストについては、ネットワークに渡す前に最初に前処理する必要があることに注意してください。predictメッセージから予測ベクトルを生成する関数を実装します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/cdsw/checkpoint.pth.tar', '/home/cdsw/nlp_handson.ipynb', '/home/cdsw/twits_dumped.json', '/home/cdsw/test.py', '/home/cdsw/data', '/home/cdsw/lib', '/home/cdsw/nlp_solution.ipynb', '/home/cdsw/README.md', '/home/cdsw/model.torch', '/home/cdsw/nltk_data', '/home/cdsw/tables.hql', '/home/cdsw/ticker.txt', '/home/cdsw/vocab.pickle', '/home/cdsw/init.sh', '/home/cdsw/output.json']\n",
      "/home/cdsw\n",
      "[ 0.00204649  0.02024106  0.08254681  0.09021453  0.80495113]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/cdsw/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "print(glob.glob(\"/home/cdsw/*\"))\n",
    "\n",
    "import pickle\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import torch\n",
    "\n",
    "#from sentiment import TextClassifier\n",
    "\n",
    "import os\n",
    "import sys\n",
    "cur_dir = os.path.dirname(os.path.abspath('__file__'))\n",
    "print(cur_dir)\n",
    "sys.path.append(cur_dir)\n",
    "\n",
    "vocab_filename = 'vocab.pickle'\n",
    "vocab_path = cur_dir + \"/\" + vocab_filename\n",
    "vocab_l = pickle.load(open(vocab_path, 'rb'))\n",
    "\n",
    "#model_path = cur_dir + \"/\" + \"model.torch\"\n",
    "#model_l = torch.load(model_path, map_location='cpu')\n",
    "\n",
    "model_l = TextClassifier(len(vocab_l)+1, 1024, 512, 5, lstm_layers=2, dropout=0.2)\n",
    "checkpoint = torch.load('./checkpoint.pth.tar')\n",
    "model_l.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "\n",
    "def preprocess(message):\n",
    "    \"\"\"\n",
    "    This function takes a string as input, then performs these operations: \n",
    "        - lowercase\n",
    "        - remove URLs\n",
    "        - remove ticker symbols \n",
    "        - removes punctuation\n",
    "        - tokenize by splitting the string on whitespace \n",
    "        - removes any single character tokens\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        message : The text message to be preprocessed.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "        tokens: The preprocessed text into tokens.\n",
    "    \"\"\" \n",
    "    \n",
    "    # Lowercase the twit message\n",
    "    text = message.lower()\n",
    "    \n",
    "    # Replace URLs with a space in the message\n",
    "    text = re.sub(\"http(s)?://([\\w\\-]+\\.)+[\\w-]+(/[\\w\\- ./?%&=]*)?\",' ', text)\n",
    "    \n",
    "    # Replace ticker symbols with a space. The ticker symbols are any stock symbol that starts with $.\n",
    "    text = re.sub(\"\\$[^ \\t\\n\\r\\f]+\", ' ', text)\n",
    "    \n",
    "    # Replace StockTwits usernames with a space. The usernames are any word that starts with @.\n",
    "    text = re.sub(\"@[^ \\t\\n\\r\\f]+\", ' ', text)\n",
    "\n",
    "    # Replace everything not a letter with a space\n",
    "    text = re.sub(\"[^a-z]\", ' ', text)\n",
    "    \n",
    "    \n",
    "    # Tokenize by splitting the string on whitespace into a list of words\n",
    "    tokens = text.split()\n",
    "\n",
    "    # Lemmatize words using the WordNetLemmatizer. You can ignore any word that is not longer than one character.\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    tokens = [wnl.lemmatize(w, pos='v') for w in tokens if len(w) > 1]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "\n",
    "def predict_func(text, model, vocab):\n",
    "    \"\"\" \n",
    "    Make a prediction on a single sentence.\n",
    "    Parameters\n",
    "    ----------\n",
    "        text : The string to make a prediction on.\n",
    "        model : The model to use for making the prediction.\n",
    "        vocab : Dictionary for word to word ids. The key is the word and the value is the word id.\n",
    "    Returns\n",
    "    -------\n",
    "        pred : Prediction vector\n",
    "    \"\"\"\n",
    "\n",
    "    tokens = preprocess(text)    \n",
    "\n",
    "    # Filter non-vocab words\n",
    "    tokens = [token for token in tokens if token in vocab] #pass\n",
    "    # Convert words to ids\n",
    "    tokens = [vocab[token] for token in tokens] #pass\n",
    "\n",
    "    if len(tokens) == 0:\n",
    "      raise UnknownWordsError\n",
    "\n",
    "    # Adding a batch dimension\n",
    "    text_input = torch.from_numpy(np.asarray(torch.LongTensor(tokens).view(-1, 1)))\n",
    "\n",
    "    # Get the NN output       \n",
    "    batch_size = 1\n",
    "    hidden = model.init_hidden(batch_size) #pass\n",
    "    \n",
    "    logps, _ = model(text_input, hidden) #pass\n",
    "    # Take the exponent of the NN output to get a range of 0 to 1 for each label.\n",
    "    pred = torch.round(logps.squeeze())#pass\n",
    "    pred = torch.exp(logps) \n",
    "    \n",
    "    return pred\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predict_api(args):\n",
    "  text = args.get('text')\n",
    "  try:\n",
    "    result = predict_func(text, model_l, vocab_l)\n",
    "    return result.detach().numpy()[0]\n",
    "  except UnknownWordsError:\n",
    "    return [0,0,1,0,0]\n",
    "    \n",
    "\n",
    "#args = {\"text\": \"Google is working on self driving cars, I'm bullish on $goog\"}\n",
    "#args = {\"text\": \"I'm bullish on $goog\"}\n",
    "args = {\"text\": \"I'll strongly recommend to buy on $goog\"}\n",
    "#args = {\"text\": \"elyoq baoq pquq $goog\"}\n",
    "result = predict_api(args)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, model, vocab):\n",
    "    \"\"\" \n",
    "    Make a prediction on a single sentence.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        text : The string to make a prediction on.\n",
    "        model : The model to use for making the prediction.\n",
    "        vocab : Dictionary for word to word ids. The key is the word and the value is the word id.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        pred : Prediction vector\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO Implement\n",
    "    tokens = preprocess(text)\n",
    "\n",
    "    # Filter non-vocab words\n",
    "    tokens = [token for token in tokens if token in vocab] #pass\n",
    "    # Convert words to ids\n",
    "    tokens = [vocab[token] for token in tokens] #pass\n",
    "\n",
    "    # Adding a batch dimension\n",
    "    text_input = torch.from_numpy(np.asarray(torch.LongTensor(tokens).view(-1, 1)))\n",
    "\n",
    "    # Get the NN output       \n",
    "    batch_size = 1\n",
    "    hidden = model.init_hidden(batch_size) #pass\n",
    "    \n",
    "    logps, _ = model(text_input, hidden) #pass\n",
    "    # Take the exponent of the NN output to get a range of 0 to 1 for each label.\n",
    "    pred = torch.round(logps.squeeze())#pass\n",
    "    pred = torch.exp(logps) \n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1989, 0.1649, 0.1976, 0.1587, 0.2799]], grad_fn=<ExpBackward>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Good good good wonderful\"\n",
    "model.eval()\n",
    "model.to(\"cpu\")\n",
    "predict(text, model, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1995, 0.1990, 0.2000, 0.1916, 0.2100]], grad_fn=<ExpBackward>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Bad bad bad worst\"\n",
    "model.eval()\n",
    "model.to(\"cpu\")\n",
    "predict(text, model, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2138, 0.1600, 0.2078, 0.1529, 0.2655]], grad_fn=<ExpBackward>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Google is working on self driving cars, I'm bullish on $goog\"\n",
    "model.eval()\n",
    "model.to(\"cpu\")\n",
    "predict(text, model, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions: What is the prediction of the model? What is the uncertainty of the prediction?\n",
    "** TODO: Answer Question**\n",
    "\n",
    "#### What is the prediction of the model?\n",
    "The prediction to the text above is positive as the highest value is positive in the probability list - very negative, negative, neutral, positive, very positive.\n",
    "#### What is the uncertainty of the prediction?\n",
    "When considering the sum of the rest values except for the highest class, the uncertainty of the prediction is low and when taking into account the both positive and very positive, the uncertainty is very low. So, the prediction seems appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a trained model and we can make predictions. We can use this model to track the sentiments of various stocks by predicting the sentiments of twits as they are coming in. Now we have a stream of twits. For each of those twits, pull out the stocks mentioned in them and keep track of the sentiments. Remember that in the twits, ticker symbols are encoded with a dollar sign as the first character, all caps, and 2-4 letters, like $AAPL. Ideally, you'd want to track the sentiments of the stocks in your universe and use this as a signal in your larger model(s).\n",
    "\n",
    "## Testing\n",
    "### Load the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('..', '..', 'data', 'project_6_stocktwits', 'test_twits.json'), 'r') as f:\n",
    "    test_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twit Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message_body': '$JWN has moved -1.69% on 10-31. Check out the movement and peers at  https://dividendbot.com?s=JWN',\n",
       " 'timestamp': '2018-11-01T00:00:05Z'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def twit_stream():\n",
    "    for twit in test_data['data']:\n",
    "        yield twit\n",
    "\n",
    "next(twit_stream())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `prediction` function, let's apply it to a stream of twits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_twits(stream, model, vocab, universe):\n",
    "    \"\"\" \n",
    "    Given a stream of twits and a universe of tickers, return sentiment scores for tickers in the universe.\n",
    "    \"\"\"\n",
    "    for twit in stream:\n",
    "\n",
    "        # Get the message text\n",
    "        text = twit['message_body']\n",
    "        symbols = re.findall('\\$[A-Z]{2,4}', text)\n",
    "        score = predict(text, model, vocab)\n",
    "\n",
    "        for symbol in symbols:\n",
    "            if symbol in universe:\n",
    "                yield {'symbol': symbol, 'score': score, 'timestamp': twit['timestamp']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'symbol': '$AAPL',\n",
       " 'score': tensor([[ 0.1006,  0.1506,  0.2158,  0.2898,  0.2432]]),\n",
       " 'timestamp': '2018-11-01T00:00:18Z'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "universe = {'$BBRY', '$AAPL', '$AMZN', '$BABA', '$YHOO', '$LQMT', '$FB', '$GOOG', '$BBBY', '$JNUG', '$SBUX', '$MU'}\n",
    "score_stream = score_twits(twit_stream(), model, vocab, universe)\n",
    "\n",
    "next(score_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql DROP DATABASE IF EXISTS user1 CASCADE;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it. You have successfully built a model for sentiment analysis! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
