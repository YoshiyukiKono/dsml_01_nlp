{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science / Machine Learning Meetup #1 Deep Learning Hands-on\n",
    "# オルタナティブ・データと自然言語処理\n",
    "\n",
    "## はじめに\n",
    "\n",
    "演習の概略は以下の通りです。\n",
    "1. [環境準備](#環境準備)\n",
    "1. [WEBスクレイピング](#WEBスクレイピング)\n",
    "1. [感情分析](#感情分析)\n",
    "    1. 前処理\n",
    "    1. ニューラル・ネットワーク構築\n",
    "    1. トレーニング\n",
    "    1. 予測\n",
    "\n",
    "以下の点にご注意ください。\n",
    "- 実行するコードの中に、ご利用中のユーザー名に合わせて、変更していただく部分があります。\n",
    "\n",
    "## 環境準備\n",
    "\n",
    "### パッケージのインストールとインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipython-sql==0.3.9\n",
      "  Downloading https://files.pythonhosted.org/packages/ab/df/427e7cf05ffc67e78672ad57dce2436c1e825129033effe6fcaf804d0c60/ipython_sql-0.3.9-py2.py3-none-any.whl\n",
      "Requirement already satisfied: ipython-genutils>=0.1.0 in /usr/local/lib/python3.6/dist-packages (from ipython-sql==0.3.9) (0.2.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from ipython-sql==0.3.9) (1.12.0)\n",
      "Requirement already satisfied: ipython>=1.0 in /usr/local/lib/python3.6/dist-packages (from ipython-sql==0.3.9) (5.1.0)\n",
      "Collecting prettytable (from ipython-sql==0.3.9)\n",
      "  Downloading https://files.pythonhosted.org/packages/ef/30/4b0746848746ed5941f052479e7c23d2b56d174b82f4fd34a25e389831f5/prettytable-0.7.2.tar.bz2\n",
      "Collecting sqlparse (from ipython-sql==0.3.9)\n",
      "  Downloading https://files.pythonhosted.org/packages/ef/53/900f7d2a54557c6a37886585a91336520e5539e3ae2423ff1102daf4f3a7/sqlparse-0.3.0-py2.py3-none-any.whl\n",
      "Collecting sqlalchemy>=0.6.7 (from ipython-sql==0.3.9)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/47/35edeb0f86c0b44934c05d961c893e223ef27e79e1f53b5e6f14820ff553/SQLAlchemy-1.3.13.tar.gz (6.0MB)\n",
      "\u001b[K     |████████████████████████████████| 6.0MB 5.4MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=1.0->ipython-sql==0.3.9) (0.8.1)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython>=1.0->ipython-sql==0.3.9) (2.4.2)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython>=1.0->ipython-sql==0.3.9) (41.0.1)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=1.0->ipython-sql==0.3.9) (0.7.5)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=1.0->ipython-sql==0.3.9) (4.7.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython>=1.0->ipython-sql==0.3.9) (4.4.0)\n",
      "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython>=1.0->ipython-sql==0.3.9) (4.3.2)\n",
      "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.3 in /usr/local/lib/python3.6/dist-packages (from ipython>=1.0->ipython-sql==0.3.9) (1.0.15)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=1.0->ipython-sql==0.3.9) (0.6.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.3->ipython>=1.0->ipython-sql==0.3.9) (0.1.7)\n",
      "Building wheels for collected packages: prettytable, sqlalchemy\n",
      "  Building wheel for prettytable (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/cdsw/.cache/pip/wheels/80/34/1c/3967380d9676d162cb59513bd9dc862d0584e045a162095606\n",
      "  Building wheel for sqlalchemy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/cdsw/.cache/pip/wheels/b3/35/98/4c9cb3fd63d21d5606b972dd70643769745adf60e622467b71\n",
      "Successfully built prettytable sqlalchemy\n",
      "Installing collected packages: prettytable, sqlparse, sqlalchemy, ipython-sql\n",
      "Successfully installed ipython-sql-0.3.9 prettytable-0.7.2 sqlalchemy-1.3.13 sqlparse-0.3.0\n",
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting PyHive==0.6.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4e/26/de91125c0d9e8947d48f387f4d1f2e7a22aa92a30771ad02f63a5653361b/PyHive-0.6.1.tar.gz (41kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 2.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting future (from PyHive==0.6.1)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
      "\u001b[K     |████████████████████████████████| 829kB 4.8MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from PyHive==0.6.1) (2.8.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil->PyHive==0.6.1) (1.12.0)\n",
      "Building wheels for collected packages: PyHive, future\n",
      "  Building wheel for PyHive (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/cdsw/.cache/pip/wheels/00/61/fb/77a0e77deb4c900276f689e62628a5ca7ba9df600f9ad7ba6a\n",
      "  Building wheel for future (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/cdsw/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
      "Successfully built PyHive future\n",
      "Installing collected packages: future, PyHive\n",
      "Successfully installed PyHive-0.6.1 future-0.18.2\n",
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: SQLAlchemy==1.3.13 in ./.local/lib/python3.6/site-packages (1.3.13)\n",
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting thrift==0.13.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/97/1e/3284d19d7be99305eda145b8aa46b0c33244e4a496ec66440dac19f8274d/thrift-0.13.0.tar.gz (59kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 3.0MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.7.2 in /usr/local/lib/python3.6/dist-packages (from thrift==0.13.0) (1.12.0)\n",
      "Building wheels for collected packages: thrift\n",
      "  Building wheel for thrift (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/cdsw/.cache/pip/wheels/02/a2/46/689ccfcf40155c23edc7cdbd9de488611c8fdf49ff34b1706e\n",
      "Successfully built thrift\n",
      "Installing collected packages: thrift\n",
      "Successfully installed thrift-0.13.0\n",
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting sasl==0.2.1\n",
      "  Downloading https://files.pythonhosted.org/packages/8e/2c/45dae93d666aea8492678499e0999269b4e55f1829b1e4de5b8204706ad9/sasl-0.2.1.tar.gz\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sasl==0.2.1) (1.12.0)\n",
      "Building wheels for collected packages: sasl\n",
      "  Building wheel for sasl (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/cdsw/.cache/pip/wheels/56/20/21/ff481fd0f4ae09d5d94c76d089f550204580b1703e44f27dd5\n",
      "Successfully built sasl\n",
      "Installing collected packages: sasl\n",
      "Successfully installed sasl-0.2.1\n",
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting thrift_sasl==0.3.0\n",
      "  Downloading https://files.pythonhosted.org/packages/50/fe/89cbc910809e3757c762f56ee190ca39e0f28b7ea451835232c0c988d706/thrift_sasl-0.3.0.tar.gz\n",
      "Requirement already satisfied: thrift>=0.10.0 in ./.local/lib/python3.6/site-packages (from thrift_sasl==0.3.0) (0.13.0)\n",
      "Requirement already satisfied: sasl>=0.2.1 in ./.local/lib/python3.6/site-packages (from thrift_sasl==0.3.0) (0.2.1)\n",
      "Requirement already satisfied: six>=1.7.2 in /usr/local/lib/python3.6/dist-packages (from thrift>=0.10.0->thrift_sasl==0.3.0) (1.12.0)\n",
      "Building wheels for collected packages: thrift-sasl\n",
      "  Building wheel for thrift-sasl (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/cdsw/.cache/pip/wheels/c8/3a/34/1d82df3d652788fc211c245d51dde857a58e603695ea41d93d\n",
      "Successfully built thrift-sasl\n",
      "Installing collected packages: thrift-sasl\n",
      "Successfully installed thrift-sasl-0.3.0\n",
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting nltk==3.4.5\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5MB 3.7MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk==3.4.5) (1.12.0)\n",
      "Building wheels for collected packages: nltk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/cdsw/.cache/pip/wheels/96/86/f6/68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n",
      "Successfully built nltk\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.4.5\n",
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting torch==1.4.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/19/4804aea17cd136f1705a5e98a00618cb8f6ccc375ad8bfa437408e09d058/torch-1.4.0-cp36-cp36m-manylinux1_x86_64.whl (753.4MB)\n",
      "\u001b[K     |████████████████████████████████| 753.4MB 53kB/s s eta 0:00:01   |▌                               | 10.9MB 4.0MB/s eta 0:03:08     |▊                               | 18.0MB 4.0MB/s eta 0:03:06     |████████████████████▏           | 474.7MB 74.6MB/s eta 0:00:04     |███████████████████████████████ | 728.5MB 76.7MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: torch\n",
      "Successfully installed torch-1.4.0\n",
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install ipython-sql==0.3.9\n",
    "!pip3 install PyHive==0.6.1\n",
    "!pip3 install SQLAlchemy==1.3.13\n",
    "!pip3 install thrift==0.13.0\n",
    "!pip3 install sasl==0.2.1\n",
    "!pip3 install thrift_sasl==0.3.0\n",
    "\n",
    "\n",
    "!pip3 install nltk==3.4.5\n",
    "!pip3 install torch==1.4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記でインストールしたPyHiveは、Pythonコードの中でimportして使われるのではなく、Hiveへの接続の際の接続文字列：`sqlalchemy.create_engine('hive://<host>:<port>')`の中でdialectsとして指定された際に必要になります。そのため、インストール後に利用するためには、新しくプロセスを始める必要があります。**インストールした後に一度、KernelをRestartしてください。**インストールしたプロセスでは、接続時に下記のようなエラーが発生します。\n",
    "`NoSuchModuleError: Can't load plugin: sqlalchemy.dialects:hive`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import subprocess\n",
    "import glob\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "\n",
    "from pyhive import hive\n",
    "import sqlalchemy\n",
    "\n",
    "import sys\n",
    "#from random import random\n",
    "from operator import add\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import torch\n",
    "import nltk\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WEBスクレイピング\n",
    "\n",
    "無償で利用できるAPIを用いて演習を行います。そのため、利用に一定の制限が課せられることにご留意ください。\n",
    "例えば、ご利用状況に応じて、下記のようなエラーメッセージを受け取ることがあります。\n",
    "\n",
    "```\n",
    "{\"response\":{\"status\":429},\"errors\":[{\"message\":\"Rate limit exceeded. Client may not make more than 200 requests an hour.\"}]}\n",
    "```\n",
    "まず、APIで取得したデータをCDSWプロジェクト内のファイルとして保存します。\n",
    "\n",
    "取得する銘柄の候補が、`ticker.txt`に定義されています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2882\n",
      "['A', 'AA', 'AAL', 'AAN', 'AAOI', 'AAON', 'AAP', 'AAPL', 'AAWW', 'AAXN', 'ABBV', 'ABC', 'ABCB', 'ABEO', 'ABG', 'ABM', 'ABMD', 'ABT', 'ABTX', 'ACA', 'ACAD', 'ACCO', 'ACEL', 'ACGL', 'ACHC', 'ACHN', 'ACHV', 'ACIA', 'ACIW', 'ACLS', 'ACM', 'ACN', 'ACNB', 'ACOR', 'ACRS', 'ACRX', 'ACTG', 'ADBE', 'ADES', 'ADI', 'ADM', 'ADMA', 'ADMP', 'ADMS', 'ADP', 'ADPT', 'ADRO', 'ADS', 'ADSK', 'ADSW', 'ADT', 'ADTN', 'ADUS', 'ADVM', 'ADXS', 'AE', 'AEE', 'AEGN', 'AEIS', 'AEL', 'AEM', 'AEMD', 'AEO', 'AEP', 'AERI', 'AES', 'AFG', 'AFI', 'AFL', 'AG', 'AGCO', 'AGEN', 'AGFS', 'AGI', 'AGIO', 'AGLE', 'AGM', 'AGN', 'AGO', 'AGR', 'AGRX', 'AGS', 'AGTC', 'AGX', 'AGYS', 'AHC', 'AHCO', 'AIG', 'AIMC', 'AIMT', 'AIN', 'AIR', 'AIRG', 'AIRT', 'AIT', 'AIZ', 'AJG', 'AJRD', 'AKAM', 'AKBA', 'AKCA', 'AKRO', 'AKRX', 'AKS', 'AL', 'ALB', 'ALCO', 'ALDX', 'ALE', 'ALEC', 'ALG', 'ALGN', 'ALGT', 'ALIM', 'ALK', 'ALKS', 'ALL', 'ALLK', 'ALLO', 'ALLY', 'ALNY', 'ALOT', 'ALPN', 'ALRM', 'ALRN', 'ALSK', 'ALSN', 'ALT', 'ALTR', 'ALV', 'ALXN', 'AM', 'AMAG', 'AMAL', 'AMAT', 'AMBA', 'AMBC', 'AMC', 'AMCX', 'AMD', 'AME', 'AMED', 'AMEH', 'AMG', 'AMGN', 'AMK', 'AMKR', 'AMN', 'AMNB', 'AMOT', 'AMP', 'AMPE', 'AMPH', 'AMRC', 'AMRS', 'AMRX', 'AMSC', 'AMSF', 'AMSWA', 'AMTB', 'AMTD', 'AMTX', 'AMWD', 'AMZN', 'AN', 'ANAB', 'ANAT', 'ANDA', 'ANDE', 'ANET', 'ANF', 'ANGI', 'ANGO', 'ANIK', 'ANIP', 'ANIX', 'ANSS', 'ANTM', 'AOBC', 'AON', 'AOS', 'AP', 'APA', 'APD', 'APDN', 'APEI', 'APEN', 'APH', 'APLS', 'APLT', 'APOG', 'APPF', 'APPN', 'APPS', 'APRE', 'APRN', 'APT', 'APTV', 'APY', 'AQB', 'AQN', 'AQUA', 'AR', 'ARA', 'ARAV', 'ARAY', 'ARCB', 'ARCH', 'ARCO', 'ARCT', 'ARDS', 'ARDX', 'ARES', 'ARGO', 'ARKR', 'ARLO', 'ARMK', 'ARMP', 'ARNA', 'ARNC', 'AROC', 'AROW', 'ARTNA', 'ARVN', 'ARW', 'ARWR', 'ASB', 'ASFI', 'ASGN', 'ASH', 'ASIX', 'ASMB', 'ASNA', 'ASPS', 'ASPU', 'ASRT', 'ASTE', 'ASYS', 'ATEC', 'ATEN', 'ATEX', 'ATGE', 'ATH', 'ATHX', 'ATI', 'ATKR', 'ATLO', 'ATNI', 'ATNX', 'ATO', 'ATR', 'ATRA', 'ATRC', 'ATRI', 'ATRO', 'ATRS', 'ATSG', 'ATUS', 'ATVI', 'AUB', 'AUMN', 'AUPH', 'AUTO', 'AUY', 'AVA', 'AVAV', 'AVD', 'AVGO', 'AVID', 'AVLR', 'AVNS', 'AVNW', 'AVRO', 'AVT', 'AVTR', 'AVX', 'AVXL', 'AVY', 'AVYA', 'AWI', 'AWK', 'AWR', 'AWRE', 'AX', 'AXAS', 'AXDX', 'AXGN', 'AXGT', 'AXL', 'AXLA', 'AXNX', 'AXP', 'AXS', 'AXSM', 'AXTA', 'AXTI', 'AYI', 'AYX', 'AZO', 'AZPN', 'AZZ', 'B', 'BA', 'BAC', 'BAH', 'BAM', 'BANC', 'BAND', 'BANF', 'BANR', 'BAP', 'BAX', 'BB', 'BBBY', 'BBI', 'BBIO', 'BBQ', 'BBW', 'BBY', 'BC', 'BCBP', 'BCC', 'BCE', 'BCEI', 'BCEL', 'BCLI', 'BCO', 'BCOR', 'BCOV', 'BCPC', 'BCRX', 'BDC', 'BDGE', 'BDSI', 'BDX', 'BE', 'BEAT', 'BECN', 'BELFB', 'BEN', 'BERY', 'BFAM', 'BFC', 'BFIN', 'BG', 'BGCP', 'BGFV', 'BGG', 'BGS', 'BH', 'BHB', 'BHC', 'BHE', 'BHF', 'BHLB', 'BIG', 'BIIB', 'BIMI', 'BIO', 'BIOS', 'BJ', 'BJRI', 'BK', 'BKD', 'BKE', 'BKH', 'BKI', 'BKNG', 'BKR', 'BKU', 'BL', 'BLBD', 'BLCM', 'BLD', 'BLDP', 'BLDR', 'BLFS', 'BLK', 'BLKB', 'BLL', 'BLMN', 'BLNK', 'BLUE', 'BLX', 'BMCH', 'BMI', 'BMO', 'BMRC', 'BMRN', 'BMTC', 'BMY', 'BNED', 'BNFT', 'BNGO', 'BNS', 'BOH', 'BOKF', 'BOMN', 'BOOM', 'BOOT', 'BOX', 'BPFH', 'BPMC', 'BPTH', 'BR', 'BRC', 'BRKB', 'BRKL', 'BRKR', 'BRKS', 'BRMK', 'BRO', 'BRY', 'BSET', 'BSIG', 'BSQR', 'BSRR', 'BSTC', 'BSX', 'BTG', 'BURL', 'BUSE', 'BV', 'BWA', 'BWEN', 'BWXT', 'BX', 'BXC', 'BXG', 'BXS', 'BY', 'BYD', 'BYND', 'BZH', 'C', 'CABO', 'CAC', 'CACC', 'CACI', 'CADE', 'CAG', 'CAH', 'CAI', 'CAKE', 'CAL', 'CALA', 'CALM', 'CALX', 'CAMP', 'CAPR', 'CAR', 'CARA', 'CARE', 'CARG', 'CARS', 'CASA', 'CASH', 'CASI', 'CASS', 'CASY', 'CAT', 'CATB', 'CATM', 'CATO', 'CATY', 'CBIO', 'CBMG', 'CBPO', 'CBPX', 'CBRE', 'CBRL', 'CBSH', 'CBT', 'CBTX', 'CBU', 'CBZ', 'CC', 'CCBG', 'CCEP', 'CCF', 'CCJ', 'CCK', 'CCL', 'CCMP', 'CCNE', 'CCO', 'CCOI', 'CCRN', 'CCS', 'CCXI', 'CDAY', 'CDE', 'CDK', 'CDLX', 'CDMO', 'CDNA', 'CDNS', 'CDW', 'CDXS', 'CE', 'CECE', 'CEIX', 'CEL', 'CELH', 'CEMI', 'CENT', 'CENTA', 'CENX', 'CERN', 'CERS', 'CEVA', 'CF', 'CFB', 'CFFI', 'CFFN', 'CFG', 'CFMS', 'CFR', 'CFX', 'CGC', 'CGEN', 'CGNX', 'CHCO', 'CHD', 'CHDN', 'CHE', 'CHEF', 'CHGG', 'CHH', 'CHK', 'CHKP', 'CHMA', 'CHMG', 'CHNG', 'CHRS', 'CHRW', 'CHS', 'CHTR', 'CHUY', 'CHWY', 'CI', 'CIEN', 'CINF', 'CIR', 'CIT', 'CKH', 'CKPT', 'CL', 'CLAR', 'CLBK', 'CLBS', 'CLCT', 'CLDR', 'CLDX', 'CLF', 'CLFD', 'CLGX', 'CLH', 'CLNE', 'CLPS', 'CLR', 'CLSD', 'CLSN', 'CLVS', 'CLW', 'CLX', 'CLXT', 'CM', 'CMA', 'CMBM', 'CMC', 'CMCO', 'CMCSA', 'CMD', 'CME', 'CMG', 'CMI', 'CMLS', 'CMP', 'CMS', 'CMTL', 'CNA', 'CNBKA', 'CNC', 'CNCE', 'CNDT', 'CNI', 'CNK', 'CNMD', 'CNNE', 'CNO', 'CNOB', 'CNP', 'CNQ', 'CNR', 'CNS', 'CNSL', 'CNST', 'CNX', 'CNXN', 'CO', 'COF', 'COG', 'COHR', 'COHU', 'COKE', 'COLB', 'COLL', 'COLM', 'COMM', 'CONN', 'COO', 'COOP', 'COP', 'CORE', 'CORT', 'COST', 'COT', 'COTY', 'COUP', 'CP', 'CPA', 'CPB', 'CPE', 'CPF', 'CPG', 'CPIX', 'CPK', 'CPRI', 'CPRT', 'CPRX', 'CPS', 'CPSI', 'CPST', 'CR', 'CRAI', 'CRBP', 'CRC', 'CRCM', 'CREE', 'CRI', 'CRIS', 'CRK', 'CRL', 'CRM', 'CRMD', 'CRMT', 'CRNC', 'CRNT', 'CRNX', 'CRON', 'CROX', 'CRS', 'CRTX', 'CRUS', 'CRVL', 'CRVS', 'CRWD', 'CRWS', 'CRY', 'CSBR', 'CSCO', 'CSFL', 'CSGP', 'CSGS', 'CSII', 'CSIQ', 'CSL', 'CSOD', 'CSPI', 'CSS', 'CSSE', 'CSTL', 'CSU', 'CSV', 'CSWI', 'CSX', 'CTAS', 'CTB', 'CTBI', 'CTG', 'CTIC', 'CTL', 'CTLT', 'CTMX', 'CTO', 'CTRA', 'CTRN', 'CTS', 'CTSH', 'CTSO', 'CTVA', 'CTXS', 'CUB', 'CUBI', 'CULP', 'CURO', 'CUTR', 'CVA', 'CVBF', 'CVCO', 'CVE', 'CVET', 'CVGW', 'CVI', 'CVLT', 'CVLY', 'CVM', 'CVNA', 'CVS', 'CVTI', 'CVU', 'CVX', 'CW', 'CWBC', 'CWCO', 'CWEN', 'CWH', 'CWK', 'CWST', 'CWT', 'CXDC', 'CXO', 'CY', 'CYAN', 'CYBE', 'CYBR', 'CYCN', 'CYD', 'CYH', 'CYRX', 'CYTK', 'CZNC', 'CZR', 'CZWI', 'CZZ', 'D', 'DAIO', 'DAKT', 'DAL', 'DAN', 'DAR', 'DARE', 'DB', 'DBD', 'DBI', 'DBX', 'DCI', 'DCO', 'DCOM', 'DCPH', 'DD', 'DDD', 'DDOG', 'DDS', 'DE', 'DECK', 'DELL', 'DENN', 'DERM', 'DFIN', 'DFS', 'DG', 'DGICA', 'DGII', 'DGLY', 'DGX', 'DHI', 'DHIL', 'DHR', 'DHT', 'DIN', 'DIOD', 'DIS', 'DISCA', 'DISCK', 'DISH', 'DJCO', 'DK', 'DKS', 'DLA', 'DLB', 'DLTR', 'DLX', 'DMRC', 'DNKN', 'DNLI', 'DNOW', 'DNR', 'DO', 'DOCU', 'DOMO', 'DOOR', 'DORM', 'DOV', 'DOW', 'DPLO', 'DPZ', 'DRAD', 'DRI', 'DRNA', 'DRQ', 'DSGX', 'DSKE', 'DSPG', 'DT', 'DTE', 'DTIL', 'DUK', 'DVA', 'DVAX', 'DVD', 'DVN', 'DXC', 'DXCM', 'DXPE', 'DXR', 'DY', 'DZSI', 'EA', 'EAF', 'EAT', 'EB', 'EBAY', 'EBF', 'EBIX', 'EBS', 'EBSB', 'EBTC', 'ECHO', 'ECL', 'ECOL', 'ECOM', 'ECPG', 'ED', 'EDIT', 'EDSA', 'EDTX', 'EDUC', 'EE', 'EEFT', 'EEX', 'EFC', 'EFSC', 'EFX', 'EGAN', 'EGBN', 'EGHT', 'EGO', 'EGOV', 'EGRX', 'EGY', 'EHC', 'EHTH', 'EIDX', 'EIG', 'EIGI', 'EIGR', 'EIX', 'EKSO', 'EL', 'ELAN', 'ELF', 'ELGX', 'ELY', 'EMCF', 'EME', 'EMKR', 'EML', 'EMMS', 'EMN', 'EMR', 'ENB', 'ENDP', 'ENLV', 'ENPH', 'ENR', 'ENS', 'ENSG', 'ENTA', 'ENTG', 'ENV', 'ENVA', 'EOG', 'EOLS', 'EPAC', 'EPAM', 'EPAY', 'EPC', 'EPIX', 'EPM', 'EPZM', 'EQH', 'EQT', 'ERA', 'ERF', 'ERI', 'ERIE', 'ERII', 'ES', 'ESCA', 'ESE', 'ESGR', 'ESLT', 'ESNT', 'ESPR', 'ESSA', 'ETFC', 'ETH', 'ETM', 'ETN', 'ETNB', 'ETR', 'ETSY', 'EV', 'EVBG', 'EVC', 'EVER', 'EVFM', 'EVH', 'EVOP', 'EVR', 'EVRG', 'EVRI', 'EW', 'EWBC', 'EXAS', 'EXC', 'EXEL', 'EXK', 'EXLS', 'EXP', 'EXPD', 'EXPE', 'EXPI', 'EXPO', 'EXPR', 'EXTR', 'EYE', 'EYPT', 'EZPW', 'F', 'FAF', 'FANG', 'FARM', 'FARO', 'FAST', 'FAT', 'FATE', 'FB', 'FBC', 'FBHS', 'FBIZ', 'FBK', 'FBM', 'FBMS', 'FBNC', 'FC', 'FCBC', 'FCEL', 'FCF', 'FCFS', 'FCN', 'FCNCA', 'FCX', 'FDEF', 'FDP', 'FDS', 'FDX', 'FE', 'FEIM', 'FELE', 'FEYE', 'FF', 'FFBC', 'FFG', 'FFIC', 'FFIN', 'FFIV', 'FFWM', 'FG', 'FGEN', 'FHB', 'FHN', 'FIBK', 'FICO', 'FII', 'FIS', 'FISI', 'FISV', 'FIT', 'FITB', 'FIVE', 'FIVN', 'FIX', 'FIXX', 'FIZZ', 'FL', 'FLDM', 'FLEX', 'FLGT', 'FLIC', 'FLIR', 'FLMN', 'FLNT', 'FLO', 'FLOW', 'FLR', 'FLS', 'FLT', 'FLWS', 'FLXN', 'FLXS', 'FMBH', 'FMBI', 'FMC', 'FMNB', 'FN', 'FNB', 'FND', 'FNF', 'FNHC', 'FNJN', 'FNKO', 'FNLC', 'FNV', 'FOCS', 'FOE', 'FOLD', 'FOMX', 'FONR', 'FOR', 'FORM', 'FORR', 'FOSL', 'FOX', 'FOXA', 'FOXF', 'FPRX', 'FRAN', 'FRC', 'FREQ', 'FRGI', 'FRHC', 'FRME', 'FRO', 'FRPH', 'FRPT', 'FRTA', 'FSB', 'FSBW', 'FSCT', 'FSFG', 'FSI', 'FSLR', 'FSLY', 'FSM', 'FSS', 'FSTR', 'FTCH', 'FTDR', 'FTEK', 'FTI', 'FTNT', 'FTR', 'FTS', 'FTSV', 'FTV', 'FUL', 'FULC', 'FULT', 'FVE', 'FVRR', 'FWONA', 'FWRD', 'GABC', 'GALT', 'GATX', 'GBCI', 'GBL', 'GBLI', 'GBT', 'GBX', 'GCAP', 'GCBC', 'GCI', 'GCO', 'GCP', 'GD', 'GDDY', 'GDEN', 'GDI', 'GDOT', 'GE', 'GEC', 'GEF', 'GEOS', 'GERN', 'GES', 'GEVO', 'GFF', 'GGG', 'GH', 'GHC', 'GHL', 'GHM', 'GIB', 'GIFI', 'GIII', 'GIL', 'GILD', 'GIS', 'GKOS', 'GL', 'GLDD', 'GLMD', 'GLNG', 'GLRE', 'GLT', 'GLUU', 'GLW', 'GLYC', 'GM', 'GME', 'GMED', 'GMS', 'GNC', 'GNCA', 'GNE', 'GNMK', 'GNMX', 'GNRC', 'GNTX', 'GNW', 'GO', 'GOGO', 'GOLD', 'GOLF', 'GOOG', 'GOOGL', 'GOOS', 'GORO', 'GOSS', 'GPC', 'GPI', 'GPK', 'GPN', 'GPOR', 'GPRE', 'GPRK', 'GPRO', 'GPS', 'GPX', 'GRA', 'GRBK', 'GRC', 'GRIF', 'GRPN', 'GRTS', 'GRUB', 'GS', 'GSB', 'GSBC', 'GSHD', 'GSKY', 'GT', 'GTES', 'GTHX', 'GTLS', 'GTN', 'GTT', 'GTX', 'GVA', 'GWB', 'GWRE', 'GWRS', 'GWW', 'H', 'HA', 'HABT', 'HAE', 'HAFC', 'HAIN', 'HAL', 'HALO', 'HARP', 'HAS', 'HAYN', 'HBAN', 'HBB', 'HBCP', 'HBI', 'HBIO', 'HBM', 'HBNC', 'HBT', 'HCA', 'HCAT', 'HCC', 'HCCI', 'HCI', 'HCKT', 'HCSG', 'HD', 'HDS', 'HDSN', 'HE', 'HEAR', 'HEES', 'HEI', 'HELE', 'HEPA', 'HES', 'HFBL', 'HFC', 'HFFG', 'HFWA', 'HGV', 'HHC', 'HHS', 'HI', 'HIBB', 'HIFS', 'HIG', 'HII', 'HIIQ', 'HJLI', 'HL', 'HLF', 'HLI', 'HLIO', 'HLIT', 'HLT', 'HLX', 'HMHC', 'HMN', 'HMST', 'HMSY', 'HMTV', 'HNGR', 'HNI', 'HOFT', 'HOG', 'HOLX', 'HOMB', 'HOME', 'HON', 'HONE', 'HOPE', 'HOV', 'HP', 'HPE', 'HPQ', 'HQY', 'HRB', 'HRC', 'HRI', 'HRL', 'HROW', 'HRTG', 'HRTX', 'HSC', 'HSIC', 'HSII', 'HSKA', 'HSTM', 'HSY', 'HTBI', 'HTBK', 'HTGM', 'HTH', 'HTLD', 'HTLF', 'HUBB', 'HUBG', 'HUBS', 'HUD', 'HUM', 'HUN', 'HURC', 'HURN', 'HVT', 'HWC', 'HWCC', 'HWKN', 'HXL', 'HY', 'HYRE', 'HZO', 'IAA', 'IAC', 'IAG', 'IART', 'IBCP', 'IBKC', 'IBKR', 'IBM', 'IBOC', 'IBP', 'IBTX', 'ICBK', 'ICE', 'ICFI', 'ICHR', 'ICON', 'ICPT', 'ICUI', 'IDA', 'IDCC', 'IDRA', 'IDT', 'IDXX', 'IESC', 'IEX', 'IFF', 'IGMS', 'IGT', 'IHC', 'IIIN', 'IIIV', 'IIVI', 'ILMN', 'IMAX', 'IMGN', 'IMH', 'IMKTA', 'IMMR', 'IMMU', 'IMUX', 'IMXI', 'INAP', 'INBK', 'INCY', 'INDB', 'INFN', 'INFO', 'INGN', 'INGR', 'INMD', 'INO', 'INOD', 'INOV', 'INS', 'INSG', 'INSM', 'INSP', 'INST', 'INT', 'INTC', 'INTL', 'INTU', 'INVA', 'INVE', 'IO', 'IONS', 'IOSP', 'IOTS', 'IOVA', 'IP', 'IPAR', 'IPG', 'IPGP', 'IPHI', 'IPHS', 'IPI', 'IPWR', 'IQV', 'IR', 'IRBT', 'IRDM', 'IRMD', 'IRTC', 'IRWD', 'ISBC', 'ISEE', 'ISNS', 'ISRG', 'IT', 'ITCI', 'ITGR', 'ITIC', 'ITRI', 'ITT', 'ITW', 'IVC', 'IVZ', 'J', 'JACK', 'JAZZ', 'JBHT', 'JBL', 'JBLU', 'JBSS', 'JBT', 'JCI', 'JCOM', 'JCP', 'JEF', 'JELD', 'JILL', 'JJSF', 'JKHY', 'JLL', 'JNCE', 'JNJ', 'JNPR', 'JOE', 'JOUT', 'JPM', 'JRVR', 'JVA', 'JWN', 'JYNT', 'K', 'KAI', 'KALA', 'KALU', 'KALV', 'KAMN', 'KAR', 'KBH', 'KBR', 'KDMN', 'KDP', 'KE', 'KELYA', 'KEM', 'KEX', 'KEY', 'KEYS', 'KFRC', 'KFS', 'KFY', 'KGC', 'KHC', 'KIDS', 'KIRK', 'KL', 'KLAC', 'KLIC', 'KMB', 'KMI', 'KMPR', 'KMT', 'KMX', 'KN', 'KNDI', 'KNL', 'KNSA', 'KNSL', 'KNX', 'KO', 'KOD', 'KODK', 'KOP', 'KOPN', 'KOS', 'KPTI', 'KR', 'KRA', 'KRNT', 'KRNY', 'KRO', 'KRTX', 'KRUS', 'KRYS', 'KSS', 'KSU', 'KTB', 'KTCC', 'KTOS', 'KURA', 'KVHI', 'KW', 'KWR', 'L', 'LAD', 'LAKE', 'LANC', 'LASR', 'LAUR', 'LAWS', 'LB', 'LBAI', 'LBC', 'LBRDA', 'LBRDK', 'LBRT', 'LBTYA', 'LBTYK', 'LBY', 'LC', 'LCI', 'LCII', 'LCNB', 'LCUT', 'LDL', 'LDOS', 'LE', 'LEA', 'LEAF', 'LECO', 'LEG', 'LEGH', 'LEN', 'LEVI', 'LFUS', 'LFVN', 'LGIH', 'LGND', 'LH', 'LHCG', 'LHX', 'LII', 'LILA', 'LILAK', 'LIN', 'LINC', 'LIND', 'LITE', 'LIVE', 'LIVN', 'LJPC', 'LKFN', 'LKQ', 'LL', 'LLNW', 'LLY', 'LM', 'LMAT', 'LMNR', 'LMNX', 'LMT', 'LNC', 'LNDC', 'LNG', 'LNN', 'LNT', 'LNTH', 'LOB', 'LOCO', 'LOGC', 'LOGM', 'LOOP', 'LOPE', 'LORL', 'LOVE', 'LOW', 'LPCN', 'LPI', 'LPLA', 'LPSN', 'LPX', 'LQDA', 'LQDT', 'LRCX', 'LRN', 'LSCC', 'LSTR', 'LTHM', 'LTRPA', 'LTS', 'LULU', 'LUNA', 'LUV', 'LVGO', 'LVS', 'LW', 'LWAY', 'LXRX', 'LXU', 'LYFT', 'LYTS', 'LYV', 'LZB', 'M', 'MA', 'MACK', 'MAGS', 'MAN', 'MANH', 'MANT', 'MANU', 'MAR', 'MARA', 'MAS', 'MASI', 'MAT', 'MATW', 'MATX', 'MAXR', 'MBI', 'MBIN', 'MBIO', 'MBOT', 'MBUU', 'MBWM', 'MC', 'MCD', 'MCF', 'MCHP', 'MCHX', 'MCK', 'MCO', 'MCRB', 'MCRI', 'MCS', 'MCY', 'MD', 'MDB', 'MDC', 'MDGL', 'MDLA', 'MDLZ', 'MDP', 'MDRX', 'MDT', 'MDU', 'MDWD', 'MEC', 'MED', 'MEDP', 'MEET', 'MEI', 'MELI', 'MEOH', 'MERC', 'MESA', 'MET', 'MFC', 'MFIN', 'MFSF', 'MG', 'MGA', 'MGEE', 'MGI', 'MGIC', 'MGLN', 'MGM', 'MGNX', 'MGPI', 'MGRC', 'MGTA', 'MGTX', 'MGY', 'MHK', 'MHO', 'MIC', 'MIDD', 'MIK', 'MIME', 'MINI', 'MIRM', 'MIST', 'MITK', 'MKC', 'MKL', 'MKSI', 'MKTX', 'MLAB', 'MLHR', 'MLI', 'MLM', 'MLND', 'MLNT', 'MLNX', 'MLR', 'MMC', 'MMM', 'MMS', 'MMSI', 'MMYT', 'MNI', 'MNK', 'MNKD', 'MNLO', 'MNOV', 'MNRO', 'MNST', 'MNTA', 'MO', 'MOBL', 'MOD', 'MODN', 'MOFG', 'MOH', 'MORF', 'MORN', 'MOS', 'MOV', 'MPAA', 'MPC', 'MPWR', 'MPX', 'MRAM', 'MRC', 'MRCY', 'MRIN', 'MRK', 'MRKR', 'MRLN', 'MRNA', 'MRNS', 'MRO', 'MRTN', 'MRTX', 'MRVL', 'MS', 'MSA', 'MSBI', 'MSCI', 'MSEX', 'MSFT', 'MSG', 'MSGN', 'MSI', 'MSM', 'MSON', 'MSTR', 'MTB', 'MTBC', 'MTCH', 'MTD', 'MTDR', 'MTEM', 'MTEX', 'MTG', 'MTH', 'MTN', 'MTOR', 'MTRN', 'MTRX', 'MTSC', 'MTSI', 'MTW', 'MTX', 'MTZ', 'MU', 'MUR', 'MUSA', 'MUX', 'MVIS', 'MWA', 'MXIM', 'MXL', 'MYE', 'MYGN', 'MYL', 'MYOK', 'MYOV', 'MYRG', 'NAII', 'NAT', 'NATH', 'NATI', 'NATR', 'NAV', 'NAVI', 'NBEV', 'NBHC', 'NBIX', 'NBL', 'NBR', 'NBSE', 'NBTB', 'NC', 'NCBS', 'NCLH', 'NCMI', 'NCR', 'NDAQ', 'NDLS', 'NDSN', 'NEE', 'NEM', 'NEO', 'NEOG', 'NEON', 'NEP', 'NERV', 'NET', 'NETE', 'NEU', 'NEWR', 'NEXT', 'NFBK', 'NFE', 'NFG', 'NFLX', 'NG', 'NGHC', 'NGM', 'NGS', 'NGVC', 'NGVT', 'NHC', 'NHTC', 'NI', 'NJR', 'NK', 'NKE', 'NKSH', 'NKTR', 'NL', 'NLNK', 'NLOK', 'NLSN', 'NLTX', 'NMIH', 'NMRK', 'NNBR', 'NNI', 'NOC', 'NOV', 'NOVA', 'NOVN', 'NOVT', 'NOW', 'NP', 'NPK', 'NPO', 'NPTN', 'NR', 'NRC', 'NRG', 'NRIM', 'NSC', 'NSIT', 'NSP', 'NSSC', 'NSTG', 'NTAP', 'NTB', 'NTCT', 'NTGR', 'NTLA', 'NTNX', 'NTR', 'NTRA', 'NTRS', 'NTUS', 'NTWK', 'NUAN', 'NUE', 'NUS', 'NUVA', 'NVAX', 'NVCN', 'NVCR', 'NVDA', 'NVEC', 'NVEE', 'NVMI', 'NVR', 'NVRO', 'NVST', 'NVTA', 'NWBI', 'NWE', 'NWFL', 'NWHM', 'NWL', 'NWLI', 'NWN', 'NWPX', 'NWS', 'NWSA', 'NX', 'NXGN', 'NXPI', 'NXST', 'NXTC', 'NYCB', 'NYT', 'OAS', 'OBCI', 'OBNK', 'OC', 'OCFC', 'OCGN', 'OCN', 'OCUL', 'ODC', 'ODFL', 'ODP', 'ODT', 'OFIX', 'OFLX', 'OGE', 'OGS', 'OI', 'OII', 'OIS', 'OKE', 'OKTA', 'OLED', 'OLLI', 'OLN', 'OMC', 'OMCL', 'OMER', 'OMEX', 'OMF', 'OMI', 'ON', 'ONB', 'ONCS', 'ONCT', 'ONDK', 'ONTO', 'ONTX', 'OOMA', 'OPB', 'OPES', 'OPGN', 'OPK', 'OPRT', 'OPTN', 'OPTT', 'OPY', 'ORA', 'ORBC', 'ORCC', 'ORCL', 'ORI', 'ORLY', 'ORMP', 'ORRF', 'OSIS', 'OSK', 'OSPN', 'OSTK', 'OSUR', 'OTEX', 'OTIC', 'OTTR', 'OVV', 'OXM', 'OXY', 'OZK', 'P', 'PAAS', 'PACB', 'PACQ', 'PACW', 'PAG', 'PAGP', 'PAGS', 'PAH', 'PAHC', 'PANW', 'PAR', 'PARR', 'PATK', 'PAYC', 'PAYS', 'PAYX', 'PB', 'PBCT', 'PBF', 'PBH', 'PBI', 'PBPB', 'PBYI', 'PCAR', 'PCG', 'PCOM', 'PCRX', 'PCTI', 'PCTY', 'PCYG', 'PD', 'PDCE', 'PDCO', 'PDFS', 'PDLI', 'PE', 'PEBO', 'PEG', 'PEGA', 'PEGI', 'PEIX', 'PEN', 'PENN', 'PEP', 'PERI', 'PETQ', 'PETS', 'PFBC', 'PFBI', 'PFE', 'PFG', 'PFGC', 'PFIS', 'PFNX', 'PFPT', 'PFS', 'PFSI', 'PFSW', 'PG', 'PGC', 'PGNX', 'PGNY', 'PGR', 'PGTI', 'PH', 'PHAS', 'PHAT', 'PHM', 'PHR', 'PHX', 'PI', 'PICO', 'PII', 'PINC', 'PING', 'PINS', 'PIR', 'PIRS', 'PJT', 'PKE', 'PKG', 'PKI', 'PKOH', 'PLAB', 'PLAN', 'PLAY', 'PLCE', 'PLIN', 'PLMR', 'PLNT', 'PLOW', 'PLPC', 'PLSE', 'PLT', 'PLUG', 'PLUS', 'PLXP', 'PLXS', 'PLYA', 'PM', 'PMD', 'PME', 'PNC', 'PNFP', 'PNM', 'PNR', 'PNRG', 'PNTG', 'PNW', 'PODD', 'POL', 'POOL', 'POR', 'POST', 'POWI', 'POWL', 'PPBI', 'PPC', 'PPG', 'PPIH', 'PPL', 'PPSI', 'PQG', 'PRA', 'PRAA', 'PRAH', 'PRCP', 'PRFT', 'PRGO', 'PRGS', 'PRI', 'PRIM', 'PRK', 'PRLB', 'PRMW', 'PRNB', 'PRO', 'PROS', 'PROV', 'PRPL', 'PRSC', 'PRSP', 'PRTA', 'PRTK', 'PRTY', 'PRU', 'PRVB', 'PRVL', 'PS', 'PSMT', 'PSN', 'PSNL', 'PSTG', 'PSTI', 'PSTV', 'PSX', 'PTC', 'PTCT', 'PTEN', 'PTGX', 'PTI', 'PTLA', 'PTON', 'PTSI', 'PTVCB', 'PUB', 'PUMP', 'PVG', 'PVH', 'PWOD', 'PWR', 'PXD', 'PXLW', 'PYPL', 'PZN', 'PZZA', 'QADA', 'QCOM', 'QCRH', 'QDEL', 'QEP', 'QLYS', 'QNST', 'QRTEA', 'QRVO', 'QSR', 'QTRX', 'QTWO', 'QUAD', 'QUIK', 'QUMU', 'QUOT', 'R', 'RACE', 'RAD', 'RADA', 'RAIL', 'RAMP', 'RAPT', 'RARE', 'RAVE', 'RAVN', 'RBA', 'RBBN', 'RBC', 'RBCAA', 'RBCN', 'RCI', 'RCII', 'RCKT', 'RCKY', 'RCL', 'RCM', 'RCUS', 'RDFN', 'RDI', 'RDN', 'RDNT', 'RDUS', 'RDWR', 'RE', 'REAL', 'RECN', 'REGI', 'REGN', 'RELL', 'REPH', 'REPL', 'RES', 'RESN', 'RETA', 'REV', 'REVG', 'REX', 'REZI', 'RF', 'RFIL', 'RFL', 'RFP', 'RGA', 'RGEN', 'RGLD', 'RGNX', 'RGR', 'RGS', 'RH', 'RHI', 'RICK', 'RILY', 'RIOT', 'RJF', 'RL', 'RLGY', 'RLH', 'RLI', 'RM', 'RMAX', 'RMBS', 'RMCF', 'RMD', 'RMNI', 'RMR', 'RMTI', 'RNET', 'RNG', 'RNR', 'RNST', 'RNWK', 'ROCK', 'ROG', 'ROK', 'ROKU', 'ROL', 'ROLL', 'ROP', 'ROST', 'RP', 'RPAY', 'RPD', 'RPM', 'RRC', 'RRD', 'RRGB', 'RRR', 'RRTS', 'RS', 'RSG', 'RST', 'RTIX', 'RTN', 'RTRX', 'RUBI', 'RUBY', 'RUN', 'RUSHA', 'RUTH', 'RVLV', 'RVNC', 'RWLK', 'RXN', 'RY', 'RYI', 'RYTM', 'S', 'SA', 'SABR', 'SAFM', 'SAFT', 'SAGE', 'SAH', 'SAIA', 'SAIC', 'SAIL', 'SAM', 'SANM', 'SANW', 'SASR', 'SATS', 'SAVE', 'SBCF', 'SBGI', 'SBH', 'SBNY', 'SBPH', 'SBSI', 'SBT', 'SBUX', 'SC', 'SCCO', 'SCHL', 'SCHN', 'SCHW', 'SCI', 'SCKT', 'SCL', 'SCOR', 'SCPL', 'SCS', 'SCSC', 'SCU', 'SCVL', 'SCWX', 'SCX', 'SDC', 'SDRL', 'SEAS', 'SEB', 'SEDG', 'SEE', 'SEIC', 'SEM', 'SENEA', 'SERV', 'SF', 'SFBS', 'SFE', 'SFIX', 'SFM', 'SFNC', 'SG', 'SGA', 'SGBX', 'SGC', 'SGEN', 'SGH', 'SGMO', 'SGMS', 'SGRY', 'SGU', 'SHAK', 'SHEN', 'SHLD', 'SHLO', 'SHOO', 'SHOP', 'SHW', 'SIBN', 'SIEN', 'SIF', 'SIG', 'SIGA', 'SIGI', 'SILK', 'SINA', 'SINT', 'SIRI', 'SITE', 'SIVB', 'SIX', 'SJI', 'SJM', 'SJR', 'SJW', 'SKX', 'SKY', 'SKYW', 'SLAB', 'SLB', 'SLCA', 'SLCT', 'SLDB', 'SLF', 'SLGG', 'SLGN', 'SLM', 'SLP', 'SLRX', 'SM', 'SMAR', 'SMBC', 'SMED', 'SMG', 'SMMF', 'SMP', 'SMPL', 'SMTC', 'SMTX', 'SNA', 'SNAP', 'SNBR', 'SNCR', 'SND', 'SNDX', 'SNOA', 'SNPS', 'SNSS', 'SNV', 'SNX', 'SO', 'SOI', 'SON', 'SONM', 'SONO', 'SORL', 'SP', 'SPAR', 'SPB', 'SPCE', 'SPFI', 'SPGI', 'SPKE', 'SPLK', 'SPN', 'SPNE', 'SPNS', 'SPOK', 'SPOT', 'SPPI', 'SPR', 'SPSC', 'SPTN', 'SPWR', 'SPXC', 'SQ', 'SR', 'SRAX', 'SRCE', 'SRCL', 'SRDX', 'SRE', 'SRI', 'SRL', 'SRPT', 'SRRK', 'SSB', 'SSD', 'SSI', 'SSNC', 'SSP', 'SSRM', 'SSTI', 'SSTK', 'SSYS', 'ST', 'STAA', 'STBA', 'STC', 'STE', 'STFC', 'STIM', 'STL', 'STLD', 'STMP', 'STNE', 'STOK', 'STRA', 'STRL', 'STRO', 'STRS', 'STRT', 'STSA', 'STT', 'STX', 'STZ', 'SU', 'SUM', 'SUP', 'SUPN', 'SVMK', 'SWAV', 'SWCH', 'SWIR', 'SWK', 'SWKS', 'SWM', 'SWN', 'SWTX', 'SWX', 'SXC', 'SXI', 'SXT', 'SYBT', 'SYBX', 'SYF', 'SYK', 'SYKE', 'SYNA', 'SYNC', 'SYNH', 'SYRS', 'SYX', 'SYY', 'T', 'TA', 'TACO', 'TACT', 'TALO', 'TAP', 'TARA', 'TARO', 'TAST', 'TBBK', 'TBI', 'TBIO', 'TBK', 'TBNK', 'TBPH', 'TCBI', 'TCBK', 'TCDA', 'TCMD', 'TCON', 'TCRR', 'TCS', 'TCX', 'TD', 'TDC', 'TDG', 'TDOC', 'TDS', 'TDW', 'TDY', 'TEAM', 'TECD', 'TECH', 'TECK', 'TEL', 'TELL', 'TEN', 'TENB', 'TER', 'TERP', 'TESS', 'TEX', 'TFC', 'TFSL', 'TFX', 'TG', 'TGE', 'TGEN', 'TGH', 'TGI', 'TGLS', 'TGNA', 'TGP', 'TGT', 'TGTX', 'THC', 'THFF', 'THG', 'THMO', 'THO', 'THR', 'THRM', 'THS', 'TIF', 'TILE', 'TISI', 'TIVO', 'TJX', 'TKKS', 'TKR', 'TLRA', 'TLRD', 'TLRY', 'TLYS', 'TMHC', 'TMO', 'TMP', 'TMST', 'TMUS', 'TNAV', 'TNC', 'TNDM', 'TNET', 'TNP', 'TOL', 'TORC', 'TOWN', 'TPC', 'TPCO', 'TPH', 'TPIC', 'TPR', 'TPRE', 'TPTX', 'TPX', 'TR', 'TRC', 'TREE', 'TREX', 'TRGP', 'TRHC', 'TRI', 'TRIP', 'TRMB', 'TRMK', 'TRN', 'TROW', 'TROX', 'TRP', 'TRQ', 'TRS', 'TRST', 'TRT', 'TRTN', 'TRU', 'TRUE', 'TRUP', 'TRV', 'TRWH', 'TRXC', 'TSC', 'TSCO', 'TSEM', 'TSG', 'TSLA', 'TSN', 'TSQ', 'TSRI', 'TTC', 'TTD', 'TTEC', 'TTEK', 'TTGT', 'TTMI', 'TTOO', 'TTPH', 'TTWO', 'TUFN', 'TUP', 'TUSK', 'TVTY', 'TW', 'TWIN', 'TWLO', 'TWNK', 'TWOU', 'TWST', 'TWTR', 'TXG', 'TXMD', 'TXN', 'TXRH', 'TXT', 'TYL', 'TZOO', 'UAA', 'UAL', 'UBER', 'UBSI', 'UBX', 'UCBI', 'UCTT', 'UEIC', 'UFCS', 'UFI', 'UFPI', 'UFPT', 'UFS', 'UG', 'UGI', 'UHAL', 'UHS', 'UI', 'UIHC', 'UIS', 'ULBI', 'ULH', 'ULTA', 'UMBF', 'UMPQ', 'UNB', 'UNF', 'UNFI', 'UNH', 'UNM', 'UNP', 'UNT', 'UNVR', 'UPLD', 'UPS', 'UPWK', 'URBN', 'URGN', 'URI', 'UROV', 'USAK', 'USAP', 'USAS', 'USB', 'USCR', 'USFD', 'USLM', 'USM', 'USNA', 'USPH', 'USX', 'UTHR', 'UTI', 'UTL', 'UTMD', 'UTSI', 'UTX', 'UVE', 'UVSP', 'UVV', 'V', 'VAC', 'VAL', 'VALU', 'VAPO', 'VAR', 'VBTX', 'VC', 'VCEL', 'VCNX', 'VCRA', 'VCYT', 'VEC', 'VECO', 'VEEV', 'VERI', 'VERU', 'VFC', 'VGR', 'VHI', 'VIAC', 'VIAV', 'VICR', 'VIE', 'VIR', 'VIRT', 'VIVE', 'VIVO', 'VKTX', 'VLGEA', 'VLO', 'VLY', 'VMC', 'VMI', 'VMW', 'VNCE', 'VNDA', 'VNE', 'VOXX', 'VOYA', 'VPG', 'VRA', 'VRAY', 'VRCA', 'VREX', 'VRNS', 'VRNT', 'VRRM', 'VRS', 'VRSK', 'VRSN', 'VRTS', 'VRTU', 'VRTV', 'VRTX', 'VSAT', 'VSEC', 'VSH', 'VSLR', 'VST', 'VSTO', 'VUZI', 'VVI', 'VVUS', 'VVV', 'VYGR', 'VZ', 'W', 'WAB', 'WABC', 'WAFD', 'WAL', 'WASH', 'WAT', 'WATT', 'WBA', 'WBS', 'WBT', 'WCC', 'WCN', 'WD', 'WDAY', 'WDC', 'WDFC', 'WDR', 'WEC', 'WEN', 'WERN', 'WETF', 'WEX', 'WEYS', 'WFC', 'WGO', 'WH', 'WHD', 'WHG', 'WHR', 'WIFI', 'WINA', 'WING', 'WINS', 'WIRE', 'WIX', 'WK', 'WLDN', 'WLFC', 'WLH', 'WLK', 'WLL', 'WLTW', 'WM', 'WMB', 'WMK', 'WMS', 'WMT', 'WNC', 'WNEB', 'WOR', 'WORK', 'WORX', 'WOW', 'WPM', 'WPRT', 'WPX', 'WRB', 'WRK', 'WRLD', 'WRTC', 'WSBC', 'WSBF', 'WSC', 'WSFS', 'WSM', 'WSO', 'WST', 'WTBA', 'WTFC', 'WTI', 'WTM', 'WTR', 'WTRE', 'WTS', 'WU', 'WVE', 'WVFC', 'WVVI', 'WW', 'WWD', 'WWE', 'WWR', 'WWW', 'WYND', 'WYNN', 'X', 'XAIR', 'XBIT', 'XEC', 'XEL', 'XENE', 'XENT', 'XLNX', 'XLRN', 'XNCR', 'XOG', 'XOM', 'XOMA', 'XON', 'XONE', 'XPEL', 'XPER', 'XPO', 'XRAY', 'XRX', 'XYL', 'Y', 'YELP', 'YETI', 'YEXT', 'YMAB', 'YNDX', 'YORW', 'YRCW', 'YUM', 'YUMA', 'YUMC', 'ZAGG', 'ZBH', 'ZBRA', 'ZEN', 'ZFGN', 'ZG', 'ZGNX', 'ZION', 'ZIOP', 'ZIXI', 'ZM', 'ZNGA', 'ZS', 'ZTS', 'ZUMZ', 'ZUO', 'ZVO', 'ZYME', 'ZYNE']\n"
     ]
    }
   ],
   "source": [
    "ticker_file = open(\"ticker.txt\")\n",
    "data = ticker_file.readlines()\n",
    "ticker_file.close()\n",
    "\n",
    "ticker_list = [i.rstrip('\\n') for i in data]\n",
    "\n",
    "print(len(ticker_list))\n",
    "print(ticker_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.stocktwits.com/api/2/streams/symbol/BBRY.json\n",
      "./data/BBRY_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/AAPL.json\n",
      "./data/AAPL_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/AMZN.json\n",
      "./data/AMZN_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/BABA.json\n",
      "./data/BABA_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/YHOO.json\n",
      "./data/YHOO_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/FB.json\n",
      "./data/FB_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/GOOG.json\n",
      "./data/GOOG_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/BBBY.json\n",
      "./data/BBBY_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/JNUG.json\n",
      "./data/JNUG_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/SBUX.json\n",
      "./data/SBUX_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/MU.json\n",
      "./data/MU_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/MRVL.json\n",
      "./data/MRVL_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ADMA.json\n",
      "./data/ADMA_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/HIG.json\n",
      "./data/HIG_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/LGND.json\n",
      "./data/LGND_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/DORM.json\n",
      "./data/DORM_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/GIB.json\n",
      "./data/GIB_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/SIEN.json\n",
      "./data/SIEN_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/NFE.json\n",
      "./data/NFE_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/CSFL.json\n",
      "./data/CSFL_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/LM.json\n",
      "./data/LM_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/CERN.json\n",
      "./data/CERN_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/NEE.json\n",
      "./data/NEE_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/FULC.json\n",
      "./data/FULC_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/SGBX.json\n",
      "./data/SGBX_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/TROW.json\n",
      "./data/TROW_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/CWBC.json\n",
      "./data/CWBC_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/TFC.json\n",
      "./data/TFC_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/SBSI.json\n",
      "./data/SBSI_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/DGLY.json\n",
      "./data/DGLY_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/KODK.json\n",
      "./data/KODK_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/BLD.json\n",
      "./data/BLD_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/REZI.json\n",
      "./data/REZI_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/MOS.json\n",
      "./data/MOS_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/SRAX.json\n",
      "./data/SRAX_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/PRA.json\n",
      "./data/PRA_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/CTL.json\n",
      "./data/CTL_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/COLB.json\n",
      "./data/COLB_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/EFC.json\n",
      "./data/EFC_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/BBY.json\n",
      "./data/BBY_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/DK.json\n",
      "./data/DK_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/JILL.json\n",
      "./data/JILL_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/IIIV.json\n",
      "./data/IIIV_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/AJG.json\n",
      "./data/AJG_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/OCGN.json\n",
      "./data/OCGN_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/JBL.json\n",
      "./data/JBL_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ALG.json\n",
      "./data/ALG_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/PSN.json\n",
      "./data/PSN_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/WYND.json\n",
      "./data/WYND_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/MRNA.json\n",
      "./data/MRNA_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ABG.json\n",
      "./data/ABG_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ACIA.json\n",
      "./data/ACIA_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/VRA.json\n",
      "./data/VRA_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/CSS.json\n",
      "./data/CSS_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/SPAR.json\n",
      "./data/SPAR_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/CZNC.json\n",
      "./data/CZNC_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/GWB.json\n",
      "./data/GWB_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/BRKL.json\n",
      "./data/BRKL_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/UTMD.json\n",
      "./data/UTMD_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/MRIN.json\n",
      "./data/MRIN_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/BZH.json\n",
      "./data/BZH_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/WMS.json\n",
      "./data/WMS_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/SNV.json\n",
      "./data/SNV_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/DBD.json\n",
      "./data/DBD_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/LNN.json\n",
      "./data/LNN_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/FBIZ.json\n",
      "./data/FBIZ_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/RBBN.json\n",
      "./data/RBBN_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/SWCH.json\n",
      "./data/SWCH_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/INVA.json\n",
      "./data/INVA_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/AN.json\n",
      "./data/AN_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/FBHS.json\n",
      "./data/FBHS_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/CDAY.json\n",
      "./data/CDAY_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/PNFP.json\n",
      "./data/PNFP_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/EPAM.json\n",
      "./data/EPAM_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/SCI.json\n",
      "./data/SCI_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/VYGR.json\n",
      "./data/VYGR_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/DFS.json\n",
      "./data/DFS_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/FSLY.json\n",
      "./data/FSLY_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/SSRM.json\n",
      "./data/SSRM_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/NEOG.json\n",
      "./data/NEOG_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/QLYS.json\n",
      "./data/QLYS_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/CHMA.json\n",
      "./data/CHMA_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ANF.json\n",
      "./data/ANF_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/CSTL.json\n",
      "./data/CSTL_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/HMHC.json\n",
      "./data/HMHC_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ADES.json\n",
      "./data/ADES_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/CORT.json\n",
      "./data/CORT_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/EYE.json\n",
      "./data/EYE_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/KFY.json\n",
      "./data/KFY_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/RNWK.json\n",
      "./data/RNWK_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/JJSF.json\n",
      "./data/JJSF_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/CRK.json\n",
      "./data/CRK_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/MANT.json\n",
      "./data/MANT_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/IMMR.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/IMMR_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ADUS.json\n",
      "./data/ADUS_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/AR.json\n",
      "./data/AR_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ATO.json\n",
      "./data/ATO_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/NRC.json\n",
      "./data/NRC_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/BCC.json\n",
      "./data/BCC_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/MATX.json\n",
      "./data/MATX_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/CZZ.json\n",
      "./data/CZZ_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ADS.json\n",
      "./data/ADS_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/LFUS.json\n",
      "./data/LFUS_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ENVA.json\n",
      "./data/ENVA_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/WIRE.json\n",
      "./data/WIRE_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/PTEN.json\n",
      "./data/PTEN_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/CDW.json\n",
      "./data/CDW_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/PCTY.json\n",
      "./data/PCTY_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/TOL.json\n",
      "./data/TOL_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ATRC.json\n",
      "./data/ATRC_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/AEE.json\n",
      "./data/AEE_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/TRI.json\n",
      "./data/TRI_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/PINC.json\n",
      "./data/PINC_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/INFN.json\n",
      "./data/INFN_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/TELL.json\n",
      "./data/TELL_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/MTDR.json\n",
      "./data/MTDR_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/SAIC.json\n",
      "./data/SAIC_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/EBIX.json\n",
      "./data/EBIX_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/LCI.json\n",
      "./data/LCI_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/VNE.json\n",
      "./data/VNE_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/YUMA.json\n",
      "./data/YUMA_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/CCS.json\n",
      "./data/CCS_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/SCSC.json\n",
      "./data/SCSC_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/COP.json\n",
      "./data/COP_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/BCOR.json\n",
      "./data/BCOR_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/PLCE.json\n",
      "./data/PLCE_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/CSWI.json\n",
      "./data/CSWI_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ISEE.json\n",
      "./data/ISEE_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/FARM.json\n",
      "./data/FARM_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ECOM.json\n",
      "./data/ECOM_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/CSPI.json\n",
      "./data/CSPI_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/CSIQ.json\n",
      "./data/CSIQ_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/HAE.json\n",
      "./data/HAE_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/CORE.json\n",
      "./data/CORE_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/RMD.json\n",
      "./data/RMD_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/BLBD.json\n",
      "./data/BLBD_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/QTWO.json\n",
      "./data/QTWO_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/XRAY.json\n",
      "./data/XRAY_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ABEO.json\n",
      "./data/ABEO_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/CO.json\n",
      "./data/CO_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/VERI.json\n",
      "./data/VERI_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/PLOW.json\n",
      "./data/PLOW_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/DELL.json\n",
      "./data/DELL_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/TDG.json\n",
      "./data/TDG_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/BNFT.json\n",
      "./data/BNFT_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/GBX.json\n",
      "./data/GBX_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/PRGO.json\n",
      "./data/PRGO_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/MLND.json\n",
      "./data/MLND_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/AVNS.json\n",
      "./data/AVNS_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/OFLX.json\n",
      "./data/OFLX_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/DBI.json\n",
      "./data/DBI_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/FIZZ.json\n",
      "./data/FIZZ_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/DDS.json\n",
      "./data/DDS_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ALGN.json\n",
      "./data/ALGN_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/LLY.json\n",
      "./data/LLY_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/IO.json\n",
      "./data/IO_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/CWT.json\n",
      "./data/CWT_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/MDT.json\n",
      "./data/MDT_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/CYD.json\n",
      "./data/CYD_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/CYH.json\n",
      "./data/CYH_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/SLDB.json\n",
      "./data/SLDB_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/WLK.json\n",
      "./data/WLK_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/FND.json\n",
      "./data/FND_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/MPWR.json\n",
      "./data/MPWR_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/PTLA.json\n",
      "./data/PTLA_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/XBIT.json\n",
      "./data/XBIT_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/MSCI.json\n",
      "./data/MSCI_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/INBK.json\n",
      "./data/INBK_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/CDK.json\n",
      "./data/CDK_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/AVY.json\n",
      "./data/AVY_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/UIHC.json\n",
      "./data/UIHC_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/OFIX.json\n",
      "./data/OFIX_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/DRAD.json\n",
      "./data/DRAD_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/TRQ.json\n",
      "./data/TRQ_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/TNDM.json\n",
      "./data/TNDM_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/SLCA.json\n",
      "./data/SLCA_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/EBS.json\n",
      "./data/EBS_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/CLDX.json\n",
      "./data/CLDX_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/REX.json\n",
      "./data/REX_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/FBMS.json\n",
      "./data/FBMS_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/INVE.json\n",
      "./data/INVE_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/SINT.json\n",
      "./data/SINT_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/HCAT.json\n",
      "./data/HCAT_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/CFFI.json\n",
      "./data/CFFI_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/RNET.json\n",
      "./data/RNET_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/PSMT.json\n",
      "./data/PSMT_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/XNCR.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/XNCR_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/WAFD.json\n",
      "./data/WAFD_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ATH.json\n",
      "./data/ATH_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/FTR.json\n",
      "./data/FTR_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/MYOK.json\n",
      "./data/MYOK_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/AOS.json\n",
      "./data/AOS_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/LBY.json\n",
      "./data/LBY_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/PZZA.json\n",
      "./data/PZZA_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/RLI.json\n",
      "./data/RLI_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/SMED.json\n",
      "./data/SMED_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/CAG.json\n",
      "./data/CAG_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/TRU.json\n",
      "./data/TRU_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/AVYA.json\n",
      "./data/AVYA_20200203_0609.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/CREE.json\n",
      "./data/CREE_20200203_0609.json\n"
     ]
    }
   ],
   "source": [
    "symbols = ['BBRY', 'AAPL', 'AMZN', 'BABA', 'YHOO', 'FB', 'GOOG', 'BBBY', 'JNUG', 'SBUX', 'MU']\n",
    "\n",
    "NUM_REQUEST = 200 - len(symbols)\n",
    "\n",
    "random.seed(12345)\n",
    "symbols.extend(random.sample(ticker_list, NUM_REQUEST))\n",
    "\n",
    "args = ['curl', '-X', 'GET', '']\n",
    "URL = \"https://api.stocktwits.com/api/2/streams/symbol/\"\n",
    "\n",
    "FILE_PATH = \"./data/\"\n",
    "\n",
    "start_datetime = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "for symbol in symbols:\n",
    "    try:\n",
    "        args[3] = URL + symbol + \".json\"\n",
    "        print(args[3])\n",
    "        proc = subprocess.run(args,stdout = subprocess.PIPE, stderr = subprocess.PIPE)\n",
    "\n",
    "        path = FILE_PATH + symbol + \"_\" + start_datetime + \".json\"\n",
    "        print(path)\n",
    "        with open(path, mode='w') as f:\n",
    "            f.write(proc.stdout.decode(\"utf8\"))\n",
    "    except:\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正常なレスポンス・ステータスを持っていないファイルを取り除きます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/CSWI_20200203_0609.json\r\n",
      "data/INBK_20200203_0609.json\r\n"
     ]
    }
   ],
   "source": [
    "!grep -rlv '{\"response\":{\"status\":200}' data\n",
    "!grep -rlv '{\"response\":{\"status\":200}' data | xargs rm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat data/*.json > all_data.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、保存したファイルを、分散処理環境（クラスター）を使って加工するためにHDFSへコピーします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: log4j.properties is not found. HADOOP_CONF_DIR may be incomplete.\r\n"
     ]
    }
   ],
   "source": [
    "!export HADOOP_CONF_DIR=/etc/hadoop/conf; hdfs dfs -put all_data.json ./twits/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\r\n",
      "-rw-r--r--   3 user5 supergroup    9306615 2020-02-03 06:30 twits/all_data.json\r\n"
     ]
    }
   ],
   "source": [
    "!export HADOOP_CONF_DIR=/etc/hadoop/conf; hdfs dfs -ls ./twits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データ変換\n",
    "\n",
    "クラスターでデータを変換します。CDSW上では、ユーザーごとに別のプロジェクトを使っていましたが、クラスター環境では、自分が利用しているユーザーとデータを意識して取り扱う必要があります。\n",
    "\n",
    "\n",
    "あなたの（HADOOPクラスターへアクセスする）ユーザ名は以下で確認できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user5\r\n"
     ]
    }
   ],
   "source": [
    "!echo $HADOOP_USER_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データベースの準備\n",
    "\n",
    "\n",
    "\n",
    "**下記のセルの中を適切なユーザ名とURL（Hiveサーバー）に置換してください。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Engine(hive://user2@master.ykono.work:10000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlalchemy.create_engine('hive://user4@master.ykono.work:10000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**下記のセルの中を適切なユーザ名とURL（Hiveサーバー）に置換してください。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Connected: user5@None'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql hive://user5@master.ykono.work:10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**あなたのユーザ名でデータベースを作成・利用してください**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hive://user4@master.ykono.work:10000\n",
      " * hive://user5@master.ykono.work:10000\n",
      "Done.\n",
      "   hive://user4@master.ykono.work:10000\n",
      " * hive://user5@master.ykono.work:10000\n",
      "Done.\n",
      "   hive://user4@master.ykono.work:10000\n",
      " * hive://user5@master.ykono.work:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>tab_name</th>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql CREATE DATABASE user5\n",
    "%sql USE user5\n",
    "%sql SHOW TABLES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ライブラリファイルのコピー・登録\n",
    "\n",
    "Hiveクエリの中でjsonファイルを扱えるようにするためのライブラリを登録します。\n",
    "ライブラリファイルはGithubリポジトリに含まれています（ライブラリの詳細は`/lib/README.jar`を参照ください）。\n",
    "はじめにCDSWからHDFSにコピーし、HDFS上のファイルをHiveへ登録します。\n",
    "\n",
    "コンパイル済みのライブラリファイルをリポジトリに含めています。\n",
    "- json-1.3.7.3.jar\n",
    "- json-serde-cdh5-shim-1.3.7.3.jar\n",
    "- json-serde-1.3.7.3.jar'\n",
    "\n",
    "- brickhouse-0.7.1-SNAPSHOT.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put: `brickhouse-0.7.1-SNAPSHOT.jar': File exists\n",
      "put: `json-1.3.7.3.jar': File exists\n",
      "put: `json-serde-1.3.7.3.jar': File exists\n",
      "put: `json-serde-cdh5-shim-1.3.7.3.jar': File exists\n",
      "Found 6 items\n",
      "-rw-r--r--   3 user5 supergroup    9306615 2020-02-03 06:23 all_data.json\n",
      "-rw-r--r--   3 user5 supergroup     308146 2020-02-03 06:39 brickhouse-0.7.1-SNAPSHOT.jar\n",
      "-rw-r--r--   3 user5 supergroup      44477 2020-02-03 06:39 json-1.3.7.3.jar\n",
      "-rw-r--r--   3 user5 supergroup      36653 2020-02-03 06:39 json-serde-1.3.7.3.jar\n",
      "-rw-r--r--   3 user5 supergroup       5110 2020-02-03 06:39 json-serde-cdh5-shim-1.3.7.3.jar\n",
      "drwxr-xr-x   - user5 supergroup          0 2020-02-03 06:30 twits\n"
     ]
    }
   ],
   "source": [
    "!export HADOOP_CONF_DIR=/etc/hadoop/conf; hdfs dfs -put `ls -1 ./lib/*.jar` .; hdfs dfs -ls ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hive://user4@master.ykono.work:10000\n",
      " * hive://user5@master.ykono.work:10000\n",
      "Done.\n",
      "   hive://user4@master.ykono.work:10000\n",
      " * hive://user5@master.ykono.work:10000\n",
      "Done.\n",
      "   hive://user4@master.ykono.work:10000\n",
      " * hive://user5@master.ykono.work:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql add jar hdfs:/user/user4/json-1.3.7.3.jar\n",
    "%sql add jar hdfs:/user/user4/json-serde-1.3.7.3.jar\n",
    "%sql add jar hdfs:/user/user4/json-serde-cdh5-shim-1.3.7.3.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hive://user4@master.ykono.work:10000\n",
      " * hive://user5@master.ykono.work:10000\n",
      "Done.\n",
      "   hive://user4@master.ykono.work:10000\n",
      " * hive://user5@master.ykono.work:10000\n",
      "Done.\n",
      "   hive://user4@master.ykono.work:10000\n",
      " * hive://user5@master.ykono.work:10000\n",
      "Done.\n",
      "   hive://user4@master.ykono.work:10000\n",
      " * hive://user5@master.ykono.work:10000\n",
      "Done.\n",
      "   hive://user4@master.ykono.work:10000\n",
      " * hive://user5@master.ykono.work:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql DROP TABLE IF EXISTS twits\n",
    "%sql DROP TABLE IF EXISTS message_extracted\n",
    "%sql DROP TABLE IF EXISTS message_filtered\n",
    "%sql DROP TABLE IF EXISTS message_exploded\n",
    "%sql DROP TABLE IF EXISTS sentiment_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SNSメッセージファイルを格納した場所を指定して、テーブルを作成します。\n",
    "\n",
    "**`LOCATION`指定にあなたがファイルをアップロードしたパスを指定してください**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hive://user4@master.ykono.work:10000\n",
      " * hive://user5@master.ykono.work:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "CREATE EXTERNAL TABLE twits (\n",
    "\tmessages \n",
    "\tARRAY<\n",
    "\t    STRUCT<body: STRING,\n",
    "\t        symbols:ARRAY<STRUCT<symbol:STRING>>,\n",
    "\t        entities:STRUCT<sentiment:STRUCT<basic:STRING>>\n",
    "\t    >\n",
    "\t>\n",
    ")\n",
    "ROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe' \n",
    "STORED AS TEXTFILE\n",
    "LOCATION '/user/user4/twits'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hive://user4@master.ykono.work:10000\n",
      " * hive://user5@master.ykono.work:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>_c0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>197</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(197,)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "select count(*) from twits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hive://user4@master.ykono.work:10000\n",
      " * hive://user5@master.ykono.work:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>messages</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>[{&quot;body&quot;:&quot;$AAPL heng is about to go negative get ready for tomorrow.&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bearish&quot;}}},{&quot;body&quot;:&quot;$AMRN $AAPL $STML $SCYX If you watch only one documentary, it should be this one:\\n\\nhttps://www.netflix.com/title/81026143\\n\\nGod bless these doctors and researchers.&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;AMRN&quot;},{&quot;symbol&quot;:&quot;STML&quot;},{&quot;symbol&quot;:&quot;SCYX&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$AMZN of course not just tesla.. $AAPL  &amp;amp; $MSFT basically did nothing on the cash open after ER&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;AMZN&quot;},{&quot;symbol&quot;:&quot;MSFT&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;Ripping $AAPL $MSFT $SPCE $SPY&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;MSFT&quot;},{&quot;symbol&quot;:&quot;SPY&quot;},{&quot;symbol&quot;:&quot;SPCE&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$AAPL $BABA virus bears at this point are gonna get steam rolled tomorrow....Asia way up&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;BABA&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$AAPL This should easily gap up to 325 tomorrow&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$SPY $TVIX $AAPL \\n\\nSPY current behavior has shown that -1% - 1.5% drop is just a consolidating respectable pullback. Investors that bought in at the beginning of 2019 only bought the dip of fear to compound over &amp;amp; over...Can easily be a 2-3 year rally with bears calling a crash at 410.&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;SPY&quot;},{&quot;symbol&quot;:&quot;TVIX&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$AAPL\\n\\n$330 tomorrow!&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$AAPL futures up&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$AAPL gap up incoming&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$AAPL Apple PT Raised to $375.00 \\n\\nhttps://newsfilter.io/a/c44f08aed41cfc4109cfd90ca575dc5e&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$AAPL Apple Upgraded to Hold by Maxim Group \\n\\nhttps://newsfilter.io/a/3cc9600276d07833855007b0b68aeba3&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$AAPL Hey kids- remember when the bitch was a big deal? 😂😂😂😂&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$AMZN literally amazon is the one us company doing things masterfully. They employs tons of US workers unlike $AAPL and contribute greatly wherever they go. Tons of\\nBezos haters look like dumbasses when they post today. He is self made entrepreneur i remember in 97 people put them down then for selling books online $SPY . Permabears never learn&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;AMZN&quot;},{&quot;symbol&quot;:&quot;SPY&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$AMZN Asia market recovering expect big day tomorrow $AAPL $MSFT $TSLA  ✌️&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;AMZN&quot;},{&quot;symbol&quot;:&quot;MSFT&quot;},{&quot;symbol&quot;:&quot;TSLA&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$SPY ALL TIME HIGHS BABY!!! BEEN CALLING THE DIP SINCE MONDAY!!! LETS GO BULLS!!! SUPER BOWL WEEK!!! MONEY REPO TEAM BACK ONCE AGAIN SIPPIN HENN MIXED WITH JUICE AND GIN! JOIN THEM!!! $$$$$ $BA $AAPL $AMZN $TSLA&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;AMZN&quot;},{&quot;symbol&quot;:&quot;BA&quot;},{&quot;symbol&quot;:&quot;SPY&quot;},{&quot;symbol&quot;:&quot;TSLA&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;Peak profit for the last 6 expired option alerts for $AAPL -18.08  | 529.17  | 3.02  | 354.20  | 402.11  | 294.90  |&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$YM_F if you want to win come trade with me if you want to lose go somewhere else $AAPL $SPY&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;YM_F&quot;},{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;SPY&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$AAPL $TSLA $AMZN $MSFT are earnings winners. Investors let your winners run. Pizza anyone?&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;AMZN&quot;},{&quot;symbol&quot;:&quot;MSFT&quot;},{&quot;symbol&quot;:&quot;TSLA&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$AAPL just leaving this here as future turn green https://www.cnbc.com/2020/01/31/china-economy-beijing-announces-official-manufacturing-pmi-for-january.html&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$AAPL $BABA so futures are up, and Asia, so maybe those numbers aren’t that bad like it said.....&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;BABA&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$NFLX $SPY $AAPL $FB \\n\\n After several successful trades in Netflix both ways it’s shown more obvious bullish strength then bearish. I have been trading Netflix swings above 340 primarily taking the outlook 80% Bull 20% bear which is a healthy practice I do personally let me not get ahead myself to be a perma-bear. Currently Netflix is 4% away from it’s all time high and has struggled several times to breakout above it, but as long as Netflix is trading well above 340 and vegans to build a base above 344 within the next couple of weeks Netflix should be kissing 362.12 - 373.40.&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;NFLX&quot;},{&quot;symbol&quot;:&quot;SPY&quot;},{&quot;symbol&quot;:&quot;FB&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$AA Alcoa’s fundamentals horrible but it may have a bounce as I explained earlier. Moreover Alcoa Day Candle Daily Chart is a Dragonfly Doji     👍\\nwhich is bullish \\nwhen it occurs in a downtrend \\nDaily technicals of RSI being extremely oversold for days and it is END OF MONTH\\n $AAPL $AMZN $WDC $SPY&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;AMZN&quot;},{&quot;symbol&quot;:&quot;AA&quot;},{&quot;symbol&quot;:&quot;SPY&quot;},{&quot;symbol&quot;:&quot;WDC&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$SPY we ripping again, so let’s make sure we stick to chicken 🐔 if we gonna be making any soup and leave them 🦇 alone, #corona $AAPL $AMZN $TSLA&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;AMZN&quot;},{&quot;symbol&quot;:&quot;SPY&quot;},{&quot;symbol&quot;:&quot;TSLA&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$AAPL Watching $317 support.  Daily chart.&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;AA Alcoa’s fundamentals horrible but it may have a bounce as I explained earlier. Moreover Alcoa Day Candle Daily Chart is a Dragonfly Doji     👍\\nwhich bullish \\nwhen it occurs in a downtrend \\nDaily technicals of RSI being extremely oversold for days and it is END OF MONTH\\n $AAPL $AMZN $WDC $SPY&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;AMZN&quot;},{&quot;symbol&quot;:&quot;SPY&quot;},{&quot;symbol&quot;:&quot;WDC&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$AAPL JPMorgan Chase &amp;amp; Boosts Apple Price Target to $350.00 \\n\\nhttps://newsfilter.io/a/9fdc74006956fbe2cb5c99471f5bf803&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$AAPL coronavirus goes to India and Philippines google closed all offices Tesla closed https://m.youtube.com/watch?v=6DBFwIlT4fg&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$AMZN $SPY $AAPL $MCD&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;AMZN&quot;},{&quot;symbol&quot;:&quot;MCD&quot;},{&quot;symbol&quot;:&quot;SPY&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$AAPL never short a market when you have  trump has president this guy won’t let the market go down&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>[{&quot;body&quot;:&quot;$ABEO Stochastic is turning. Tomorrow will likely bring the stochastic buy signal, as the slow crosses above the fast, and that Olivergarden dimwit can go pound sand&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABEO&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$ABEO 2.30s ouch... can I get 2.10s?&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABEO&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bearish&quot;}}},{&quot;body&quot;:&quot;$ABEO added bigly here...insider owns $500k for a reason here&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABEO&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$ABEO $CLSN two biggest positions and two biggest drawdowns right now&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;CLSN&quot;},{&quot;symbol&quot;:&quot;ABEO&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$ABEO if it doesn’t hold here, it’s going to get really cheap.&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABEO&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ABEO Their CSO stepped down at the beginning of the year. 🤦Big news into cell-therapy, reported earnings date of 3-21-20.👍 Look to get in around $2.20 or lower. 💵🤞&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABEO&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$ABEO Daily short volume at a whopping 75%, same as yesterday. That can&amp;#39;t be sustainable. MMs have two days to make good on all the non-existent shares they sold today, and tomorrow is the settlement date for the assloads of non-existent shares they sold yesterday. They can&amp;#39;t settle counterfeit shares using more non-existent shares. That doesn&amp;#39;t work. Let&amp;#39;s see what the short-bag-holding dimwits do tomorrow. The clock is ticking. I&amp;#39;d like to see the fraudsters stick around and get torched&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABEO&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$ABEO ⏫✔ Wait on the right price point to get in.  Looking around $2.20 🤘🐂&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABEO&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$ABEO will go to 2 soon&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABEO&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bearish&quot;}}},{&quot;body&quot;:&quot;I wish I shorted $ABEO a year ago; the -61.46% change sure looks sweet now https://wallmine.com/nasdaq/abeo?utm_source=stocktwits&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABEO&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ABEO vol just not there , I’m out GL all&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABEO&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ABEO here we go, bids stacking at 2.49, 2.5 mini wall down&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABEO&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$ABEO long hold 👍🏻&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABEO&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$ABEO need to knock down the 2.5 mini wall and we will move&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABEO&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$ABEO ↗ Charts say buy✔ I&amp;#39;m holding off for another drop before conformation on positive price movement. 🤘&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABEO&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$ABEO back to 3 please. any news in the near future?!&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABEO&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$ABEO Daily short volume was 75%, and those parasites still lost ground&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABEO&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$ABEO another head and shoulders setup on the intraday pattern. @Reformed_Trader&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABEO&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$ABEO what a tin stock I tried to add 15,000 share and pop like 5 cent -- come on&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABEO&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ABEO really, no cheerleading or pumping? for shame&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABEO&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$ABEO looking good&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABEO&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$ABEO pretty clear there’s good support here, if your short why wouldn’t you lock in profits and ride it back to $3 at this point ?&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABEO&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$ABEO some volume coming now&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABEO&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$ABEO lol these two posts juxtaposed 😂  @jsp4423&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABEO&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$ABEO add again.&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABEO&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$ABEO i am betting a dilution very soon...&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABEO&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ABEO insiders added recently. Dilution Done&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABEO&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$ABEO adding more, Wellington adding as well as other institutions is all I need to see, close to 4K shares now&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABEO&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;Wellington Management Group LLP has filed an amended 13G/A, reporting 8.04% ownership in $ABEO - https://fintel.io/so/us/abeo?utm_source=stocktwits.com&amp;amp;utm_medium=social&amp;amp;utm_campaign=owner&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABEO&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ABEO bot more down here, Wellington increase&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABEO&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>[{&quot;body&quot;:&quot;Departure of Directors or Certain  http://www.conferencecalltranscripts.org/8/summary2/?id=7351264 $ABG&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABG&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;Thinking about investing in $ABG #AsburyAutomotive? The 8-K filing touching on departure of directors or certain officers among other topics might be what you&amp;#39;re looking for https://wallmine.com/filing/redirect/12084309?utm_source=stocktwits&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABG&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ABG just filed a Event for Officers https://last10k.com/sec-filings/abg/0001144980-20-000008.htm?utm_source=stocktwits&amp;amp;utm_medium=forum&amp;amp;utm_campaign=8K&amp;amp;utm_term=abg&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABG&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ABG / Asbury Automotive Group files form 8-K - Departure of Directors or Certain Officers; Election of Directors; Appointment of Certain Officers; Compensatory Arrangements of Certain Officers https://fintel.io/s/us/abg?utm_source=stocktwits.com&amp;amp;utm_medium=Social&amp;amp;utm_campaign=filing&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABG&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ABG filed form 8-K on January 30, 17:31:01: Item5.02: Departure of Election 0f Officers or Compensatory Arrangements https://s.flashalert.me/oUEIw&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABG&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ABG Form 8-K: Departure of Directors or Certain Officers; Election of Directors; Appointment of Certain Officers; Compensatory Arrangements of Certain Officers.Asbury Automotive Group’s has announced.. \\n\\nhttps://newsfilter.io/a/db75603965c2c7ba2de12f8e4613ad92&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABG&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;Victory Capital Management Inc has filed an amended 13G/A, reporting 3.17% ownership in $ABG - https://fintel.io/so/us/abg?utm_source=stocktwits.com&amp;amp;utm_medium=social&amp;amp;utm_campaign=owner&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABG&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ABG filed form SC 13G/A on January 30, 10:08:33 https://s.flashalert.me/ctDqz5&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABG&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;Victory Capital Management Inc. just provided an update on share ownership of Asbury Automotive http://www.conferencecalltranscripts.org/13G/summary/?id=7348696 $ABG&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABG&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ABG Form SC 13G/A (statement of acquisition of beneficial ownership by individuals) filed with the SEC \\n\\nhttps://newsfilter.io/a/5596c4ca5f31b12c5d5994aa21748dfe&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABG&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;BULLISH NEWS FOR $ABG\\n\\nhttps://www.nasdaq.com/articles/asbury-automotive-group-abg-earnings-expected-to-grow%3A-what-to-know-ahead-of-next-weeks&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABG&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$ABG $2.34 Earnings Per Share Expected for Asbury Automotive Group This Quarter \\n\\nhttps://newsfilter.io/a/6362dd8c82a409e658cd91c1f1f571a7&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABG&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ABG Asbury Automotive Group Price Target Cut to $104.00 \\n\\nhttps://newsfilter.io/a/9539ffb509fcedb246b9bf036f5b143c&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABG&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;Asbury Automotive Group&amp;#39;s PT cut by Morgan Stanley to $104.00. equal weight rating. https://www.marketbeat.com/r/1333835 $ABG&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABG&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ABG Morgan Stanley Maintains to Equal-Weight : PT $104.00 https://stockhoot.com/ExtSymbol.aspx?from=AnalystRatingTweet&amp;amp;symbol=ABG&amp;amp;t=593&amp;amp;Social=StockTwits&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABG&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;Asbury (ABG) to release earnings before the market closes on Monday, February 3. Expected EPS: 2.34. $ABG https://www.tipranks.com/stocks/ABG/earnings-calendar?ref=TREarnings&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABG&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ABG Asbury Automotive Group Schedules Release of Fourth Quarter and Full Year 2019 Financial Results \\n\\nhttps://newsfilter.io/a/f62d14cb5a8c54a2a4c53a2891ff572f&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABG&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;Undervalued Signal Alert: $ABG. More insights: https://stockinvest.us/technical-analysis/ABG?utm_source=stocktwits&amp;amp;utm_medium=autopost&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABG&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;How does this make you feel? $ABG RSI Indicator left the oversold zone. View odds of uptrend. https://tickeron.com/go/1133340&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABG&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;AsburyAutomotiveGroup $ABG BidaskScore is Downgraded to Held https://bidaskclub.com/news/company-news/company-news-company-news/2020/01/asbury-automotive-group-abg-bidaskscore-is-downgraded-to-held/&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABG&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$GPI - Hold Group 1 and Asbury Automotive ($ABG) in the same… http://dlvr.it/RMsv0z #portfolio_prospective #better_portfolio #diversify&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABG&quot;},{&quot;symbol&quot;:&quot;GPI&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;Departure of Directors or Certain  http://www.conferencecalltranscripts.org/8/summary/?id=7272462 $ABG&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABG&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ABG / Asbury Automotive Group files form 8-K - Departure of Directors or Certain Officers; Election of Directors; Appointment of Certain Officers; Compensatory Arrangements of Certain Officers https://fintel.io/s/us/abg?utm_source=stocktwits.com&amp;amp;utm_medium=Social&amp;amp;utm_campaign=filing&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABG&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ABG just filed a Event for Officers https://last10k.com/sec-filings/abg/0001144980-20-000005.htm?utm_source=stocktwits&amp;amp;utm_medium=forum&amp;amp;utm_campaign=8K&amp;amp;utm_term=abg&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABG&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;Fresh - #AsburyAutomotive released a current report talking about election of directors and other topics. See what others think $ABG https://wallmine.com/filing/redirect/12031426?utm_source=stocktwits&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABG&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ABG filed form 8-K on January 09, 17:29:18: Item5.02: Departure of Election 0f Officers or Compensatory Arrangements https://s.flashalert.me/6seZQ&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABG&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ABG  Form 8-K: Departure of Directors or Certain Officers; Election of Directors; Appointment of Certain Officers; Compensatory Arrangements of Certain Officers.As previously reported by Asbury Automo.. https://newsfilter.io/a/38a217f41dadf750c697717dc4a83b50&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABG&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ABG  Should Value Investors Pick Asbury Automotive Stock?: Value investing is easily one of the most popular ways to find great stocks in any market environment. After all, who wouldn’t want to find s.. https://newsfilter.io/a/2523652df6f32201b94582f805d556f8&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABG&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;BULLISH NEWS FOR $ABG\\n\\nhttps://simplywall.st/stocks/us/retail/nyse-abg/asbury-automotive-group/news/heres-why-i-think-asbury-automotive-group-nyseabg-is-an-interesting-stock/&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABG&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$LAD $ABG $GOLF $MRC $CMC  5 Growth Stocks to Buy as Middle-East Tensions Subside: When the markets were expecting Middle-East tensions to intensify following Tehran’s retaliatory attack, President Don.. https://newsfilter.io/a/bd29d2fae3108f0c0ea80b2e20467ba7&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;GOLF&quot;},{&quot;symbol&quot;:&quot;ABG&quot;},{&quot;symbol&quot;:&quot;CMC&quot;},{&quot;symbol&quot;:&quot;LAD&quot;},{&quot;symbol&quot;:&quot;MRC&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}}]</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[('[{\"body\":\"$AAPL heng is about to go negative get ready for tomorrow.\",\"symbols\":[{\"symbol\":\"AAPL\"}],\"entities\":{\"sentiment\":{\"basic\":\"Bearish\"}}},{\"b ... (6923 characters truncated) ... hen you have  trump has president this guy won’t let the market go down\",\"symbols\":[{\"symbol\":\"AAPL\"}],\"entities\":{\"sentiment\":{\"basic\":\"Bullish\"}}}]',),\n",
       " ('[{\"body\":\"$ABEO Stochastic is turning. Tomorrow will likely bring the stochastic buy signal, as the slow crosses above the fast, and that Olivergarde ... (4839 characters truncated) ... entiment\":null}},{\"body\":\"$ABEO bot more down here, Wellington increase\",\"symbols\":[{\"symbol\":\"ABEO\"}],\"entities\":{\"sentiment\":{\"basic\":\"Bullish\"}}}]',),\n",
       " ('[{\"body\":\"Departure of Directors or Certain  http://www.conferencecalltranscripts.org/8/summary2/?id=7351264 $ABG\",\"symbols\":[{\"symbol\":\"ABG\"}],\"enti ... (7325 characters truncated) ... 0c0ea80b2e20467ba7\",\"symbols\":[{\"symbol\":\"GOLF\"},{\"symbol\":\"ABG\"},{\"symbol\":\"CMC\"},{\"symbol\":\"LAD\"},{\"symbol\":\"MRC\"}],\"entities\":{\"sentiment\":null}}]',)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "select * from twits limit 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データ変換のためのテーブルを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hive://user4@master.ykono.work:10000\n",
      " * hive://user5@master.ykono.work:10000\n",
      "Done.\n",
      "   hive://user4@master.ykono.work:10000\n",
      " * hive://user5@master.ykono.work:10000\n",
      "Done.\n",
      "   hive://user4@master.ykono.work:10000\n",
      " * hive://user5@master.ykono.work:10000\n",
      "Done.\n",
      "   hive://user4@master.ykono.work:10000\n",
      " * hive://user5@master.ykono.work:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql create table message_extracted (symbols array<struct<symbol:string>>, sentiment STRING, body STRING) STORED AS TEXTFILE\n",
    "%sql create table message_filtered (symbols array<struct<symbol:string>>, sentiment STRING, body STRING) STORED AS TEXTFILE\n",
    "%sql create table message_exploded (symbol string, sentiment STRING, body STRING) STORED AS TEXTFILE\n",
    "%sql create table sentiment_data (sentiment int, body STRING) STORED AS TEXTFILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "元のデータから必要なデータのみを抽出します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hive://user4@master.ykono.work:10000\n",
      " * hive://user5@master.ykono.work:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "insert overwrite table message_extracted \n",
    "select message.symbols, message.entities.sentiment, message.body from twits \n",
    "lateral view explode(messages) messages as message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hive://user4@master.ykono.work:10000\n",
      " * hive://user5@master.ykono.work:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>symbols</th>\n",
       "        <th>sentiment</th>\n",
       "        <th>body</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>[{&quot;symbol&quot;:&quot;AAPL&quot;}]</td>\n",
       "        <td>Bearish</td>\n",
       "        <td>$AAPL heng is about to go negative get ready for tomorrow.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;AMRN&quot;},{&quot;symbol&quot;:&quot;STML&quot;},{&quot;symbol&quot;:&quot;SCYX&quot;}]</td>\n",
       "        <td>Bullish</td>\n",
       "        <td>$AMRN $AAPL $STML $SCYX If you watch only one documentary, it should be this one:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>[]</td>\n",
       "        <td>None</td>\n",
       "        <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>[{&quot;symbol&quot;:&quot;https://www.netflix.com/title/81026143&quot;}]</td>\n",
       "        <td>None</td>\n",
       "        <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>[]</td>\n",
       "        <td>None</td>\n",
       "        <td>None</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[('[{\"symbol\":\"AAPL\"}]', 'Bearish', '$AAPL heng is about to go negative get ready for tomorrow.'),\n",
       " ('[{\"symbol\":\"AAPL\"},{\"symbol\":\"AMRN\"},{\"symbol\":\"STML\"},{\"symbol\":\"SCYX\"}]', 'Bullish', '$AMRN $AAPL $STML $SCYX If you watch only one documentary, it should be this one:'),\n",
       " ('[]', None, None),\n",
       " ('[{\"symbol\":\"https://www.netflix.com/title/81026143\"}]', None, None),\n",
       " ('[]', None, None)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "select * from message_extracted limit 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データから、メッセージ・ボディが含まれているデータのみを取り出します。同時に、銘柄に対するセンチメントを文字列からを数値に置換します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hive://user4@master.ykono.work:10000\n",
      " * hive://user5@master.ykono.work:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "insert overwrite table message_filtered \n",
    "select symbols, \n",
    "    case sentiment when 'Bearish' then -2 when 'Bullish' then 2 ELSE 0 END as sentiment, \n",
    "    body from message_extracted \n",
    "    where body is not null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hive://user4@master.ykono.work:10000\n",
      " * hive://user5@master.ykono.work:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>symbols</th>\n",
       "        <th>sentiment</th>\n",
       "        <th>body</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>[{&quot;symbol&quot;:&quot;AAPL&quot;}]</td>\n",
       "        <td>-2</td>\n",
       "        <td>$AAPL heng is about to go negative get ready for tomorrow.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;AMRN&quot;},{&quot;symbol&quot;:&quot;STML&quot;},{&quot;symbol&quot;:&quot;SCYX&quot;}]</td>\n",
       "        <td>2</td>\n",
       "        <td>$AMRN $AAPL $STML $SCYX If you watch only one documentary, it should be this one:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;AMZN&quot;},{&quot;symbol&quot;:&quot;MSFT&quot;}]</td>\n",
       "        <td>0</td>\n",
       "        <td>$AMZN of course not just tesla.. $AAPL  &amp;amp; $MSFT basically did nothing on the cash open after ER</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[('[{\"symbol\":\"AAPL\"}]', '-2', '$AAPL heng is about to go negative get ready for tomorrow.'),\n",
       " ('[{\"symbol\":\"AAPL\"},{\"symbol\":\"AMRN\"},{\"symbol\":\"STML\"},{\"symbol\":\"SCYX\"}]', '2', '$AMRN $AAPL $STML $SCYX If you watch only one documentary, it should be this one:'),\n",
       " ('[{\"symbol\":\"AAPL\"},{\"symbol\":\"AMZN\"},{\"symbol\":\"MSFT\"}]', '0', '$AMZN of course not just tesla.. $AAPL  &amp; $MSFT basically did nothing on the cash open after ER')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "select * from message_filtered limit 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一つのメッセージに複数の銘柄が紐づけられています。データ正規化のため、データ１行につき、一つの銘柄を持つようにデータを変換します（同じメッセージを持つ行が複数作られます）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hive://user4@master.ykono.work:10000\n",
      " * hive://user5@master.ykono.work:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "insert overwrite table message_exploded \n",
    "select symbol.symbol, sentiment, body from message_filtered lateral view explode(symbols) symbols as symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hive://user4@master.ykono.work:10000\n",
      " * hive://user5@master.ykono.work:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>symbol</th>\n",
       "        <th>sentiment</th>\n",
       "        <th>body</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>AAPL</td>\n",
       "        <td>-2</td>\n",
       "        <td>$AAPL heng is about to go negative get ready for tomorrow.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>AAPL</td>\n",
       "        <td>2</td>\n",
       "        <td>$AMRN $AAPL $STML $SCYX If you watch only one documentary, it should be this one:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>AMRN</td>\n",
       "        <td>2</td>\n",
       "        <td>$AMRN $AAPL $STML $SCYX If you watch only one documentary, it should be this one:</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[('AAPL', '-2', '$AAPL heng is about to go negative get ready for tomorrow.'),\n",
       " ('AAPL', '2', '$AMRN $AAPL $STML $SCYX If you watch only one documentary, it should be this one:'),\n",
       " ('AMRN', '2', '$AMRN $AAPL $STML $SCYX If you watch only one documentary, it should be this one:')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "select * from message_exploded limit 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここまでの操作で、元の複雑な構造のデータから、１レコードにつき、銘柄、センチメント、メッセージ本文を持つフォーマットに変換されました。\n",
    "銘柄毎のセンチメントの件数などの分析を行うには、このテーブルを利用します。\n",
    "\n",
    "この後の感情分析では、メッセージ本文の文字列から、センチメントを判定する予測モデルを構築します。そのため銘柄情報は利用しないため、センチメントとメッセージ本文のみを取り出します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hive://user4@master.ykono.work:10000\n",
      " * hive://user5@master.ykono.work:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "insert overwrite table sentiment_data \n",
    "select sentiment, body from message_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hive://user4@master.ykono.work:10000\n",
      " * hive://user5@master.ykono.work:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>sentiment</th>\n",
       "        <th>body</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>-2</td>\n",
       "        <td>$AAPL heng is about to go negative get ready for tomorrow.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2</td>\n",
       "        <td>$AMRN $AAPL $STML $SCYX If you watch only one documentary, it should be this one:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>0</td>\n",
       "        <td>$AMZN of course not just tesla.. $AAPL  &amp;amp; $MSFT basically did nothing on the cash open after ER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2</td>\n",
       "        <td>Ripping $AAPL $MSFT $SPCE $SPY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>0</td>\n",
       "        <td>$AAPL $BABA virus bears at this point are gonna get steam rolled tomorrow....Asia way up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2</td>\n",
       "        <td>$AAPL This should easily gap up to 325 tomorrow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2</td>\n",
       "        <td>$SPY $TVIX $AAPL </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2</td>\n",
       "        <td>$AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2</td>\n",
       "        <td>$AAPL futures up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>0</td>\n",
       "        <td>$AAPL gap up incoming</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(-2, '$AAPL heng is about to go negative get ready for tomorrow.'),\n",
       " (2, '$AMRN $AAPL $STML $SCYX If you watch only one documentary, it should be this one:'),\n",
       " (0, '$AMZN of course not just tesla.. $AAPL  &amp; $MSFT basically did nothing on the cash open after ER'),\n",
       " (2, 'Ripping $AAPL $MSFT $SPCE $SPY'),\n",
       " (0, '$AAPL $BABA virus bears at this point are gonna get steam rolled tomorrow....Asia way up'),\n",
       " (2, '$AAPL This should easily gap up to 325 tomorrow'),\n",
       " (2, '$SPY $TVIX $AAPL '),\n",
       " (2, '$AAPL'),\n",
       " (2, '$AAPL futures up'),\n",
       " (0, '$AAPL gap up incoming')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "select * from sentiment_data limit 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSONファイルの作成\n",
    "\n",
    "加工したデータをJSONファイルとして出力します。\n",
    "\n",
    "感情分析を担当するデータサイエンティスト・機械学習エンジニアは、このJSONファイルを使います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hive://user4@master.ykono.work:10000\n",
      " * hive://user5@master.ykono.work:10000\n",
      "Done.\n",
      "   hive://user4@master.ykono.work:10000\n",
      " * hive://user5@master.ykono.work:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql add jar hdfs:/tmp/brickhouse-0.7.1-SNAPSHOT.jar\n",
    "%sql CREATE TEMPORARY FUNCTION to_json AS 'brickhouse.udf.json.ToJsonUDF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hive://user4@master.ykono.work:10000\n",
      " * hive://user5@master.ykono.work:10000\n",
      "Done.\n",
      "   hive://user4@master.ykono.work:10000\n",
      " * hive://user5@master.ykono.work:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql DROP TABLE IF EXISTS json_message\n",
    "%sql create table json_message (message STRING) STORED AS TEXTFILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hive://user4@master.ykono.work:10000\n",
      " * hive://user5@master.ykono.work:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "insert overwrite table json_message\n",
    "select to_json(named_struct('message_body', body, 'sentiment', sentiment)) from sentiment_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hive://user4@master.ykono.work:10000\n",
      " * hive://user5@master.ykono.work:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>message</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>{&quot;message_body&quot;:&quot;$AAPL heng is about to go negative get ready for tomorrow.&quot;,&quot;sentiment&quot;:-2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>{&quot;message_body&quot;:&quot;$AMRN $AAPL $STML $SCYX If you watch only one documentary, it should be this one:&quot;,&quot;sentiment&quot;:2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>{&quot;message_body&quot;:&quot;$AMZN of course not just tesla.. $AAPL  &amp;amp; $MSFT basically did nothing on the cash open after ER&quot;,&quot;sentiment&quot;:0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>{&quot;message_body&quot;:&quot;Ripping $AAPL $MSFT $SPCE $SPY&quot;,&quot;sentiment&quot;:2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>{&quot;message_body&quot;:&quot;$AAPL $BABA virus bears at this point are gonna get steam rolled tomorrow....Asia way up&quot;,&quot;sentiment&quot;:0}</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[('{\"message_body\":\"$AAPL heng is about to go negative get ready for tomorrow.\",\"sentiment\":-2}',),\n",
       " ('{\"message_body\":\"$AMRN $AAPL $STML $SCYX If you watch only one documentary, it should be this one:\",\"sentiment\":2}',),\n",
       " ('{\"message_body\":\"$AMZN of course not just tesla.. $AAPL  &amp; $MSFT basically did nothing on the cash open after ER\",\"sentiment\":0}',),\n",
       " ('{\"message_body\":\"Ripping $AAPL $MSFT $SPCE $SPY\",\"sentiment\":2}',),\n",
       " ('{\"message_body\":\"$AAPL $BABA virus bears at this point are gonna get steam rolled tomorrow....Asia way up\",\"sentiment\":0}',)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "select * from json_message limit 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`HQL_SELECT_MESSAGE`をあなたが作成したデータベースを指定してください**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from __future__ import print_function\n",
    "\n",
    "HQL_SELECT_MESSAGE = \"select * from user5.json_message\"\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"JsonGen\")\\\n",
    "    .getOrCreate()\n",
    "    \n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "json_list = spark.sql(HQL_SELECT_MESSAGE)\n",
    "\n",
    "path = \"./output.json\"\n",
    "\n",
    "with open(path, mode='w') as f:\n",
    "    f.write('{\"data\":[')\n",
    "    bool_first_line = True\n",
    "    for row in json_list.rdd.collect():\n",
    "        if bool_first_line:\n",
    "            bool_first_line = False\n",
    "            f.write(row.message)\n",
    "        else:\n",
    "            # あまりスマートではありませんが、ある程度の量のデータを使ったDeep Learning処理をシミュレーションするため、\n",
    "            # 同じ情報を使って、データを嵩増ししています。\n",
    "            # API利用の制約や、演習時間の制約がなければ、\n",
    "            # 上記のWebスクレイピングで、大量の訓練データを取得することが可能です。\n",
    "            for i in range(100): \n",
    "                f.write(\",\\n\")\n",
    "                f.write(row.message)\n",
    "    \n",
    "    f.write(\"]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 198992\r\n",
      "-rw-r--r-- 1 cdsw cdsw  9306615 Feb  3 06:22 all_data.json\r\n",
      "-rwxr-xr-x 1 cdsw cdsw       56 Feb  3 06:06 cdsw-build.sh\r\n",
      "drwxr-xr-x 2 cdsw cdsw    12288 Feb  3 06:13 data\r\n",
      "-rwxr-xr-x 1 cdsw cdsw      305 Feb  3 06:06 git_amend.sh\r\n",
      "-rw-r--r-- 1 cdsw cdsw      278 Feb  3 06:06 git_env.sh\r\n",
      "-rw-r--r-- 1 cdsw cdsw      122 Feb  3 06:06 hadoop_env.sh\r\n",
      "-rwxr-xr-x 1 cdsw cdsw      774 Feb  3 06:06 json_get.py\r\n",
      "drwxr-xr-x 2 cdsw cdsw     4096 Feb  3 06:06 lib\r\n",
      "drwxr-xr-x 2 cdsw cdsw     4096 Feb  3 06:06 misc\r\n",
      "-rwxr-xr-x 1 cdsw cdsw     3865 Feb  3 06:06 model_api.py\r\n",
      "-rw-r--r-- 1 cdsw cdsw    51856 Feb  3 06:06 nlp_handson.ipynb\r\n",
      "-rw-r--r-- 1 cdsw cdsw   199209 Feb  3 06:54 nlp_solution.ipynb\r\n",
      "-rw-r--r-- 1 cdsw cdsw 97060803 Feb  3 06:52 output5.json\r\n",
      "-rw-r--r-- 1 cdsw cdsw 97060803 Feb  3 06:56 output.json\r\n",
      "-rw-r--r-- 1 cdsw cdsw      230 Feb  3 06:06 README.md\r\n",
      "-rw-r--r-- 1 cdsw cdsw     1243 Feb  3 06:06 requirements.txt\r\n",
      "drwxr-xr-x 2 cdsw cdsw     4096 Feb  3 06:06 sentiment\r\n",
      "-rw-r--r-- 1 cdsw cdsw    13089 Feb  3 06:06 ticker.txt\r\n",
      "-rwxr-xr-x 1 cdsw cdsw     1377 Feb  3 06:06 train_model.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 感情分析\n",
    "\n",
    "投資判断のために、企業の価値を考慮する際のアプローチとして、従来の枠組みにとらわれない様々な情報（オルタナティブ・データ）を用いることを考えます。\n",
    "\n",
    "投資家の判断を左右し得る様々な情報を入力とし、投資判断のための定量的なシグナルに変換する予測モデルを構築します。\n",
    "入力となるデータには様々なものがあります。以下はその例です。\n",
    "\n",
    "- ニュース（製品のリコール、自然災害など）\n",
    "\n",
    "ニューラルネットワークを使ったDeep Learningによって、入力データの形式を問わず、予測モデルを構築することができます。\n",
    "\n",
    "ここでは、ソーシャルメディアサイトStockTwitsの投稿を使用します。\n",
    "StockTwitsのコミュニティは、投資家、トレーダー、起業家により利用されています。\n",
    "\n",
    "感情のスコアを生成するこれらのtwitを中心にモデルを構築します。\n",
    "\n",
    "モデルの訓練のためには、入力に対応するラベルが必要になります。ラベルの精度は、モデルの訓練に当たって大変重要な要素です。\n",
    "\n",
    "センチメントの度合いを把握するために、非常にネガティブ、ネガティブ、ニュートラル、ポジティブ、非常にポジティブという5段階のスケールを使用します。それぞれ、-2から2までの数値に対応しています。\n",
    "\n",
    "このラベル付きデータによって訓練されたモデルを使用して、自然言語を入力として、その文章の背後にある感情を予測するモデルを構築します。\n",
    "\n",
    "\n",
    "### データの確認\n",
    "データがどのように見えるかを確認します。\n",
    "\n",
    "各フィールドの意味:\n",
    "\n",
    "* `'message_body'`: メッセージ本文テキスト\n",
    "* `'sentiment'`: センチメントスコア。-2から2までの５段階。0は中立。\n",
    "\n",
    "下記のような内容になっているはずです。\n",
    "```\n",
    "{'data':\n",
    "  {'message_body': '............................',\n",
    "   'sentiment': 2},\n",
    "  {'message_body': '............................',\n",
    "   'sentiment': -2},\n",
    "   ...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"data\":[{\"message_body\":\"$AAPL heng is about to go negative get ready for tomorrow.\",\"sentiment\":-2},\n",
      "{\"message_body\":\"$AMRN $AAPL $STML $SCYX If you watch only one documentary, it should be this one:\",\"sentiment\":2},\n",
      "{\"message_body\":\"$AMRN $AAPL $STML $SCYX If you watch only one documentary, it should be this one:\",\"sentiment\":2},\n",
      "{\"message_body\":\"$AMRN $AAPL $STML $SCYX If you watch only one documentary, it should be this one:\",\"sentiment\":2},\n",
      "{\"message_body\":\"$AMRN $AAPL $STML $SCYX If you watch only one documentary, it should be this one:\",\"sentiment\":2},\n",
      "{\"message_body\":\"$AMRN $AAPL $STML $SCYX If you watch only one documentary, it should be this one:\",\"sentiment\":2},\n",
      "{\"message_body\":\"$AMRN $AAPL $STML $SCYX If you watch only one documentary, it should be this one:\",\"sentiment\":2},\n",
      "{\"message_body\":\"$AMRN $AAPL $STML $SCYX If you watch only one documentary, it should be this one:\",\"sentiment\":2},\n",
      "{\"message_body\":\"$AMRN $AAPL $STML $SCYX If you watch only one documentary, it should be this one:\",\"sentiment\":2},\n",
      "{\"message_body\":\"$AMRN $AAPL $STML $SCYX If you watch only one documentary, it should be this one:\",\"sentiment\":2},\n",
      "{\"message_body\":\"$CEI $SAEX on fire in PM could fuel shares of $BIMI and $YUMA\",\"sentiment\":2},\n",
      "{\"message_body\":\"$CEI $SAEX on fire in PM could fuel shares of $BIMI and $YUMA\",\"sentiment\":2},\n",
      "{\"message_body\":\"$CEI $SAEX on fire in PM could fuel shares of $BIMI and $YUMA\",\"sentiment\":2},\n",
      "{\"message_body\":\"$CEI $SAEX on fire in PM could fuel shares of $BIMI and $YUMA\",\"sentiment\":2},\n",
      "{\"message_body\":\"$CEI $SAEX on fire in PM could fuel shares of $BIMI and $YUMA\",\"sentiment\":2},\n",
      "{\"message_body\":\"$CEI $SAEX on fire in PM could fuel shares of $BIMI and $YUMA\",\"sentiment\":2},\n",
      "{\"message_body\":\"$CEI $SAEX on fire in PM could fuel shares of $BIMI and $YUMA\",\"sentiment\":2},\n",
      "{\"message_body\":\"$CEI $SAEX on fire in PM could fuel shares of $BIMI and $YUMA\",\"sentiment\":2},\n",
      "{\"message_body\":\"$CEI $SAEX on fire in PM could fuel shares of $BIMI and $YUMA\",\"sentiment\":2},\n",
      "{\"message_body\":\"$CEI $SAEX on fire in PM could fuel shares of $BIMI and $YUMA\",\"sentiment\":2}]}"
     ]
    }
   ],
   "source": [
    "!head output.json\n",
    "!tail output.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データを読み込みます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'message_body': '$AAPL heng is about to go negative get ready for tomorrow.', 'sentiment': -2}, {'message_body': '$AMRN $AAPL $STML $SCYX If you watch only one documentary, it should be this one:', 'sentiment': 2}, {'message_body': '$AMRN $AAPL $STML $SCYX If you watch only one documentary, it should be this one:', 'sentiment': 2}, {'message_body': '$AMRN $AAPL $STML $SCYX If you watch only one documentary, it should be this one:', 'sentiment': 2}, {'message_body': '$AMRN $AAPL $STML $SCYX If you watch only one documentary, it should be this one:', 'sentiment': 2}, {'message_body': '$AMRN $AAPL $STML $SCYX If you watch only one documentary, it should be this one:', 'sentiment': 2}, {'message_body': '$AMRN $AAPL $STML $SCYX If you watch only one documentary, it should be this one:', 'sentiment': 2}, {'message_body': '$AMRN $AAPL $STML $SCYX If you watch only one documentary, it should be this one:', 'sentiment': 2}, {'message_body': '$AMRN $AAPL $STML $SCYX If you watch only one documentary, it should be this one:', 'sentiment': 2}, {'message_body': '$AMRN $AAPL $STML $SCYX If you watch only one documentary, it should be this one:', 'sentiment': 2}]\n"
     ]
    }
   ],
   "source": [
    "with open('./output.json', 'r') as f:\n",
    "    twits = json.load(f)\n",
    "\n",
    "print(twits['data'][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データ件数の確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "590901\n"
     ]
    }
   ],
   "source": [
    "print(len(twits['data']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データの前処理\n",
    "\n",
    "テキストを前処理します。\n",
    "\n",
    "本文に含まれるティッカーシンボル（「$シンボル」で示される）はセンチメントに関する情報を提供しないため削除します。\n",
    "また、「@ユーザー名」で、ユーザに関する情報が記載されていますが、これもまたセンチメント情報を提供しないため、削除します。\n",
    "URLも削除します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### メッセージ本文とセンチメント・ラベルのリスト化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [twit['message_body'] for twit in twits['data']]\n",
    "# Since the sentiment scores are discrete, we'll scale the sentiments to 0 to 4 for use in our network\n",
    "sentiments = [twit['sentiment'] + 2 for twit in twits['data']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### プリプロセス関数の定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/cdsw/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "\n",
    "def preprocess(message):\n",
    "    \"\"\"\n",
    "    入力として文字列を受け取り、次の操作を実行する: \n",
    "        - 全てのアルファベットを小文字に変換\n",
    "        - URLを削除\n",
    "        - ティッカーシンボルを削除 \n",
    "        - 句読点を削除\n",
    "        - 文字列をスペースで分割しトークン化する\n",
    "        - シングル・キャラクターのトークンを削除\n",
    "    \n",
    "    パラメータ\n",
    "    ----------\n",
    "        message : 前処理の対象テキストメッセージ\n",
    "        \n",
    "    戻り値\n",
    "    -------\n",
    "        tokens: 前処理後のトークン配列\n",
    "    \"\"\" \n",
    "    #TODO: Implement \n",
    "    \n",
    "    # Lowercase the twit message\n",
    "    text = message.lower()\n",
    "    \n",
    "    # Replace URLs with a space in the message\n",
    "    text = re.sub(\"http(s)?://([\\w\\-]+\\.)+[\\w-]+(/[\\w\\- ./?%&=]*)?\",' ', text)\n",
    "    \n",
    "    # Replace ticker symbols with a space. The ticker symbols are any stock symbol that starts with $.\n",
    "    text = re.sub(\"\\$[^ \\t\\n\\r\\f]+\", ' ', text)\n",
    "    \n",
    "    # Replace StockTwits usernames with a space. The usernames are any word that starts with @.\n",
    "    text = re.sub(\"@[^ \\t\\n\\r\\f]+\", ' ', text)\n",
    "\n",
    "    # Replace everything not a letter with a space\n",
    "    text = re.sub(\"[^a-z]\", ' ', text)\n",
    "    \n",
    "    \n",
    "    # Tokenize by splitting the string on whitespace into a list of words\n",
    "    tokens = text.split()\n",
    "\n",
    "    # Lemmatize words using the WordNetLemmatizer. You can ignore any word that is not longer than one character.\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    tokens = [wnl.lemmatize(w, pos='v') for w in tokens if len(w) > 1]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitsメッセージ前処理\n",
    "上記で定義した`preprocess`関数を全てStockTwitメッセージ・データに適用します。\n",
    "\n",
    "※この処理には、データのサイズに応じて多少時間がかかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['heng', 'be', 'about', 'to', 'go', 'negative', 'get', 'ready', 'for', 'tomorrow'], ['if', 'you', 'watch', 'only', 'one', 'documentary', 'it', 'should', 'be', 'this', 'one'], ['if', 'you', 'watch', 'only', 'one', 'documentary', 'it', 'should', 'be', 'this', 'one']]\n",
      "590901\n"
     ]
    }
   ],
   "source": [
    "tokenized = list(map(preprocess, messages))\n",
    "\n",
    "print(tokenized[:3])\n",
    "print(len(tokenized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words\n",
    "\n",
    "すべてのメッセージがトークン化されたので、ボキャブラリ（語彙）データを作成します。\n",
    "その際に、コーパス全体で各単語が出現する頻度をカウントします\n",
    "（[`Counter`](https://docs.python.org/3.1/library/collections.html#collections.Counter)関数を利用）。\n",
    "\n",
    "※この処理には、データのサイズに応じて多少時間がかかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['heng', 'be', 'about', 'to', 'go', 'negative', 'get', 'ready', 'for', 'tomorrow', 'if', 'you', 'watch']\n",
      "7680210\n",
      "590901\n",
      "[[5949, 3, 83, 2, 37, 702, 51, 674, 9, 151], [100, 56, 208, 204, 106, 3350, 30, 216, 3, 17, 106], [100, 56, 208, 204, 106, 3350, 30, 216, 3, 17, 106]]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "#words = []\n",
    "#for tokens in tokenized:\n",
    "#    for token in tokens:\n",
    "#        words.append(token)\n",
    "out_list = tokenized\n",
    "words = [element for in_list in out_list for element in in_list]\n",
    "\n",
    "print(words[:13])\n",
    "print(len(words))\n",
    "\n",
    "\"\"\"\n",
    "Create a vocabulary by using Bag of words\n",
    "\"\"\"\n",
    "\n",
    "word_counts = Counter(words)\n",
    "sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "int_to_vocab = {ii: word for ii, word in enumerate(sorted_vocab)}\n",
    "vocab_to_int = {word:ii for ii, word in int_to_vocab.items()}\n",
    "\n",
    "bow = []\n",
    "for tokens in tokenized:\n",
    "    bow.append([vocab_to_int[token] for token in tokens])\n",
    "\n",
    "print(len(bow))\n",
    "print(bow[:3])\n",
    "\n",
    "# This BOW will not be used because it is not filtered to eliminate common words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 単語の重要性（メッセージに現れる頻度）に応じた調整\n",
    "\n",
    "ボキャブラリーを使用して、「the」、「and」、「it」などの最も一般的な単語の一部を削除します。\n",
    "これらの単語は非常に一般的であるため、センチメントを特定する目的に寄与せず、ニューラルネットワークへの入力のノイズとなります。これらを除外することで、ネットワークの学習時間を短縮することができます。\n",
    "\n",
    "また、非常に稀にしか用いられない単語も削除します。\n",
    "ここでは、各単語のカウントをメッセージの数で除算する必要があります。\n",
    "\n",
    "次に、メッセージのごく一部にしか表示されない単語を削除します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(sorted_vocab): 5950\n",
      "sorted_vocab - top: ['the', 'of', 'to']\n",
      "sorted_vocab - least: ['desktop', 'verge', 'catalysts', 'nyseamerican', 'driver', 'astronomically', 'edit', 'theme', 'ignore', 'particular', 'explode', 'autoscalp', 'yaawn', 'salamis', 'heng']\n",
      "freqs[the]: 0.027121654225600603\n",
      "high_cutoff: 20\n",
      "low_cutoff: 2e-06\n",
      "K_most_common: ['the', 'of', 'to', 'be', 'amp', 'utm', 'and', 'on', 'in', 'for', 'file', 'form', 'share', 'stock', 'by', 'sec', 'report', 'this', 'earn', 'at']\n",
      "len(filtered_words): 5929\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Set the following variables:\n",
    "    freqs\n",
    "    low_cutoff\n",
    "    high_cutoff\n",
    "    K_most_common\n",
    "\"\"\"\n",
    "\n",
    "print(\"len(sorted_vocab):\",len(sorted_vocab))\n",
    "print(\"sorted_vocab - top:\", sorted_vocab[:3])\n",
    "print(\"sorted_vocab - least:\", sorted_vocab[-15:])\n",
    "\n",
    "# Dictionart that contains the Frequency of words appearing in messages.\n",
    "# The key is the token and the value is the frequency of that word in the corpus.\n",
    "total_count = len(words)\n",
    "freqs = {word: count/total_count for word, count in word_counts.items()}\n",
    "\n",
    "#print(\"freqs[supplication]:\",freqs[\"supplication\"] )\n",
    "print(\"freqs[the]:\",freqs[\"the\"] )\n",
    "\n",
    "\"\"\"\n",
    "This was the post by Ricardo:\n",
    "\n",
    "there's no exact value for low_cutoff and high_cutoff, \n",
    "however I'd recommend you to use \n",
    "a low_cutoff that's around 0.000002 and 0.000007 \n",
    "(This depends on the values you get from your freqs calculations) and \n",
    "a high_cutofffrom 5 to 20 (this depends on the most_common values from the bow).\n",
    "\"\"\"\n",
    "\n",
    "# Float that is the frequency cutoff. Drop words with a frequency that is lower or equal to this number.\n",
    "low_cutoff = 0.000002\n",
    "\n",
    "# Integer that is the cut off for most common words. Drop words that are the `high_cutoff` most common words.\n",
    "\"\"\"\n",
    "example_count = []\n",
    "example_count.append(sorted_vocab.index(\"the\"))\n",
    "example_count.append(sorted_vocab.index(\"for\"))\n",
    "example_count.append(sorted_vocab.index(\"of\"))\n",
    "print(example_count)\n",
    "high_cutoff = min(example_count)\n",
    "\"\"\"\n",
    "high_cutoff = 20\n",
    "print(\"high_cutoff:\",high_cutoff)\n",
    "print(\"low_cutoff:\",low_cutoff)\n",
    "\n",
    "# The k most common words in the corpus. Use `high_cutoff` as the k.\n",
    "#K_most_common = [word for word in sorted_vocab[:high_cutoff]]\n",
    "K_most_common = sorted_vocab[:high_cutoff]\n",
    "\n",
    "print(\"K_most_common:\",K_most_common)\n",
    "\n",
    "\n",
    "filtered_words = [word for word in freqs if (freqs[word] > low_cutoff and word not in K_most_common)]\n",
    "\n",
    "print(\"len(filtered_words):\",len(filtered_words)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### フィルターされた単語を削除して語彙を更新\n",
    "ボキャブラリーに役立つ3つの変数を作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(tokenized): 590901\n",
      "len(filtered): 590901\n",
      "tokenized[:1] [['heng', 'be', 'about', 'to', 'go', 'negative', 'get', 'ready', 'for', 'tomorrow']]\n",
      "filtered[:1] [['about', 'go', 'negative', 'get', 'ready', 'tomorrow']]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Set the following variables:\n",
    "    vocab\n",
    "    id2vocab\n",
    "    filtered\n",
    "\"\"\"\n",
    "\n",
    "# A dictionary for the `filtered_words`. The key is the word and value is an id that represents the word. \n",
    "vocab =  {word:ii for ii, word in enumerate(filtered_words)}\n",
    "# Reverse of the `vocab` dictionary. The key is word id and value is the word. \n",
    "id2vocab = {ii:word for word, ii in vocab.items()}\n",
    "# tokenized with the words not in `filtered_words` removed.\n",
    "\n",
    "print(\"len(tokenized):\", len(tokenized))\n",
    "\n",
    "filtered = [[token for token in tokens if token in vocab] for tokens in tokenized]\n",
    "print(\"len(filtered):\", len(filtered))\n",
    "print(\"tokenized[:1]\", tokenized[:1])\n",
    "print(\"filtered[:1]\",filtered[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分類クラス間のバランス\n",
    "\n",
    "訓練データのラベルには、一般に偏りがあることがよく見受けられます（例外的なデータは少ない）。\n",
    "例えば、データの50％がニュートラルであること場合、毎回0（ニュートラル）を予測するだけで、ネットワークの精度が50％になることを意味します。\n",
    "\n",
    "ネットワークが適切に学習できるように、クラスのバランスを取る必要があります。つまり、それぞれのセンチメントスコアがデータにほぼ同じ頻度で表含まれていることが望ましいと言えます。\n",
    "\n",
    "ここでは、中立的な感情を持つデータを全体の20%になるように、ランダムにドロップします。\n",
    "\n",
    "データに含まれるニュートラルデータのパーセンテージと、データ削除により期待されるパーセンテージの値を使って、\n",
    "データをドロップする確率を求めます。\n",
    "\n",
    "同時に、長さが0のメッセージを削除します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep prob: 0.0562932821895086\n"
     ]
    }
   ],
   "source": [
    "balanced = {'messages': [], 'sentiments':[]}\n",
    "\n",
    "n_neutral = sum(1 for each in sentiments if each == 2)\n",
    "N_examples = len(sentiments)\n",
    "\n",
    "keep_prob = (N_examples - n_neutral)/4/n_neutral\n",
    "\n",
    "print(\"keep prob:\", keep_prob)\n",
    "\n",
    "for idx, sentiment in enumerate(sentiments):\n",
    "    message = filtered[idx]\n",
    "    if len(message) == 0:\n",
    "        # skip this message because it has length zero\n",
    "        continue\n",
    "    elif sentiment != 2 or random.random() < keep_prob:\n",
    "        balanced['messages'].append(message)\n",
    "        balanced['sentiments'].append(sentiment) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "バランスされたデータ中、センチメントが「ニュートラル」であるデータの割合を確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2128921529496288"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_neutral = sum(1 for each in balanced['sentiments'] if each == 2)\n",
    "N_examples = len(balanced['sentiments'])\n",
    "n_neutral/N_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's convert our tokens into integer ids which we can pass to the network.\n",
    "\n",
    "メッセージをID（数値）に変換します。この処理は、ニューラルネットワークの入力として用いるために必要です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = [[vocab[word] for word in message] for message in balanced['messages']]\n",
    "sentiments = balanced['sentiments']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ボキャブラリ・ファイルを保存します。このファイルは、予測の際に、入力を変換するために必要になります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('vocab.pickle', 'wb') as f:\n",
    "    pickle.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ニューラルネットワーク\n",
    "これでボキャブラリーができたので、トークンをIDに変換し、それをネットワークに渡すことができます。ネットワークを定義します\n",
    "\n",
    "下記は、ネットワークの概要です：\n",
    "\n",
    "#### Embed -> RNN -> Dense -> Softmax\n",
    "\n",
    "### SentimentClassifier (感情分類器)実装\n",
    "\n",
    "クラスは、3つの主要な部分で構成されています：: \n",
    "\n",
    "1. init function `__init__` \n",
    "2. forward pass `forward`  \n",
    "3. hidden state `init_hidden`. \n",
    "\n",
    "出力層では、softmaxを使用します。出力フォーマットによって出力層を選択します。\n",
    "\n",
    "（例えば、出力が２値/バイナリであれば、シグモイド関数）\n",
    "\n",
    "このネットワークでは、センチメントスコアには5つのクラスがあるためsoftmaxが適しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, lstm_size, output_size, lstm_layers=1, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "            vocab_size : The vocabulary size.\n",
    "            embed_size : The embedding layer size.\n",
    "            lstm_size : The LSTM layer size.\n",
    "            output_size : The output size.\n",
    "            lstm_layers : The number of LSTM layers.\n",
    "            dropout : The dropout probability.\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.lstm_size = lstm_size\n",
    "        self.output_size = output_size\n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embed_size)\n",
    "        self.lstm = nn.LSTM(self.embed_size, self.lstm_size, self.lstm_layers, dropout=self.dropout)\n",
    "        \n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(lstm_size, output_size)\n",
    "        \n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\" \n",
    "        Initializes hidden state\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "            batch_size : The size of batches.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "            hidden_state\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # 隠れ層として、n_layers x batch_size x hidden_dimの構造を持つテンソルを二つ作成し、ゼロで初期化\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        \n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        hidden = (weight.new(self.lstm_layers, batch_size,self.lstm_size).zero_(),\n",
    "                         weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_())\n",
    "        return hidden\n",
    "\n",
    "\n",
    "    def forward(self, nn_input, hidden_state):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on nn_input.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "            nn_input : The batch of input to the NN.\n",
    "            hidden_state : The LSTM hidden state.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            logps: log softmax output\n",
    "            hidden_state: The new hidden state.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = nn_input.size(0)\n",
    "        \n",
    "        # embed\n",
    "        embeds = self.embedding(nn_input)\n",
    "        \n",
    "        # LSTM\n",
    "        lstm_out, hidden_state = self.lstm(embeds, hidden_state)\n",
    "        \n",
    "        \"\"\"\n",
    "        remember here you do not have batch_first=True, \n",
    "        so accordingly shape your input. \n",
    "        Moreover, since now input is seq_length x batch you just need to transform lstm_out = lstm_out[-1,:,:].\n",
    "        you don't have to use batch_first=True in this case, \n",
    "        nor reshape the outputs with .view just transform your lstm_out as advised and you should be good to go.\n",
    "        \"\"\"\n",
    "        #lstm_out = lstm_out.contiguous().view(-1, self.lstm_size)    \n",
    "        lstm_out = lstm_out[-1,:,:]\n",
    "        \n",
    "        # dropout\n",
    "        out = self.dropout(lstm_out)\n",
    "        \n",
    "        # Dense Layer (nn.Linear) RNNの隠れ層から値を予測\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        # Softmax関数\n",
    "        logps = self.softmax(out)\n",
    "        \n",
    "        \n",
    "        return logps, hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルの確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.7733, -2.0665, -1.3319, -1.5904, -1.4448],\n",
      "        [-1.7649, -2.0427, -1.3049, -1.6599, -1.4365],\n",
      "        [-1.7764, -2.0752, -1.3315, -1.5847, -1.4433],\n",
      "        [-1.7762, -2.0626, -1.3176, -1.6049, -1.4486]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "model = SentimentClassifier(len(vocab), 10, 6, 5, dropout=0.1, lstm_layers=2)\n",
    "model.embedding.weight.data.uniform_(-1, 1)\n",
    "input = torch.randint(0, 1000, (5, 4), dtype=torch.int64)\n",
    "batch_size = 4\n",
    "hidden = model.init_hidden(4)\n",
    "\n",
    "logps, _ = model.forward(input, hidden)\n",
    "print(logps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### トレーニング\n",
    "### DataLoaderとバッチ処理\n",
    "ここで、データをループするために使用できるジェネレーターを構築します。\n",
    "\n",
    "効率化のため、シーケンスをバッチとして渡します。\n",
    "\n",
    "入力テンソルは次のような形になります：(sequence_length, batch_size)\n",
    "\n",
    "したがって、シーケンスが40トークンで、25シーケンスを渡す場合、入力サイズは(40, 25)になります。\n",
    "\n",
    "シーケンスの長さを40に設定した場合、40トークンより多いまたは少ないメッセージは、以下のように処理します。\n",
    "- 40トークン未満のメッセージの場合、空のスポットにゼロを埋め込む。\n",
    "   - データを処理する前にRNNが何も開始しないように、必ずパッドを残しておく必要がある。\n",
    "   - メッセージに20個のトークンがある場合、最初の20個のスポットは0になる。\n",
    "- メッセージに40個を超えるトークンがある場合、最初の40個のトークンを保持。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def dataloader(messages, labels, sequence_length=30, batch_size=32, shuffle=False):\n",
    "def dataloader(messages, labels, sequence_length=20, batch_size=32, shuffle=False):\n",
    "    \"\"\" \n",
    "    Build a dataloader.\n",
    "    \"\"\"\n",
    "    if shuffle:\n",
    "        indices = list(range(len(messages)))\n",
    "        random.shuffle(indices)\n",
    "        messages = [messages[idx] for idx in indices]\n",
    "        labels = [labels[idx] for idx in indices]\n",
    "\n",
    "    total_sequences = len(messages)\n",
    "\n",
    "    for ii in range(0, total_sequences, batch_size):\n",
    "        batch_messages = messages[ii: ii+batch_size]\n",
    "        \n",
    "        # First initialize a tensor of all zeros\n",
    "        batch = torch.zeros((sequence_length, len(batch_messages)), dtype=torch.int64)\n",
    "        for batch_num, tokens in enumerate(batch_messages):\n",
    "            token_tensor = torch.tensor(tokens)\n",
    "            # Left pad!\n",
    "            start_idx = max(sequence_length - len(token_tensor), 0)\n",
    "            batch[start_idx:, batch_num] = token_tensor[:sequence_length]\n",
    "        \n",
    "        label_tensor = torch.tensor(labels[ii: ii+len(batch_messages)])\n",
    "        \n",
    "        yield batch, label_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  データの分割（訓練用と検証用）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Split data into training and validation datasets. Use an appropriate split size.\n",
    "The features are the `token_ids` and the labels are the `sentiments`.\n",
    "\"\"\"   \n",
    "\n",
    "split_frac = 0.98 # for small data\n",
    "#split_frac = 0.8 # for big data\n",
    "\n",
    "## split data into training, validation, and test data (features and labels, x and y)\n",
    "\n",
    "split_idx = int(len(token_ids)*split_frac)\n",
    "train_features, remaining_features = token_ids[:split_idx], token_ids[split_idx:]\n",
    "train_labels, remaining_labels = sentiments[:split_idx], sentiments[split_idx:]\n",
    "\n",
    "test_idx = int(len(remaining_features)*0.5)\n",
    "valid_features, test_features = remaining_features[:test_idx], remaining_features[test_idx:]\n",
    "valid_labels, test_labels = remaining_labels[:test_idx], remaining_labels[test_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [ 3, 12, 12,  ..., 12, 12, 12],\n",
      "        [ 4, 13, 13,  ..., 13, 13, 13],\n",
      "        [ 5, 10, 10,  ..., 10, 10, 10]])\n",
      "(tensor([[[ 0.2675, -0.0482,  0.0554, -0.0824, -0.2360, -0.0629],\n",
      "         [ 0.0143, -0.0622, -0.0563, -0.1774,  0.0205, -0.1005],\n",
      "         [ 0.0143, -0.0622, -0.0563, -0.1774,  0.0205, -0.1005],\n",
      "         [ 0.0143, -0.0622, -0.0563, -0.1774,  0.0205, -0.1005],\n",
      "         [ 0.0143, -0.0622, -0.0563, -0.1774,  0.0205, -0.1005],\n",
      "         [ 0.0143, -0.0622, -0.0563, -0.1774,  0.0205, -0.1005],\n",
      "         [ 0.0143, -0.0622, -0.0563, -0.1774,  0.0205, -0.1005],\n",
      "         [ 0.0143, -0.0622, -0.0563, -0.1774,  0.0205, -0.1005],\n",
      "         [ 0.0143, -0.0622, -0.0563, -0.1774,  0.0205, -0.1005],\n",
      "         [ 0.0143, -0.0622, -0.0563, -0.1774,  0.0205, -0.1005],\n",
      "         [ 0.0143, -0.0622, -0.0563, -0.1774,  0.0205, -0.1005],\n",
      "         [ 0.0143, -0.0622, -0.0563, -0.1774,  0.0205, -0.1005],\n",
      "         [ 0.0143, -0.0622, -0.0563, -0.1774,  0.0205, -0.1005],\n",
      "         [ 0.0143, -0.0622, -0.0563, -0.1774,  0.0205, -0.1005],\n",
      "         [ 0.0143, -0.0622, -0.0563, -0.1774,  0.0205, -0.1005],\n",
      "         [ 0.0143, -0.0622, -0.0563, -0.1774,  0.0205, -0.1005],\n",
      "         [ 0.0143, -0.0622, -0.0563, -0.1774,  0.0205, -0.1005],\n",
      "         [ 0.0143, -0.0622, -0.0563, -0.1774,  0.0205, -0.1005],\n",
      "         [ 0.0143, -0.0622, -0.0563, -0.1774,  0.0205, -0.1005],\n",
      "         [ 0.0143, -0.0622, -0.0563, -0.1774,  0.0205, -0.1005],\n",
      "         [ 0.0143, -0.0622, -0.0563, -0.1774,  0.0205, -0.1005],\n",
      "         [ 0.0143, -0.0622, -0.0563, -0.1774,  0.0205, -0.1005],\n",
      "         [ 0.0143, -0.0622, -0.0563, -0.1774,  0.0205, -0.1005],\n",
      "         [ 0.0143, -0.0622, -0.0563, -0.1774,  0.0205, -0.1005],\n",
      "         [ 0.0143, -0.0622, -0.0563, -0.1774,  0.0205, -0.1005],\n",
      "         [ 0.0143, -0.0622, -0.0563, -0.1774,  0.0205, -0.1005],\n",
      "         [ 0.0143, -0.0622, -0.0563, -0.1774,  0.0205, -0.1005],\n",
      "         [ 0.0143, -0.0622, -0.0563, -0.1774,  0.0205, -0.1005],\n",
      "         [ 0.0143, -0.0622, -0.0563, -0.1774,  0.0205, -0.1005],\n",
      "         [ 0.0143, -0.0622, -0.0563, -0.1774,  0.0205, -0.1005],\n",
      "         [ 0.0143, -0.0622, -0.0563, -0.1774,  0.0205, -0.1005],\n",
      "         [ 0.0143, -0.0622, -0.0563, -0.1774,  0.0205, -0.1005],\n",
      "         [ 0.0143, -0.0622, -0.0563, -0.1774,  0.0205, -0.1005],\n",
      "         [ 0.0143, -0.0622, -0.0563, -0.1774,  0.0205, -0.1005],\n",
      "         [ 0.0143, -0.0622, -0.0563, -0.1774,  0.0205, -0.1005],\n",
      "         [ 0.0143, -0.0622, -0.0563, -0.1774,  0.0205, -0.1005],\n",
      "         [ 0.0143, -0.0622, -0.0563, -0.1774,  0.0205, -0.1005],\n",
      "         [ 0.0143, -0.0622, -0.0563, -0.1774,  0.0205, -0.1005],\n",
      "         [ 0.0143, -0.0622, -0.0563, -0.1774,  0.0205, -0.1005],\n",
      "         [ 0.0143, -0.0622, -0.0563, -0.1774,  0.0205, -0.1005],\n",
      "         [ 0.0143, -0.0622, -0.0563, -0.1774,  0.0205, -0.1005],\n",
      "         [ 0.0143, -0.0622, -0.0563, -0.1774,  0.0205, -0.1005],\n",
      "         [ 0.0143, -0.0622, -0.0563, -0.1774,  0.0205, -0.1005],\n",
      "         [ 0.0143, -0.0622, -0.0563, -0.1774,  0.0205, -0.1005],\n",
      "         [ 0.0143, -0.0622, -0.0563, -0.1774,  0.0205, -0.1005],\n",
      "         [ 0.0143, -0.0622, -0.0563, -0.1774,  0.0205, -0.1005],\n",
      "         [ 0.0143, -0.0622, -0.0563, -0.1774,  0.0205, -0.1005],\n",
      "         [ 0.0143, -0.0622, -0.0563, -0.1774,  0.0205, -0.1005],\n",
      "         [ 0.0143, -0.0622, -0.0563, -0.1774,  0.0205, -0.1005],\n",
      "         [ 0.0143, -0.0622, -0.0563, -0.1774,  0.0205, -0.1005],\n",
      "         [ 0.0143, -0.0622, -0.0563, -0.1774,  0.0205, -0.1005],\n",
      "         [ 0.0143, -0.0622, -0.0563, -0.1774,  0.0205, -0.1005],\n",
      "         [ 0.0143, -0.0622, -0.0563, -0.1774,  0.0205, -0.1005],\n",
      "         [ 0.0143, -0.0622, -0.0563, -0.1774,  0.0205, -0.1005],\n",
      "         [ 0.0143, -0.0622, -0.0563, -0.1774,  0.0205, -0.1005],\n",
      "         [ 0.0143, -0.0622, -0.0563, -0.1774,  0.0205, -0.1005],\n",
      "         [ 0.0143, -0.0622, -0.0563, -0.1774,  0.0205, -0.1005],\n",
      "         [ 0.0143, -0.0622, -0.0563, -0.1774,  0.0205, -0.1005],\n",
      "         [ 0.0143, -0.0622, -0.0563, -0.1774,  0.0205, -0.1005],\n",
      "         [ 0.0143, -0.0622, -0.0563, -0.1774,  0.0205, -0.1005],\n",
      "         [ 0.0143, -0.0622, -0.0563, -0.1774,  0.0205, -0.1005],\n",
      "         [ 0.0143, -0.0622, -0.0563, -0.1774,  0.0205, -0.1005],\n",
      "         [ 0.0143, -0.0622, -0.0563, -0.1774,  0.0205, -0.1005],\n",
      "         [ 0.0143, -0.0622, -0.0563, -0.1774,  0.0205, -0.1005]],\n",
      "\n",
      "        [[ 0.1882, -0.1304, -0.0919,  0.1329, -0.0223, -0.0900],\n",
      "         [ 0.1516, -0.1343, -0.0533,  0.1467,  0.0460, -0.1163],\n",
      "         [ 0.1635, -0.1350, -0.0600,  0.1535,  0.0418, -0.1180],\n",
      "         [ 0.1473, -0.1362, -0.0578,  0.1376,  0.0496, -0.1170],\n",
      "         [ 0.1490, -0.1346, -0.0574,  0.1398,  0.0469, -0.1180],\n",
      "         [ 0.1555, -0.1390, -0.0610,  0.1434,  0.0440, -0.1159],\n",
      "         [ 0.1568, -0.1324, -0.0485,  0.1399,  0.0373, -0.1077],\n",
      "         [ 0.1453, -0.1407, -0.0537,  0.1390,  0.0586, -0.1190],\n",
      "         [ 0.1634, -0.1384, -0.0548,  0.1603,  0.0355, -0.1096],\n",
      "         [ 0.1498, -0.1386, -0.0570,  0.1397,  0.0516, -0.1189],\n",
      "         [ 0.1495, -0.1352, -0.0531,  0.1462,  0.0506, -0.1175],\n",
      "         [ 0.1624, -0.1404, -0.0636,  0.1590,  0.0488, -0.1174],\n",
      "         [ 0.1480, -0.1355, -0.0524,  0.1422,  0.0512, -0.1188],\n",
      "         [ 0.1477, -0.1270, -0.0450,  0.1332,  0.0438, -0.1146],\n",
      "         [ 0.1495, -0.1379, -0.0554,  0.1503,  0.0564, -0.1164],\n",
      "         [ 0.1490, -0.1345, -0.0575,  0.1397,  0.0466, -0.1181],\n",
      "         [ 0.1475, -0.1353, -0.0530,  0.1416,  0.0507, -0.1191],\n",
      "         [ 0.1450, -0.1389, -0.0591,  0.1359,  0.0551, -0.1157],\n",
      "         [ 0.1484, -0.1340, -0.0553,  0.1410,  0.0498, -0.1183],\n",
      "         [ 0.1464, -0.1367, -0.0507,  0.1390,  0.0537, -0.1208],\n",
      "         [ 0.1641, -0.1328, -0.0611,  0.1535,  0.0390, -0.1174],\n",
      "         [ 0.1508, -0.1369, -0.0579,  0.1402,  0.0472, -0.1178],\n",
      "         [ 0.1484, -0.1334, -0.0537,  0.1419,  0.0477, -0.1176],\n",
      "         [ 0.1476, -0.1348, -0.0526,  0.1411,  0.0504, -0.1191],\n",
      "         [ 0.1474, -0.1356, -0.0531,  0.1409,  0.0511, -0.1188],\n",
      "         [ 0.1478, -0.1345, -0.0550,  0.1433,  0.0486, -0.1179],\n",
      "         [ 0.1620, -0.1274, -0.0518,  0.1422,  0.0350, -0.1149],\n",
      "         [ 0.1485, -0.1263, -0.0455,  0.1323,  0.0407, -0.1132],\n",
      "         [ 0.1508, -0.1349, -0.0563,  0.1433,  0.0463, -0.1176],\n",
      "         [ 0.1501, -0.1362, -0.0584,  0.1384,  0.0465, -0.1176],\n",
      "         [ 0.1479, -0.1333, -0.0541,  0.1408,  0.0475, -0.1178],\n",
      "         [ 0.1481, -0.1336, -0.0536,  0.1421,  0.0482, -0.1182],\n",
      "         [ 0.1467, -0.1372, -0.0561,  0.1425,  0.0535, -0.1178],\n",
      "         [ 0.1620, -0.1386, -0.0587,  0.1513,  0.0480, -0.1216],\n",
      "         [ 0.1480, -0.1274, -0.0475,  0.1326,  0.0425, -0.1142],\n",
      "         [ 0.1488, -0.1342, -0.0528,  0.1441,  0.0510, -0.1188],\n",
      "         [ 0.1488, -0.1338, -0.0541,  0.1419,  0.0458, -0.1170],\n",
      "         [ 0.1477, -0.1336, -0.0544,  0.1404,  0.0479, -0.1176],\n",
      "         [ 0.1478, -0.1368, -0.0569,  0.1246,  0.0482, -0.1115],\n",
      "         [ 0.1478, -0.1339, -0.0541,  0.1409,  0.0486, -0.1177],\n",
      "         [ 0.1478, -0.1338, -0.0543,  0.1414,  0.0481, -0.1178],\n",
      "         [ 0.1464, -0.1372, -0.0508,  0.1390,  0.0543, -0.1207],\n",
      "         [ 0.1490, -0.1328, -0.0551,  0.1430,  0.0471, -0.1165],\n",
      "         [ 0.1468, -0.1343, -0.0556,  0.1422,  0.0485, -0.1180],\n",
      "         [ 0.1475, -0.1267, -0.0454,  0.1318,  0.0423, -0.1144],\n",
      "         [ 0.1591, -0.1378, -0.0550,  0.1551,  0.0412, -0.1122],\n",
      "         [ 0.1490, -0.1342, -0.0556,  0.1417,  0.0470, -0.1176],\n",
      "         [ 0.1481, -0.1335, -0.0537,  0.1418,  0.0480, -0.1180],\n",
      "         [ 0.1485, -0.1287, -0.0489,  0.1313,  0.0436, -0.1145],\n",
      "         [ 0.1485, -0.1346, -0.0568,  0.1392,  0.0468, -0.1170],\n",
      "         [ 0.1496, -0.1272, -0.0467,  0.1337,  0.0401, -0.1134],\n",
      "         [ 0.1498, -0.1349, -0.0555,  0.1428,  0.0472, -0.1169],\n",
      "         [ 0.1749, -0.1364, -0.0632,  0.1644,  0.0294, -0.1107],\n",
      "         [ 0.1507, -0.1352, -0.0527,  0.1431,  0.0480, -0.1166],\n",
      "         [ 0.1467, -0.1357, -0.0556,  0.1389,  0.0510, -0.1174],\n",
      "         [ 0.1479, -0.1334, -0.0538,  0.1412,  0.0480, -0.1179],\n",
      "         [ 0.1468, -0.1369, -0.0550,  0.1439,  0.0573, -0.1178],\n",
      "         [ 0.1464, -0.1357, -0.0553,  0.1400,  0.0526, -0.1180],\n",
      "         [ 0.1467, -0.1356, -0.0556,  0.1386,  0.0508, -0.1174],\n",
      "         [ 0.1483, -0.1348, -0.0537,  0.1452,  0.0552, -0.1179],\n",
      "         [ 0.1513, -0.1346, -0.0531,  0.1483,  0.0498, -0.1161],\n",
      "         [ 0.1483, -0.1314, -0.0543,  0.1439,  0.0523, -0.1176],\n",
      "         [ 0.1487, -0.1340, -0.0532,  0.1438,  0.0494, -0.1177],\n",
      "         [ 0.1475, -0.1372, -0.0571,  0.1416,  0.0511, -0.1155]]],\n",
      "       grad_fn=<StackBackward>), tensor([[[ 0.5853, -0.1957,  0.1049, -0.1292, -0.5178, -0.1807],\n",
      "         [ 0.0265, -0.1763, -0.1008, -0.4734,  0.0538, -0.3235],\n",
      "         [ 0.0265, -0.1763, -0.1008, -0.4734,  0.0538, -0.3235],\n",
      "         [ 0.0265, -0.1763, -0.1008, -0.4734,  0.0538, -0.3235],\n",
      "         [ 0.0265, -0.1763, -0.1008, -0.4734,  0.0538, -0.3235],\n",
      "         [ 0.0265, -0.1763, -0.1008, -0.4734,  0.0538, -0.3235],\n",
      "         [ 0.0265, -0.1763, -0.1008, -0.4734,  0.0538, -0.3235],\n",
      "         [ 0.0265, -0.1763, -0.1008, -0.4734,  0.0538, -0.3235],\n",
      "         [ 0.0265, -0.1763, -0.1008, -0.4734,  0.0538, -0.3235],\n",
      "         [ 0.0265, -0.1763, -0.1008, -0.4734,  0.0538, -0.3235],\n",
      "         [ 0.0265, -0.1763, -0.1008, -0.4734,  0.0538, -0.3235],\n",
      "         [ 0.0265, -0.1763, -0.1008, -0.4734,  0.0538, -0.3235],\n",
      "         [ 0.0265, -0.1763, -0.1008, -0.4734,  0.0538, -0.3235],\n",
      "         [ 0.0265, -0.1763, -0.1008, -0.4734,  0.0538, -0.3235],\n",
      "         [ 0.0265, -0.1763, -0.1008, -0.4734,  0.0538, -0.3235],\n",
      "         [ 0.0265, -0.1763, -0.1008, -0.4734,  0.0538, -0.3235],\n",
      "         [ 0.0265, -0.1763, -0.1008, -0.4734,  0.0538, -0.3235],\n",
      "         [ 0.0265, -0.1763, -0.1008, -0.4734,  0.0538, -0.3235],\n",
      "         [ 0.0265, -0.1763, -0.1008, -0.4734,  0.0538, -0.3235],\n",
      "         [ 0.0265, -0.1763, -0.1008, -0.4734,  0.0538, -0.3235],\n",
      "         [ 0.0265, -0.1763, -0.1008, -0.4734,  0.0538, -0.3235],\n",
      "         [ 0.0265, -0.1763, -0.1008, -0.4734,  0.0538, -0.3235],\n",
      "         [ 0.0265, -0.1763, -0.1008, -0.4734,  0.0538, -0.3235],\n",
      "         [ 0.0265, -0.1763, -0.1008, -0.4734,  0.0538, -0.3235],\n",
      "         [ 0.0265, -0.1763, -0.1008, -0.4734,  0.0538, -0.3235],\n",
      "         [ 0.0265, -0.1763, -0.1008, -0.4734,  0.0538, -0.3235],\n",
      "         [ 0.0265, -0.1763, -0.1008, -0.4734,  0.0538, -0.3235],\n",
      "         [ 0.0265, -0.1763, -0.1008, -0.4734,  0.0538, -0.3235],\n",
      "         [ 0.0265, -0.1763, -0.1008, -0.4734,  0.0538, -0.3235],\n",
      "         [ 0.0265, -0.1763, -0.1008, -0.4734,  0.0538, -0.3235],\n",
      "         [ 0.0265, -0.1763, -0.1008, -0.4734,  0.0538, -0.3235],\n",
      "         [ 0.0265, -0.1763, -0.1008, -0.4734,  0.0538, -0.3235],\n",
      "         [ 0.0265, -0.1763, -0.1008, -0.4734,  0.0538, -0.3235],\n",
      "         [ 0.0265, -0.1763, -0.1008, -0.4734,  0.0538, -0.3235],\n",
      "         [ 0.0265, -0.1763, -0.1008, -0.4734,  0.0538, -0.3235],\n",
      "         [ 0.0265, -0.1763, -0.1008, -0.4734,  0.0538, -0.3235],\n",
      "         [ 0.0265, -0.1763, -0.1008, -0.4734,  0.0538, -0.3235],\n",
      "         [ 0.0265, -0.1763, -0.1008, -0.4734,  0.0538, -0.3235],\n",
      "         [ 0.0265, -0.1763, -0.1008, -0.4734,  0.0538, -0.3235],\n",
      "         [ 0.0265, -0.1763, -0.1008, -0.4734,  0.0538, -0.3235],\n",
      "         [ 0.0265, -0.1763, -0.1008, -0.4734,  0.0538, -0.3235],\n",
      "         [ 0.0265, -0.1763, -0.1008, -0.4734,  0.0538, -0.3235],\n",
      "         [ 0.0265, -0.1763, -0.1008, -0.4734,  0.0538, -0.3235],\n",
      "         [ 0.0265, -0.1763, -0.1008, -0.4734,  0.0538, -0.3235],\n",
      "         [ 0.0265, -0.1763, -0.1008, -0.4734,  0.0538, -0.3235],\n",
      "         [ 0.0265, -0.1763, -0.1008, -0.4734,  0.0538, -0.3235],\n",
      "         [ 0.0265, -0.1763, -0.1008, -0.4734,  0.0538, -0.3235],\n",
      "         [ 0.0265, -0.1763, -0.1008, -0.4734,  0.0538, -0.3235],\n",
      "         [ 0.0265, -0.1763, -0.1008, -0.4734,  0.0538, -0.3235],\n",
      "         [ 0.0265, -0.1763, -0.1008, -0.4734,  0.0538, -0.3235],\n",
      "         [ 0.0265, -0.1763, -0.1008, -0.4734,  0.0538, -0.3235],\n",
      "         [ 0.0265, -0.1763, -0.1008, -0.4734,  0.0538, -0.3235],\n",
      "         [ 0.0265, -0.1763, -0.1008, -0.4734,  0.0538, -0.3235],\n",
      "         [ 0.0265, -0.1763, -0.1008, -0.4734,  0.0538, -0.3235],\n",
      "         [ 0.0265, -0.1763, -0.1008, -0.4734,  0.0538, -0.3235],\n",
      "         [ 0.0265, -0.1763, -0.1008, -0.4734,  0.0538, -0.3235],\n",
      "         [ 0.0265, -0.1763, -0.1008, -0.4734,  0.0538, -0.3235],\n",
      "         [ 0.0265, -0.1763, -0.1008, -0.4734,  0.0538, -0.3235],\n",
      "         [ 0.0265, -0.1763, -0.1008, -0.4734,  0.0538, -0.3235],\n",
      "         [ 0.0265, -0.1763, -0.1008, -0.4734,  0.0538, -0.3235],\n",
      "         [ 0.0265, -0.1763, -0.1008, -0.4734,  0.0538, -0.3235],\n",
      "         [ 0.0265, -0.1763, -0.1008, -0.4734,  0.0538, -0.3235],\n",
      "         [ 0.0265, -0.1763, -0.1008, -0.4734,  0.0538, -0.3235],\n",
      "         [ 0.0265, -0.1763, -0.1008, -0.4734,  0.0538, -0.3235]],\n",
      "\n",
      "        [[ 0.3879, -0.2567, -0.2600,  0.2341, -0.0363, -0.2243],\n",
      "         [ 0.3465, -0.2507, -0.1320,  0.2725,  0.0749, -0.2644],\n",
      "         [ 0.3604, -0.2598, -0.1444,  0.2770,  0.0684, -0.2567],\n",
      "         [ 0.3360, -0.2547, -0.1431,  0.2545,  0.0809, -0.2658],\n",
      "         [ 0.3402, -0.2515, -0.1423,  0.2587,  0.0764, -0.2680],\n",
      "         [ 0.3527, -0.2581, -0.1542,  0.2661,  0.0718, -0.2670],\n",
      "         [ 0.3688, -0.2523, -0.1232,  0.2612,  0.0610, -0.2488],\n",
      "         [ 0.3315, -0.2634, -0.1314,  0.2555,  0.0953, -0.2731],\n",
      "         [ 0.3786, -0.2592, -0.1380,  0.2986,  0.0572, -0.2495],\n",
      "         [ 0.3392, -0.2571, -0.1426,  0.2596,  0.0847, -0.2727],\n",
      "         [ 0.3416, -0.2524, -0.1314,  0.2716,  0.0824, -0.2669],\n",
      "         [ 0.3591, -0.2707, -0.1524,  0.2853,  0.0793, -0.2590],\n",
      "         [ 0.3375, -0.2530, -0.1292,  0.2636,  0.0836, -0.2696],\n",
      "         [ 0.3434, -0.2406, -0.1128,  0.2487,  0.0722, -0.2649],\n",
      "         [ 0.3439, -0.2579, -0.1372,  0.2806,  0.0920, -0.2639],\n",
      "         [ 0.3401, -0.2513, -0.1425,  0.2586,  0.0759, -0.2683],\n",
      "         [ 0.3363, -0.2526, -0.1307,  0.2624,  0.0828, -0.2702],\n",
      "         [ 0.3300, -0.2597, -0.1449,  0.2492,  0.0893, -0.2661],\n",
      "         [ 0.3394, -0.2501, -0.1365,  0.2618,  0.0812, -0.2679],\n",
      "         [ 0.3340, -0.2557, -0.1249,  0.2576,  0.0880, -0.2736],\n",
      "         [ 0.3606, -0.2561, -0.1479,  0.2773,  0.0638, -0.2556],\n",
      "         [ 0.3403, -0.2535, -0.1451,  0.2599,  0.0773, -0.2715],\n",
      "         [ 0.3381, -0.2488, -0.1326,  0.2630,  0.0778, -0.2672],\n",
      "         [ 0.3363, -0.2517, -0.1299,  0.2614,  0.0823, -0.2703],\n",
      "         [ 0.3360, -0.2534, -0.1311,  0.2612,  0.0835, -0.2695],\n",
      "         [ 0.3373, -0.2511, -0.1359,  0.2658,  0.0792, -0.2681],\n",
      "         [ 0.3637, -0.2490, -0.1264,  0.2581,  0.0579, -0.2541],\n",
      "         [ 0.3451, -0.2393, -0.1140,  0.2470,  0.0671, -0.2617],\n",
      "         [ 0.3447, -0.2522, -0.1398,  0.2656,  0.0754, -0.2671],\n",
      "         [ 0.3395, -0.2521, -0.1462,  0.2569,  0.0760, -0.2700],\n",
      "         [ 0.3370, -0.2486, -0.1337,  0.2608,  0.0774, -0.2678],\n",
      "         [ 0.3376, -0.2492, -0.1325,  0.2635,  0.0786, -0.2686],\n",
      "         [ 0.3352, -0.2566, -0.1389,  0.2644,  0.0873, -0.2676],\n",
      "         [ 0.3591, -0.2676, -0.1408,  0.2736,  0.0790, -0.2630],\n",
      "         [ 0.3455, -0.2417, -0.1191,  0.2483,  0.0700, -0.2631],\n",
      "         [ 0.3387, -0.2512, -0.1311,  0.2677,  0.0833, -0.2696],\n",
      "         [ 0.3393, -0.2495, -0.1338,  0.2631,  0.0746, -0.2659],\n",
      "         [ 0.3364, -0.2491, -0.1343,  0.2601,  0.0782, -0.2673],\n",
      "         [ 0.3404, -0.2582, -0.1439,  0.2304,  0.0794, -0.2648],\n",
      "         [ 0.3368, -0.2499, -0.1336,  0.2610,  0.0793, -0.2674],\n",
      "         [ 0.3368, -0.2495, -0.1341,  0.2621,  0.0784, -0.2678],\n",
      "         [ 0.3339, -0.2566, -0.1252,  0.2576,  0.0891, -0.2734],\n",
      "         [ 0.3389, -0.2483, -0.1369,  0.2657,  0.0768, -0.2648],\n",
      "         [ 0.3359, -0.2506, -0.1374,  0.2643,  0.0791, -0.2675],\n",
      "         [ 0.3427, -0.2400, -0.1137,  0.2460,  0.0698, -0.2644],\n",
      "         [ 0.3673, -0.2581, -0.1378,  0.2881,  0.0667, -0.2550],\n",
      "         [ 0.3401, -0.2504, -0.1374,  0.2625,  0.0765, -0.2674],\n",
      "         [ 0.3374, -0.2489, -0.1327,  0.2628,  0.0782, -0.2683],\n",
      "         [ 0.3459, -0.2447, -0.1230,  0.2449,  0.0719, -0.2643],\n",
      "         [ 0.3386, -0.2514, -0.1405,  0.2577,  0.0763, -0.2658],\n",
      "         [ 0.3483, -0.2411, -0.1173,  0.2498,  0.0661, -0.2622],\n",
      "         [ 0.3422, -0.2519, -0.1374,  0.2647,  0.0769, -0.2657],\n",
      "         [ 0.3888, -0.2641, -0.1552,  0.2974,  0.0477, -0.2410],\n",
      "         [ 0.3441, -0.2525, -0.1305,  0.2654,  0.0782, -0.2645],\n",
      "         [ 0.3344, -0.2536, -0.1373,  0.2572,  0.0832, -0.2668],\n",
      "         [ 0.3370, -0.2489, -0.1330,  0.2617,  0.0782, -0.2680],\n",
      "         [ 0.3353, -0.2559, -0.1359,  0.2671,  0.0937, -0.2675],\n",
      "         [ 0.3348, -0.2535, -0.1366,  0.2601,  0.0858, -0.2672],\n",
      "         [ 0.3343, -0.2532, -0.1375,  0.2566,  0.0829, -0.2667],\n",
      "         [ 0.3378, -0.2525, -0.1332,  0.2699,  0.0904, -0.2674],\n",
      "         [ 0.3460, -0.2511, -0.1315,  0.2757,  0.0811, -0.2639],\n",
      "         [ 0.3379, -0.2455, -0.1344,  0.2679,  0.0853, -0.2660],\n",
      "         [ 0.3403, -0.2500, -0.1313,  0.2675,  0.0806, -0.2665],\n",
      "         [ 0.3362, -0.2559, -0.1403,  0.2603,  0.0827, -0.2659]]],\n",
      "       grad_fn=<StackBackward>))\n"
     ]
    }
   ],
   "source": [
    "#　色々試したみただけのエリア（のはず）\n",
    "#text_batch, labels = next(iter(dataloader(train_features, train_labels, sequence_length=20, batch_size=64)))\n",
    "#print(text_batch)\n",
    "#hidden = model.init_hidden(64)\n",
    "#logps, hidden = model.forward(text_batch, hidden)\n",
    "#print(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### トレーニング準備\n",
    "\n",
    "利用可能なデバイス(CUDA/GPUまたはGPU)を確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentClassifier(\n",
       "  (embedding): Embedding(5930, 1024)\n",
       "  (lstm): LSTM(1024, 512, num_layers=2, dropout=0.2)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (fc): Linear(in_features=512, out_features=5, bias=True)\n",
       "  (softmax): LogSoftmax()\n",
       ")"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model = SentimentClassifier(len(vocab)+1, 200, 128, 5, dropout=0.)\n",
    "model = SentimentClassifier(len(vocab)+1, 1024, 512, 5, lstm_layers=2, dropout=0.2)\n",
    "model.embedding.weight.data.uniform_(-1, 1)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### トレーニング実施\n",
    "\n",
    "トレーニングを実行します。モデルの訓練の進行度合を確認するために、定期的にLossを出力します。\n",
    "\n",
    "※この処理には、データのサイズに応じて、十分な時間が必要です。\n",
    "\n",
    "GPUを備えた環境で実行する場合、ターミナルで以下のコマンドを実行することで、GPUが利用されていることを確認することができます（ GPU実行中、コマンド実行により表示されるテーブルの右上のVolatile GPU-Utilのパーセンテージ値が増えます）\n",
    "```\n",
    "$ watch nvidia-smi\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "Epoch: 1/5... Step: 100... Loss: 0.017694... Total Loss: 0.041833\n",
      "Epoch: 1/5... Step: 200... Loss: 0.000830... Total Loss: 0.024216\n",
      "Starting epoch 2\n",
      "Epoch: 2/5... Step: 100... Loss: 0.011427... Total Loss: 0.015538\n",
      "Epoch: 2/5... Step: 200... Loss: 0.000126... Total Loss: 0.012561\n",
      "Starting epoch 3\n",
      "Epoch: 3/5... Step: 100... Loss: 0.001006... Total Loss: 0.009969\n",
      "Epoch: 3/5... Step: 200... Loss: 0.000395... Total Loss: 0.008784\n",
      "Starting epoch 4\n",
      "Epoch: 4/5... Step: 100... Loss: 0.000173... Total Loss: 0.007565\n",
      "Epoch: 4/5... Step: 200... Loss: 0.000247... Total Loss: 0.006909\n",
      "Starting epoch 5\n",
      "Epoch: 5/5... Step: 100... Loss: 0.000030... Total Loss: 0.006198\n",
      "Epoch: 5/5... Step: 200... Loss: 0.000130... Total Loss: 0.005765\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "epochs = 5\n",
    "batch_size =  64\n",
    "batch_size =  512\n",
    "learning_rate = 0.001\n",
    "\n",
    "print_every = 100\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "model.train()\n",
    "\n",
    "#val_losses = []\n",
    "total_losses = []\n",
    "#accuracy = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Starting epoch {}'.format(epoch + 1))\n",
    "    \n",
    "    steps = 0\n",
    "    for text_batch, labels in dataloader(\n",
    "            train_features, train_labels, batch_size=batch_size, sequence_length=20, shuffle=True):\n",
    "        steps += 1\n",
    "        hidden = model.init_hidden(labels.shape[0]) \n",
    "        \n",
    "        # デバイス(CPU, GPU)の設定\n",
    "        text_batch, labels = text_batch.to(device), labels.to(device)\n",
    "        for each in hidden:\n",
    "            each.to(device)\n",
    "        \n",
    "        # モデルのトレーニング\n",
    "        hidden = tuple([each.data for each in hidden])\n",
    "        model.zero_grad()\n",
    "        output, hidden = model(text_batch, hidden)\n",
    "        loss = criterion(output.squeeze(), labels)\n",
    "        loss.backward()\n",
    "        clip = 5\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate loss\n",
    "        #val_losses.append(loss.item())\n",
    "        total_losses.append(loss.item())\n",
    "        \n",
    "        correct_count = 0.0\n",
    "        if steps % print_every == 0:\n",
    "            model.eval()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            ps = torch.exp(output)\n",
    "            top_p, top_class = ps.topk(1, dim=1)\n",
    "            #?top_class = top_class.to(device)\n",
    "            #?labels = labels.to(device)\n",
    "\n",
    "            correct_count += torch.sum(top_class.squeeze()== labels)\n",
    "            #accuracy.append(100*correct_count/len(labels))\n",
    "            \n",
    "            # TODO Implement: Print metrics\n",
    "            print(\"Epoch: {}/{}...\".format(epoch+1, epochs),\n",
    "                 \"Step: {}...\".format(steps),\n",
    "                 \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                 \"Total Loss: {:.6f}\".format(np.mean(total_losses)),\n",
    "                 #\"Collect Count: {}\".format(correct_count),\n",
    "                 #\"Accuracy: {:.2f}\".format((100*correct_count/len(labels))),\n",
    "                 # AttributeError: 'torch.dtype' object has no attribute 'type'\n",
    "                 #\"Accuracy Avg: {:.2f}\".format(np.mean(accuracy))\n",
    "                 )\n",
    "            \n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'state_dict': model.state_dict()}, 'checkpoint.pth.tar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 予測（Prediction）関数の作成\n",
    "\n",
    "訓練されたモデルを使って、入力されたテキストから予測結果を生成するpredict関数を実装します。\n",
    "\n",
    "テキストは、ネットワークに渡される前に前処理される必要があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/cdsw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/cdsw/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import pickle\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "cur_dir = os.path.dirname(os.path.abspath('__file__'))\n",
    "print(cur_dir)\n",
    "sys.path.append(cur_dir)\n",
    "\n",
    "vocab_filename = 'vocab.pickle'\n",
    "vocab_path = cur_dir + \"/\" + vocab_filename\n",
    "vocab_l = pickle.load(open(vocab_path, 'rb'))\n",
    "\n",
    "#model_path = cur_dir + \"/\" + \"model.torch\"\n",
    "#model_l = torch.load(model_path, map_location='cpu')\n",
    "\n",
    "model_l = SentimentClassifier(len(vocab_l)+1, 1024, 512, 5, lstm_layers=2, dropout=0.2)\n",
    "checkpoint = torch.load('./checkpoint.pth.tar')\n",
    "model_l.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "class UnknownWordsError(Exception):\n",
    "  \"Only unknown words are included in text\"\n",
    "\n",
    "\n",
    "def predict_func(text, model, vocab):\n",
    "    \"\"\" \n",
    "    Make a prediction on a single sentence.\n",
    "    Parameters\n",
    "    ----------\n",
    "        text : The string to make a prediction on.\n",
    "        model : The model to use for making the prediction.\n",
    "        vocab : Dictionary for word to word ids. The key is the word and the value is the word id.\n",
    "    Returns\n",
    "    -------\n",
    "        pred : 予測値（numpyベクトル）\n",
    "    \"\"\"\n",
    "\n",
    "    tokens = preprocess(text)    \n",
    "\n",
    "    # Filter non-vocab words\n",
    "    tokens = [token for token in tokens if token in vocab] #pass\n",
    "    # Convert words to ids\n",
    "    tokens = [vocab[token] for token in tokens] #pass\n",
    "\n",
    "    if len(tokens) == 0:\n",
    "        raise UnknownWordsError\n",
    "\n",
    "    # Adding a batch dimension\n",
    "    text_input = torch.from_numpy(np.asarray(torch.LongTensor(tokens).view(-1, 1)))\n",
    "\n",
    "    # Get the NN output       \n",
    "    batch_size = 1\n",
    "    hidden = model.init_hidden(batch_size) #pass\n",
    "    \n",
    "    logps, _ = model(text_input, hidden) #pass\n",
    "    # Take the exponent of the NN output to get a range of 0 to 1 for each label.\n",
    "    pred = torch.round(logps.squeeze())#pass\n",
    "    pred = torch.exp(logps) \n",
    "    \n",
    "    return pred\n",
    "\n",
    "\n",
    "def predict_api(args):\n",
    "    \"\"\" \n",
    "    Make a prediction on a single sentence.\n",
    "    Parameters\n",
    "    ----------\n",
    "        args : 入力（Pythonディクショナリ）\n",
    "    Returns\n",
    "    -------\n",
    "        pred : 予測値（Python配列）\n",
    "    \"\"\"\n",
    "    text = args.get('text')\n",
    "    try:\n",
    "        result = predict_func(text, model_l, vocab_l)\n",
    "        return result.detach().numpy()[0]\n",
    "    except UnknownWordsError:\n",
    "        return [0,0,1,0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ポジティブなセンチメントを連想させる文章を入力として予測（適宜、文章を変更して実行してみることができます）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.03567122  0.01573995  0.05834575  0.01576803  0.874475  ]\n"
     ]
    }
   ],
   "source": [
    "args = {\"text\": \"I'm bullish on $goog\"}\n",
    "result = predict_api(args)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ネガティブなセンチメントを連想させる文章を入力として予測（適宜、文章を変更して実行してみることができます）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.23630378  0.20161127  0.31348726  0.19983846  0.04875924]\n"
     ]
    }
   ],
   "source": [
    "args = {\"text\": \"I'm bearish on $goog\"}\n",
    "result = predict_api(args)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ボキャブラリ辞書に存在しない単語のみの文章を入力として予測（適宜、文章を変更して実行してみることができます）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "args = {\"text\": \"kono yoshiyuki\"}\n",
    "result = predict_api(args)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最後に\n",
    "\n",
    "データベースを削除する場合は、**データベース名を適切に変更した後で**下記を実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hive://user4@master.ykono.work:10000\n",
      " * hive://user5@master.ykono.work:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql DROP DATABASE IF EXISTS user5 CASCADE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
