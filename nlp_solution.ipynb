{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science / Machine Learning Meetup #1 Deep Learning Hands-on\n",
    "# オルタナティブ・データと自然言語処理\n",
    "\n",
    "## はじめに\n",
    "\n",
    "演習の概略は以下の通りです。\n",
    "1. [環境準備](#環境準備)\n",
    "1. Web Scraping\n",
    "1. データ変換\n",
    "1. [感情分析](#感情分析)\n",
    "    1. 前処理\n",
    "    1. ニューラル・ネットワーク構築\n",
    "    1. トレーニング\n",
    "    1. 予測\n",
    "\n",
    "以下の点にご注意ください。\n",
    "- 実行するコードの中に、ご利用中のユーザー名に合わせて、変更していただく部分があります。\n",
    "\n",
    "## 1. 環境準備\n",
    "\n",
    "### パッケージのインストールとインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipython-sql==0.3.9\n",
      "  Downloading https://files.pythonhosted.org/packages/ab/df/427e7cf05ffc67e78672ad57dce2436c1e825129033effe6fcaf804d0c60/ipython_sql-0.3.9-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from ipython-sql==0.3.9) (1.12.0)\n",
      "Requirement already satisfied: ipython>=1.0 in /usr/local/lib/python3.6/dist-packages (from ipython-sql==0.3.9) (5.1.0)\n",
      "Requirement already satisfied: ipython-genutils>=0.1.0 in /usr/local/lib/python3.6/dist-packages (from ipython-sql==0.3.9) (0.2.0)\n",
      "Collecting sqlalchemy>=0.6.7 (from ipython-sql==0.3.9)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/47/35edeb0f86c0b44934c05d961c893e223ef27e79e1f53b5e6f14820ff553/SQLAlchemy-1.3.13.tar.gz (6.0MB)\n",
      "\u001b[K     |████████████████████████████████| 6.0MB 10.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sqlparse (from ipython-sql==0.3.9)\n",
      "  Downloading https://files.pythonhosted.org/packages/ef/53/900f7d2a54557c6a37886585a91336520e5539e3ae2423ff1102daf4f3a7/sqlparse-0.3.0-py2.py3-none-any.whl\n",
      "Collecting prettytable (from ipython-sql==0.3.9)\n",
      "  Downloading https://files.pythonhosted.org/packages/ef/30/4b0746848746ed5941f052479e7c23d2b56d174b82f4fd34a25e389831f5/prettytable-0.7.2.tar.bz2\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython>=1.0->ipython-sql==0.3.9) (4.4.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython>=1.0->ipython-sql==0.3.9) (41.0.1)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=1.0->ipython-sql==0.3.9) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.3 in /usr/local/lib/python3.6/dist-packages (from ipython>=1.0->ipython-sql==0.3.9) (1.0.15)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=1.0->ipython-sql==0.3.9) (4.7.0)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython>=1.0->ipython-sql==0.3.9) (2.4.2)\n",
      "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython>=1.0->ipython-sql==0.3.9) (4.3.2)\n",
      "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=1.0->ipython-sql==0.3.9) (0.8.1)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.3->ipython>=1.0->ipython-sql==0.3.9) (0.1.7)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=1.0->ipython-sql==0.3.9) (0.6.0)\n",
      "Building wheels for collected packages: sqlalchemy, prettytable\n",
      "  Building wheel for sqlalchemy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/cdsw/.cache/pip/wheels/b3/35/98/4c9cb3fd63d21d5606b972dd70643769745adf60e622467b71\n",
      "  Building wheel for prettytable (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/cdsw/.cache/pip/wheels/80/34/1c/3967380d9676d162cb59513bd9dc862d0584e045a162095606\n",
      "Successfully built sqlalchemy prettytable\n",
      "Installing collected packages: sqlalchemy, sqlparse, prettytable, ipython-sql\n",
      "Successfully installed ipython-sql-0.3.9 prettytable-0.7.2 sqlalchemy-1.3.13 sqlparse-0.3.0\n",
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting PyHive==0.6.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4e/26/de91125c0d9e8947d48f387f4d1f2e7a22aa92a30771ad02f63a5653361b/PyHive-0.6.1.tar.gz (41kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 4.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting future (from PyHive==0.6.1)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
      "\u001b[K     |████████████████████████████████| 829kB 21.6MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from PyHive==0.6.1) (2.8.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil->PyHive==0.6.1) (1.12.0)\n",
      "Building wheels for collected packages: PyHive, future\n",
      "  Building wheel for PyHive (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/cdsw/.cache/pip/wheels/00/61/fb/77a0e77deb4c900276f689e62628a5ca7ba9df600f9ad7ba6a\n",
      "  Building wheel for future (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/cdsw/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
      "Successfully built PyHive future\n",
      "Installing collected packages: future, PyHive\n",
      "Successfully installed PyHive-0.6.1 future-0.18.2\n",
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: SQLAlchemy==1.3.13 in ./.local/lib/python3.6/site-packages (1.3.13)\n",
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting thrift==0.13.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/97/1e/3284d19d7be99305eda145b8aa46b0c33244e4a496ec66440dac19f8274d/thrift-0.13.0.tar.gz (59kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 6.8MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: six>=1.7.2 in /usr/local/lib/python3.6/dist-packages (from thrift==0.13.0) (1.12.0)\n",
      "Building wheels for collected packages: thrift\n",
      "  Building wheel for thrift (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/cdsw/.cache/pip/wheels/02/a2/46/689ccfcf40155c23edc7cdbd9de488611c8fdf49ff34b1706e\n",
      "Successfully built thrift\n",
      "Installing collected packages: thrift\n",
      "Successfully installed thrift-0.13.0\n",
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting sasl==0.2.1\n",
      "  Downloading https://files.pythonhosted.org/packages/8e/2c/45dae93d666aea8492678499e0999269b4e55f1829b1e4de5b8204706ad9/sasl-0.2.1.tar.gz\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sasl==0.2.1) (1.12.0)\n",
      "Building wheels for collected packages: sasl\n",
      "  Building wheel for sasl (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/cdsw/.cache/pip/wheels/56/20/21/ff481fd0f4ae09d5d94c76d089f550204580b1703e44f27dd5\n",
      "Successfully built sasl\n",
      "Installing collected packages: sasl\n",
      "Successfully installed sasl-0.2.1\n",
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting thrift_sasl==0.3.0\n",
      "  Downloading https://files.pythonhosted.org/packages/50/fe/89cbc910809e3757c762f56ee190ca39e0f28b7ea451835232c0c988d706/thrift_sasl-0.3.0.tar.gz\n",
      "Requirement already satisfied: thrift>=0.10.0 in ./.local/lib/python3.6/site-packages (from thrift_sasl==0.3.0) (0.13.0)\n",
      "Requirement already satisfied: sasl>=0.2.1 in ./.local/lib/python3.6/site-packages (from thrift_sasl==0.3.0) (0.2.1)\n",
      "Requirement already satisfied: six>=1.7.2 in /usr/local/lib/python3.6/dist-packages (from thrift>=0.10.0->thrift_sasl==0.3.0) (1.12.0)\n",
      "Building wheels for collected packages: thrift-sasl\n",
      "  Building wheel for thrift-sasl (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/cdsw/.cache/pip/wheels/c8/3a/34/1d82df3d652788fc211c245d51dde857a58e603695ea41d93d\n",
      "Successfully built thrift-sasl\n",
      "Installing collected packages: thrift-sasl\n",
      "Successfully installed thrift-sasl-0.3.0\n",
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting nltk==3.4.5\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5MB 4.3MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk==3.4.5) (1.12.0)\n",
      "Building wheels for collected packages: nltk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/cdsw/.cache/pip/wheels/96/86/f6/68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n",
      "Successfully built nltk\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.4.5\n",
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting torch==1.4.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/19/4804aea17cd136f1705a5e98a00618cb8f6ccc375ad8bfa437408e09d058/torch-1.4.0-cp36-cp36m-manylinux1_x86_64.whl (753.4MB)\n",
      "\u001b[K     |████████████████████████████████| 753.4MB 52kB/s s eta 0:00:01                      | 62.5MB 4.4MB/s eta 0:02:39     |██▉                             | 66.4MB 4.4MB/s eta 0:02:38 eta 0:00:07     |███████████████▊                | 370.3MB 74.3MB/s eta 0:00:06     |██████████████████████████▊     | 628.8MB 77.8MB/s eta 0:00:02\n",
      "\u001b[?25hInstalling collected packages: torch\n",
      "Successfully installed torch-1.4.0\n",
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install ipython-sql==0.3.9\n",
    "!pip3 install PyHive==0.6.1\n",
    "!pip3 install SQLAlchemy==1.3.13\n",
    "!pip3 install thrift==0.13.0\n",
    "!pip3 install sasl==0.2.1\n",
    "!pip3 install thrift_sasl==0.3.0\n",
    "\n",
    "\n",
    "!pip3 install nltk==3.4.5\n",
    "!pip3 install torch==1.4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記でインストールしたPyHiveは、Pythonコードの中でimportして使われるのではなく、Hiveへの接続の際の接続文字列：`sqlalchemy.create_engine('hive://<host>:<port>')`の中でdialectsとして指定された際に必要になります。そのため、インストール後に利用するためには、新しくプロセスを始める必要があります。**インストールした後に一度、KernelをRestartしてください。**インストールしたプロセスでは、接続時に下記のようなエラーが発生します。\n",
    "`NoSuchModuleError: Can't load plugin: sqlalchemy.dialects:hive`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import subprocess\n",
    "import glob\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "\n",
    "from pyhive import hive\n",
    "import sqlalchemy\n",
    "\n",
    "import sys\n",
    "#from random import random\n",
    "from operator import add\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import torch\n",
    "import nltk\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Web Scraping\n",
    "\n",
    "無償で利用できるAPIを用いて演習を行います。そのため、利用に一定の制限が課せられることにご留意ください。\n",
    "例えば、ご利用状況に応じて、下記のようなエラーメッセージを受け取ることがあります。\n",
    "\n",
    "```\n",
    "{\"response\":{\"status\":429},\"errors\":[{\"message\":\"Rate limit exceeded. Client may not make more than 200 requests an hour.\"}]}\n",
    "```\n",
    "まず、APIで取得したデータをCDSWプロジェクト内のファイルとして保存します。\n",
    "\n",
    "取得する銘柄の候補が、`ticker.txt`に定義されています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2882\n",
      "['A', 'AA', 'AAL', 'AAN', 'AAOI', 'AAON', 'AAP', 'AAPL', 'AAWW', 'AAXN', 'ABBV', 'ABC', 'ABCB', 'ABEO', 'ABG', 'ABM', 'ABMD', 'ABT', 'ABTX', 'ACA', 'ACAD', 'ACCO', 'ACEL', 'ACGL', 'ACHC', 'ACHN', 'ACHV', 'ACIA', 'ACIW', 'ACLS', 'ACM', 'ACN', 'ACNB', 'ACOR', 'ACRS', 'ACRX', 'ACTG', 'ADBE', 'ADES', 'ADI', 'ADM', 'ADMA', 'ADMP', 'ADMS', 'ADP', 'ADPT', 'ADRO', 'ADS', 'ADSK', 'ADSW', 'ADT', 'ADTN', 'ADUS', 'ADVM', 'ADXS', 'AE', 'AEE', 'AEGN', 'AEIS', 'AEL', 'AEM', 'AEMD', 'AEO', 'AEP', 'AERI', 'AES', 'AFG', 'AFI', 'AFL', 'AG', 'AGCO', 'AGEN', 'AGFS', 'AGI', 'AGIO', 'AGLE', 'AGM', 'AGN', 'AGO', 'AGR', 'AGRX', 'AGS', 'AGTC', 'AGX', 'AGYS', 'AHC', 'AHCO', 'AIG', 'AIMC', 'AIMT', 'AIN', 'AIR', 'AIRG', 'AIRT', 'AIT', 'AIZ', 'AJG', 'AJRD', 'AKAM', 'AKBA', 'AKCA', 'AKRO', 'AKRX', 'AKS', 'AL', 'ALB', 'ALCO', 'ALDX', 'ALE', 'ALEC', 'ALG', 'ALGN', 'ALGT', 'ALIM', 'ALK', 'ALKS', 'ALL', 'ALLK', 'ALLO', 'ALLY', 'ALNY', 'ALOT', 'ALPN', 'ALRM', 'ALRN', 'ALSK', 'ALSN', 'ALT', 'ALTR', 'ALV', 'ALXN', 'AM', 'AMAG', 'AMAL', 'AMAT', 'AMBA', 'AMBC', 'AMC', 'AMCX', 'AMD', 'AME', 'AMED', 'AMEH', 'AMG', 'AMGN', 'AMK', 'AMKR', 'AMN', 'AMNB', 'AMOT', 'AMP', 'AMPE', 'AMPH', 'AMRC', 'AMRS', 'AMRX', 'AMSC', 'AMSF', 'AMSWA', 'AMTB', 'AMTD', 'AMTX', 'AMWD', 'AMZN', 'AN', 'ANAB', 'ANAT', 'ANDA', 'ANDE', 'ANET', 'ANF', 'ANGI', 'ANGO', 'ANIK', 'ANIP', 'ANIX', 'ANSS', 'ANTM', 'AOBC', 'AON', 'AOS', 'AP', 'APA', 'APD', 'APDN', 'APEI', 'APEN', 'APH', 'APLS', 'APLT', 'APOG', 'APPF', 'APPN', 'APPS', 'APRE', 'APRN', 'APT', 'APTV', 'APY', 'AQB', 'AQN', 'AQUA', 'AR', 'ARA', 'ARAV', 'ARAY', 'ARCB', 'ARCH', 'ARCO', 'ARCT', 'ARDS', 'ARDX', 'ARES', 'ARGO', 'ARKR', 'ARLO', 'ARMK', 'ARMP', 'ARNA', 'ARNC', 'AROC', 'AROW', 'ARTNA', 'ARVN', 'ARW', 'ARWR', 'ASB', 'ASFI', 'ASGN', 'ASH', 'ASIX', 'ASMB', 'ASNA', 'ASPS', 'ASPU', 'ASRT', 'ASTE', 'ASYS', 'ATEC', 'ATEN', 'ATEX', 'ATGE', 'ATH', 'ATHX', 'ATI', 'ATKR', 'ATLO', 'ATNI', 'ATNX', 'ATO', 'ATR', 'ATRA', 'ATRC', 'ATRI', 'ATRO', 'ATRS', 'ATSG', 'ATUS', 'ATVI', 'AUB', 'AUMN', 'AUPH', 'AUTO', 'AUY', 'AVA', 'AVAV', 'AVD', 'AVGO', 'AVID', 'AVLR', 'AVNS', 'AVNW', 'AVRO', 'AVT', 'AVTR', 'AVX', 'AVXL', 'AVY', 'AVYA', 'AWI', 'AWK', 'AWR', 'AWRE', 'AX', 'AXAS', 'AXDX', 'AXGN', 'AXGT', 'AXL', 'AXLA', 'AXNX', 'AXP', 'AXS', 'AXSM', 'AXTA', 'AXTI', 'AYI', 'AYX', 'AZO', 'AZPN', 'AZZ', 'B', 'BA', 'BAC', 'BAH', 'BAM', 'BANC', 'BAND', 'BANF', 'BANR', 'BAP', 'BAX', 'BB', 'BBBY', 'BBI', 'BBIO', 'BBQ', 'BBW', 'BBY', 'BC', 'BCBP', 'BCC', 'BCE', 'BCEI', 'BCEL', 'BCLI', 'BCO', 'BCOR', 'BCOV', 'BCPC', 'BCRX', 'BDC', 'BDGE', 'BDSI', 'BDX', 'BE', 'BEAT', 'BECN', 'BELFB', 'BEN', 'BERY', 'BFAM', 'BFC', 'BFIN', 'BG', 'BGCP', 'BGFV', 'BGG', 'BGS', 'BH', 'BHB', 'BHC', 'BHE', 'BHF', 'BHLB', 'BIG', 'BIIB', 'BIMI', 'BIO', 'BIOS', 'BJ', 'BJRI', 'BK', 'BKD', 'BKE', 'BKH', 'BKI', 'BKNG', 'BKR', 'BKU', 'BL', 'BLBD', 'BLCM', 'BLD', 'BLDP', 'BLDR', 'BLFS', 'BLK', 'BLKB', 'BLL', 'BLMN', 'BLNK', 'BLUE', 'BLX', 'BMCH', 'BMI', 'BMO', 'BMRC', 'BMRN', 'BMTC', 'BMY', 'BNED', 'BNFT', 'BNGO', 'BNS', 'BOH', 'BOKF', 'BOMN', 'BOOM', 'BOOT', 'BOX', 'BPFH', 'BPMC', 'BPTH', 'BR', 'BRC', 'BRKB', 'BRKL', 'BRKR', 'BRKS', 'BRMK', 'BRO', 'BRY', 'BSET', 'BSIG', 'BSQR', 'BSRR', 'BSTC', 'BSX', 'BTG', 'BURL', 'BUSE', 'BV', 'BWA', 'BWEN', 'BWXT', 'BX', 'BXC', 'BXG', 'BXS', 'BY', 'BYD', 'BYND', 'BZH', 'C', 'CABO', 'CAC', 'CACC', 'CACI', 'CADE', 'CAG', 'CAH', 'CAI', 'CAKE', 'CAL', 'CALA', 'CALM', 'CALX', 'CAMP', 'CAPR', 'CAR', 'CARA', 'CARE', 'CARG', 'CARS', 'CASA', 'CASH', 'CASI', 'CASS', 'CASY', 'CAT', 'CATB', 'CATM', 'CATO', 'CATY', 'CBIO', 'CBMG', 'CBPO', 'CBPX', 'CBRE', 'CBRL', 'CBSH', 'CBT', 'CBTX', 'CBU', 'CBZ', 'CC', 'CCBG', 'CCEP', 'CCF', 'CCJ', 'CCK', 'CCL', 'CCMP', 'CCNE', 'CCO', 'CCOI', 'CCRN', 'CCS', 'CCXI', 'CDAY', 'CDE', 'CDK', 'CDLX', 'CDMO', 'CDNA', 'CDNS', 'CDW', 'CDXS', 'CE', 'CECE', 'CEIX', 'CEL', 'CELH', 'CEMI', 'CENT', 'CENTA', 'CENX', 'CERN', 'CERS', 'CEVA', 'CF', 'CFB', 'CFFI', 'CFFN', 'CFG', 'CFMS', 'CFR', 'CFX', 'CGC', 'CGEN', 'CGNX', 'CHCO', 'CHD', 'CHDN', 'CHE', 'CHEF', 'CHGG', 'CHH', 'CHK', 'CHKP', 'CHMA', 'CHMG', 'CHNG', 'CHRS', 'CHRW', 'CHS', 'CHTR', 'CHUY', 'CHWY', 'CI', 'CIEN', 'CINF', 'CIR', 'CIT', 'CKH', 'CKPT', 'CL', 'CLAR', 'CLBK', 'CLBS', 'CLCT', 'CLDR', 'CLDX', 'CLF', 'CLFD', 'CLGX', 'CLH', 'CLNE', 'CLPS', 'CLR', 'CLSD', 'CLSN', 'CLVS', 'CLW', 'CLX', 'CLXT', 'CM', 'CMA', 'CMBM', 'CMC', 'CMCO', 'CMCSA', 'CMD', 'CME', 'CMG', 'CMI', 'CMLS', 'CMP', 'CMS', 'CMTL', 'CNA', 'CNBKA', 'CNC', 'CNCE', 'CNDT', 'CNI', 'CNK', 'CNMD', 'CNNE', 'CNO', 'CNOB', 'CNP', 'CNQ', 'CNR', 'CNS', 'CNSL', 'CNST', 'CNX', 'CNXN', 'CO', 'COF', 'COG', 'COHR', 'COHU', 'COKE', 'COLB', 'COLL', 'COLM', 'COMM', 'CONN', 'COO', 'COOP', 'COP', 'CORE', 'CORT', 'COST', 'COT', 'COTY', 'COUP', 'CP', 'CPA', 'CPB', 'CPE', 'CPF', 'CPG', 'CPIX', 'CPK', 'CPRI', 'CPRT', 'CPRX', 'CPS', 'CPSI', 'CPST', 'CR', 'CRAI', 'CRBP', 'CRC', 'CRCM', 'CREE', 'CRI', 'CRIS', 'CRK', 'CRL', 'CRM', 'CRMD', 'CRMT', 'CRNC', 'CRNT', 'CRNX', 'CRON', 'CROX', 'CRS', 'CRTX', 'CRUS', 'CRVL', 'CRVS', 'CRWD', 'CRWS', 'CRY', 'CSBR', 'CSCO', 'CSFL', 'CSGP', 'CSGS', 'CSII', 'CSIQ', 'CSL', 'CSOD', 'CSPI', 'CSS', 'CSSE', 'CSTL', 'CSU', 'CSV', 'CSWI', 'CSX', 'CTAS', 'CTB', 'CTBI', 'CTG', 'CTIC', 'CTL', 'CTLT', 'CTMX', 'CTO', 'CTRA', 'CTRN', 'CTS', 'CTSH', 'CTSO', 'CTVA', 'CTXS', 'CUB', 'CUBI', 'CULP', 'CURO', 'CUTR', 'CVA', 'CVBF', 'CVCO', 'CVE', 'CVET', 'CVGW', 'CVI', 'CVLT', 'CVLY', 'CVM', 'CVNA', 'CVS', 'CVTI', 'CVU', 'CVX', 'CW', 'CWBC', 'CWCO', 'CWEN', 'CWH', 'CWK', 'CWST', 'CWT', 'CXDC', 'CXO', 'CY', 'CYAN', 'CYBE', 'CYBR', 'CYCN', 'CYD', 'CYH', 'CYRX', 'CYTK', 'CZNC', 'CZR', 'CZWI', 'CZZ', 'D', 'DAIO', 'DAKT', 'DAL', 'DAN', 'DAR', 'DARE', 'DB', 'DBD', 'DBI', 'DBX', 'DCI', 'DCO', 'DCOM', 'DCPH', 'DD', 'DDD', 'DDOG', 'DDS', 'DE', 'DECK', 'DELL', 'DENN', 'DERM', 'DFIN', 'DFS', 'DG', 'DGICA', 'DGII', 'DGLY', 'DGX', 'DHI', 'DHIL', 'DHR', 'DHT', 'DIN', 'DIOD', 'DIS', 'DISCA', 'DISCK', 'DISH', 'DJCO', 'DK', 'DKS', 'DLA', 'DLB', 'DLTR', 'DLX', 'DMRC', 'DNKN', 'DNLI', 'DNOW', 'DNR', 'DO', 'DOCU', 'DOMO', 'DOOR', 'DORM', 'DOV', 'DOW', 'DPLO', 'DPZ', 'DRAD', 'DRI', 'DRNA', 'DRQ', 'DSGX', 'DSKE', 'DSPG', 'DT', 'DTE', 'DTIL', 'DUK', 'DVA', 'DVAX', 'DVD', 'DVN', 'DXC', 'DXCM', 'DXPE', 'DXR', 'DY', 'DZSI', 'EA', 'EAF', 'EAT', 'EB', 'EBAY', 'EBF', 'EBIX', 'EBS', 'EBSB', 'EBTC', 'ECHO', 'ECL', 'ECOL', 'ECOM', 'ECPG', 'ED', 'EDIT', 'EDSA', 'EDTX', 'EDUC', 'EE', 'EEFT', 'EEX', 'EFC', 'EFSC', 'EFX', 'EGAN', 'EGBN', 'EGHT', 'EGO', 'EGOV', 'EGRX', 'EGY', 'EHC', 'EHTH', 'EIDX', 'EIG', 'EIGI', 'EIGR', 'EIX', 'EKSO', 'EL', 'ELAN', 'ELF', 'ELGX', 'ELY', 'EMCF', 'EME', 'EMKR', 'EML', 'EMMS', 'EMN', 'EMR', 'ENB', 'ENDP', 'ENLV', 'ENPH', 'ENR', 'ENS', 'ENSG', 'ENTA', 'ENTG', 'ENV', 'ENVA', 'EOG', 'EOLS', 'EPAC', 'EPAM', 'EPAY', 'EPC', 'EPIX', 'EPM', 'EPZM', 'EQH', 'EQT', 'ERA', 'ERF', 'ERI', 'ERIE', 'ERII', 'ES', 'ESCA', 'ESE', 'ESGR', 'ESLT', 'ESNT', 'ESPR', 'ESSA', 'ETFC', 'ETH', 'ETM', 'ETN', 'ETNB', 'ETR', 'ETSY', 'EV', 'EVBG', 'EVC', 'EVER', 'EVFM', 'EVH', 'EVOP', 'EVR', 'EVRG', 'EVRI', 'EW', 'EWBC', 'EXAS', 'EXC', 'EXEL', 'EXK', 'EXLS', 'EXP', 'EXPD', 'EXPE', 'EXPI', 'EXPO', 'EXPR', 'EXTR', 'EYE', 'EYPT', 'EZPW', 'F', 'FAF', 'FANG', 'FARM', 'FARO', 'FAST', 'FAT', 'FATE', 'FB', 'FBC', 'FBHS', 'FBIZ', 'FBK', 'FBM', 'FBMS', 'FBNC', 'FC', 'FCBC', 'FCEL', 'FCF', 'FCFS', 'FCN', 'FCNCA', 'FCX', 'FDEF', 'FDP', 'FDS', 'FDX', 'FE', 'FEIM', 'FELE', 'FEYE', 'FF', 'FFBC', 'FFG', 'FFIC', 'FFIN', 'FFIV', 'FFWM', 'FG', 'FGEN', 'FHB', 'FHN', 'FIBK', 'FICO', 'FII', 'FIS', 'FISI', 'FISV', 'FIT', 'FITB', 'FIVE', 'FIVN', 'FIX', 'FIXX', 'FIZZ', 'FL', 'FLDM', 'FLEX', 'FLGT', 'FLIC', 'FLIR', 'FLMN', 'FLNT', 'FLO', 'FLOW', 'FLR', 'FLS', 'FLT', 'FLWS', 'FLXN', 'FLXS', 'FMBH', 'FMBI', 'FMC', 'FMNB', 'FN', 'FNB', 'FND', 'FNF', 'FNHC', 'FNJN', 'FNKO', 'FNLC', 'FNV', 'FOCS', 'FOE', 'FOLD', 'FOMX', 'FONR', 'FOR', 'FORM', 'FORR', 'FOSL', 'FOX', 'FOXA', 'FOXF', 'FPRX', 'FRAN', 'FRC', 'FREQ', 'FRGI', 'FRHC', 'FRME', 'FRO', 'FRPH', 'FRPT', 'FRTA', 'FSB', 'FSBW', 'FSCT', 'FSFG', 'FSI', 'FSLR', 'FSLY', 'FSM', 'FSS', 'FSTR', 'FTCH', 'FTDR', 'FTEK', 'FTI', 'FTNT', 'FTR', 'FTS', 'FTSV', 'FTV', 'FUL', 'FULC', 'FULT', 'FVE', 'FVRR', 'FWONA', 'FWRD', 'GABC', 'GALT', 'GATX', 'GBCI', 'GBL', 'GBLI', 'GBT', 'GBX', 'GCAP', 'GCBC', 'GCI', 'GCO', 'GCP', 'GD', 'GDDY', 'GDEN', 'GDI', 'GDOT', 'GE', 'GEC', 'GEF', 'GEOS', 'GERN', 'GES', 'GEVO', 'GFF', 'GGG', 'GH', 'GHC', 'GHL', 'GHM', 'GIB', 'GIFI', 'GIII', 'GIL', 'GILD', 'GIS', 'GKOS', 'GL', 'GLDD', 'GLMD', 'GLNG', 'GLRE', 'GLT', 'GLUU', 'GLW', 'GLYC', 'GM', 'GME', 'GMED', 'GMS', 'GNC', 'GNCA', 'GNE', 'GNMK', 'GNMX', 'GNRC', 'GNTX', 'GNW', 'GO', 'GOGO', 'GOLD', 'GOLF', 'GOOG', 'GOOGL', 'GOOS', 'GORO', 'GOSS', 'GPC', 'GPI', 'GPK', 'GPN', 'GPOR', 'GPRE', 'GPRK', 'GPRO', 'GPS', 'GPX', 'GRA', 'GRBK', 'GRC', 'GRIF', 'GRPN', 'GRTS', 'GRUB', 'GS', 'GSB', 'GSBC', 'GSHD', 'GSKY', 'GT', 'GTES', 'GTHX', 'GTLS', 'GTN', 'GTT', 'GTX', 'GVA', 'GWB', 'GWRE', 'GWRS', 'GWW', 'H', 'HA', 'HABT', 'HAE', 'HAFC', 'HAIN', 'HAL', 'HALO', 'HARP', 'HAS', 'HAYN', 'HBAN', 'HBB', 'HBCP', 'HBI', 'HBIO', 'HBM', 'HBNC', 'HBT', 'HCA', 'HCAT', 'HCC', 'HCCI', 'HCI', 'HCKT', 'HCSG', 'HD', 'HDS', 'HDSN', 'HE', 'HEAR', 'HEES', 'HEI', 'HELE', 'HEPA', 'HES', 'HFBL', 'HFC', 'HFFG', 'HFWA', 'HGV', 'HHC', 'HHS', 'HI', 'HIBB', 'HIFS', 'HIG', 'HII', 'HIIQ', 'HJLI', 'HL', 'HLF', 'HLI', 'HLIO', 'HLIT', 'HLT', 'HLX', 'HMHC', 'HMN', 'HMST', 'HMSY', 'HMTV', 'HNGR', 'HNI', 'HOFT', 'HOG', 'HOLX', 'HOMB', 'HOME', 'HON', 'HONE', 'HOPE', 'HOV', 'HP', 'HPE', 'HPQ', 'HQY', 'HRB', 'HRC', 'HRI', 'HRL', 'HROW', 'HRTG', 'HRTX', 'HSC', 'HSIC', 'HSII', 'HSKA', 'HSTM', 'HSY', 'HTBI', 'HTBK', 'HTGM', 'HTH', 'HTLD', 'HTLF', 'HUBB', 'HUBG', 'HUBS', 'HUD', 'HUM', 'HUN', 'HURC', 'HURN', 'HVT', 'HWC', 'HWCC', 'HWKN', 'HXL', 'HY', 'HYRE', 'HZO', 'IAA', 'IAC', 'IAG', 'IART', 'IBCP', 'IBKC', 'IBKR', 'IBM', 'IBOC', 'IBP', 'IBTX', 'ICBK', 'ICE', 'ICFI', 'ICHR', 'ICON', 'ICPT', 'ICUI', 'IDA', 'IDCC', 'IDRA', 'IDT', 'IDXX', 'IESC', 'IEX', 'IFF', 'IGMS', 'IGT', 'IHC', 'IIIN', 'IIIV', 'IIVI', 'ILMN', 'IMAX', 'IMGN', 'IMH', 'IMKTA', 'IMMR', 'IMMU', 'IMUX', 'IMXI', 'INAP', 'INBK', 'INCY', 'INDB', 'INFN', 'INFO', 'INGN', 'INGR', 'INMD', 'INO', 'INOD', 'INOV', 'INS', 'INSG', 'INSM', 'INSP', 'INST', 'INT', 'INTC', 'INTL', 'INTU', 'INVA', 'INVE', 'IO', 'IONS', 'IOSP', 'IOTS', 'IOVA', 'IP', 'IPAR', 'IPG', 'IPGP', 'IPHI', 'IPHS', 'IPI', 'IPWR', 'IQV', 'IR', 'IRBT', 'IRDM', 'IRMD', 'IRTC', 'IRWD', 'ISBC', 'ISEE', 'ISNS', 'ISRG', 'IT', 'ITCI', 'ITGR', 'ITIC', 'ITRI', 'ITT', 'ITW', 'IVC', 'IVZ', 'J', 'JACK', 'JAZZ', 'JBHT', 'JBL', 'JBLU', 'JBSS', 'JBT', 'JCI', 'JCOM', 'JCP', 'JEF', 'JELD', 'JILL', 'JJSF', 'JKHY', 'JLL', 'JNCE', 'JNJ', 'JNPR', 'JOE', 'JOUT', 'JPM', 'JRVR', 'JVA', 'JWN', 'JYNT', 'K', 'KAI', 'KALA', 'KALU', 'KALV', 'KAMN', 'KAR', 'KBH', 'KBR', 'KDMN', 'KDP', 'KE', 'KELYA', 'KEM', 'KEX', 'KEY', 'KEYS', 'KFRC', 'KFS', 'KFY', 'KGC', 'KHC', 'KIDS', 'KIRK', 'KL', 'KLAC', 'KLIC', 'KMB', 'KMI', 'KMPR', 'KMT', 'KMX', 'KN', 'KNDI', 'KNL', 'KNSA', 'KNSL', 'KNX', 'KO', 'KOD', 'KODK', 'KOP', 'KOPN', 'KOS', 'KPTI', 'KR', 'KRA', 'KRNT', 'KRNY', 'KRO', 'KRTX', 'KRUS', 'KRYS', 'KSS', 'KSU', 'KTB', 'KTCC', 'KTOS', 'KURA', 'KVHI', 'KW', 'KWR', 'L', 'LAD', 'LAKE', 'LANC', 'LASR', 'LAUR', 'LAWS', 'LB', 'LBAI', 'LBC', 'LBRDA', 'LBRDK', 'LBRT', 'LBTYA', 'LBTYK', 'LBY', 'LC', 'LCI', 'LCII', 'LCNB', 'LCUT', 'LDL', 'LDOS', 'LE', 'LEA', 'LEAF', 'LECO', 'LEG', 'LEGH', 'LEN', 'LEVI', 'LFUS', 'LFVN', 'LGIH', 'LGND', 'LH', 'LHCG', 'LHX', 'LII', 'LILA', 'LILAK', 'LIN', 'LINC', 'LIND', 'LITE', 'LIVE', 'LIVN', 'LJPC', 'LKFN', 'LKQ', 'LL', 'LLNW', 'LLY', 'LM', 'LMAT', 'LMNR', 'LMNX', 'LMT', 'LNC', 'LNDC', 'LNG', 'LNN', 'LNT', 'LNTH', 'LOB', 'LOCO', 'LOGC', 'LOGM', 'LOOP', 'LOPE', 'LORL', 'LOVE', 'LOW', 'LPCN', 'LPI', 'LPLA', 'LPSN', 'LPX', 'LQDA', 'LQDT', 'LRCX', 'LRN', 'LSCC', 'LSTR', 'LTHM', 'LTRPA', 'LTS', 'LULU', 'LUNA', 'LUV', 'LVGO', 'LVS', 'LW', 'LWAY', 'LXRX', 'LXU', 'LYFT', 'LYTS', 'LYV', 'LZB', 'M', 'MA', 'MACK', 'MAGS', 'MAN', 'MANH', 'MANT', 'MANU', 'MAR', 'MARA', 'MAS', 'MASI', 'MAT', 'MATW', 'MATX', 'MAXR', 'MBI', 'MBIN', 'MBIO', 'MBOT', 'MBUU', 'MBWM', 'MC', 'MCD', 'MCF', 'MCHP', 'MCHX', 'MCK', 'MCO', 'MCRB', 'MCRI', 'MCS', 'MCY', 'MD', 'MDB', 'MDC', 'MDGL', 'MDLA', 'MDLZ', 'MDP', 'MDRX', 'MDT', 'MDU', 'MDWD', 'MEC', 'MED', 'MEDP', 'MEET', 'MEI', 'MELI', 'MEOH', 'MERC', 'MESA', 'MET', 'MFC', 'MFIN', 'MFSF', 'MG', 'MGA', 'MGEE', 'MGI', 'MGIC', 'MGLN', 'MGM', 'MGNX', 'MGPI', 'MGRC', 'MGTA', 'MGTX', 'MGY', 'MHK', 'MHO', 'MIC', 'MIDD', 'MIK', 'MIME', 'MINI', 'MIRM', 'MIST', 'MITK', 'MKC', 'MKL', 'MKSI', 'MKTX', 'MLAB', 'MLHR', 'MLI', 'MLM', 'MLND', 'MLNT', 'MLNX', 'MLR', 'MMC', 'MMM', 'MMS', 'MMSI', 'MMYT', 'MNI', 'MNK', 'MNKD', 'MNLO', 'MNOV', 'MNRO', 'MNST', 'MNTA', 'MO', 'MOBL', 'MOD', 'MODN', 'MOFG', 'MOH', 'MORF', 'MORN', 'MOS', 'MOV', 'MPAA', 'MPC', 'MPWR', 'MPX', 'MRAM', 'MRC', 'MRCY', 'MRIN', 'MRK', 'MRKR', 'MRLN', 'MRNA', 'MRNS', 'MRO', 'MRTN', 'MRTX', 'MRVL', 'MS', 'MSA', 'MSBI', 'MSCI', 'MSEX', 'MSFT', 'MSG', 'MSGN', 'MSI', 'MSM', 'MSON', 'MSTR', 'MTB', 'MTBC', 'MTCH', 'MTD', 'MTDR', 'MTEM', 'MTEX', 'MTG', 'MTH', 'MTN', 'MTOR', 'MTRN', 'MTRX', 'MTSC', 'MTSI', 'MTW', 'MTX', 'MTZ', 'MU', 'MUR', 'MUSA', 'MUX', 'MVIS', 'MWA', 'MXIM', 'MXL', 'MYE', 'MYGN', 'MYL', 'MYOK', 'MYOV', 'MYRG', 'NAII', 'NAT', 'NATH', 'NATI', 'NATR', 'NAV', 'NAVI', 'NBEV', 'NBHC', 'NBIX', 'NBL', 'NBR', 'NBSE', 'NBTB', 'NC', 'NCBS', 'NCLH', 'NCMI', 'NCR', 'NDAQ', 'NDLS', 'NDSN', 'NEE', 'NEM', 'NEO', 'NEOG', 'NEON', 'NEP', 'NERV', 'NET', 'NETE', 'NEU', 'NEWR', 'NEXT', 'NFBK', 'NFE', 'NFG', 'NFLX', 'NG', 'NGHC', 'NGM', 'NGS', 'NGVC', 'NGVT', 'NHC', 'NHTC', 'NI', 'NJR', 'NK', 'NKE', 'NKSH', 'NKTR', 'NL', 'NLNK', 'NLOK', 'NLSN', 'NLTX', 'NMIH', 'NMRK', 'NNBR', 'NNI', 'NOC', 'NOV', 'NOVA', 'NOVN', 'NOVT', 'NOW', 'NP', 'NPK', 'NPO', 'NPTN', 'NR', 'NRC', 'NRG', 'NRIM', 'NSC', 'NSIT', 'NSP', 'NSSC', 'NSTG', 'NTAP', 'NTB', 'NTCT', 'NTGR', 'NTLA', 'NTNX', 'NTR', 'NTRA', 'NTRS', 'NTUS', 'NTWK', 'NUAN', 'NUE', 'NUS', 'NUVA', 'NVAX', 'NVCN', 'NVCR', 'NVDA', 'NVEC', 'NVEE', 'NVMI', 'NVR', 'NVRO', 'NVST', 'NVTA', 'NWBI', 'NWE', 'NWFL', 'NWHM', 'NWL', 'NWLI', 'NWN', 'NWPX', 'NWS', 'NWSA', 'NX', 'NXGN', 'NXPI', 'NXST', 'NXTC', 'NYCB', 'NYT', 'OAS', 'OBCI', 'OBNK', 'OC', 'OCFC', 'OCGN', 'OCN', 'OCUL', 'ODC', 'ODFL', 'ODP', 'ODT', 'OFIX', 'OFLX', 'OGE', 'OGS', 'OI', 'OII', 'OIS', 'OKE', 'OKTA', 'OLED', 'OLLI', 'OLN', 'OMC', 'OMCL', 'OMER', 'OMEX', 'OMF', 'OMI', 'ON', 'ONB', 'ONCS', 'ONCT', 'ONDK', 'ONTO', 'ONTX', 'OOMA', 'OPB', 'OPES', 'OPGN', 'OPK', 'OPRT', 'OPTN', 'OPTT', 'OPY', 'ORA', 'ORBC', 'ORCC', 'ORCL', 'ORI', 'ORLY', 'ORMP', 'ORRF', 'OSIS', 'OSK', 'OSPN', 'OSTK', 'OSUR', 'OTEX', 'OTIC', 'OTTR', 'OVV', 'OXM', 'OXY', 'OZK', 'P', 'PAAS', 'PACB', 'PACQ', 'PACW', 'PAG', 'PAGP', 'PAGS', 'PAH', 'PAHC', 'PANW', 'PAR', 'PARR', 'PATK', 'PAYC', 'PAYS', 'PAYX', 'PB', 'PBCT', 'PBF', 'PBH', 'PBI', 'PBPB', 'PBYI', 'PCAR', 'PCG', 'PCOM', 'PCRX', 'PCTI', 'PCTY', 'PCYG', 'PD', 'PDCE', 'PDCO', 'PDFS', 'PDLI', 'PE', 'PEBO', 'PEG', 'PEGA', 'PEGI', 'PEIX', 'PEN', 'PENN', 'PEP', 'PERI', 'PETQ', 'PETS', 'PFBC', 'PFBI', 'PFE', 'PFG', 'PFGC', 'PFIS', 'PFNX', 'PFPT', 'PFS', 'PFSI', 'PFSW', 'PG', 'PGC', 'PGNX', 'PGNY', 'PGR', 'PGTI', 'PH', 'PHAS', 'PHAT', 'PHM', 'PHR', 'PHX', 'PI', 'PICO', 'PII', 'PINC', 'PING', 'PINS', 'PIR', 'PIRS', 'PJT', 'PKE', 'PKG', 'PKI', 'PKOH', 'PLAB', 'PLAN', 'PLAY', 'PLCE', 'PLIN', 'PLMR', 'PLNT', 'PLOW', 'PLPC', 'PLSE', 'PLT', 'PLUG', 'PLUS', 'PLXP', 'PLXS', 'PLYA', 'PM', 'PMD', 'PME', 'PNC', 'PNFP', 'PNM', 'PNR', 'PNRG', 'PNTG', 'PNW', 'PODD', 'POL', 'POOL', 'POR', 'POST', 'POWI', 'POWL', 'PPBI', 'PPC', 'PPG', 'PPIH', 'PPL', 'PPSI', 'PQG', 'PRA', 'PRAA', 'PRAH', 'PRCP', 'PRFT', 'PRGO', 'PRGS', 'PRI', 'PRIM', 'PRK', 'PRLB', 'PRMW', 'PRNB', 'PRO', 'PROS', 'PROV', 'PRPL', 'PRSC', 'PRSP', 'PRTA', 'PRTK', 'PRTY', 'PRU', 'PRVB', 'PRVL', 'PS', 'PSMT', 'PSN', 'PSNL', 'PSTG', 'PSTI', 'PSTV', 'PSX', 'PTC', 'PTCT', 'PTEN', 'PTGX', 'PTI', 'PTLA', 'PTON', 'PTSI', 'PTVCB', 'PUB', 'PUMP', 'PVG', 'PVH', 'PWOD', 'PWR', 'PXD', 'PXLW', 'PYPL', 'PZN', 'PZZA', 'QADA', 'QCOM', 'QCRH', 'QDEL', 'QEP', 'QLYS', 'QNST', 'QRTEA', 'QRVO', 'QSR', 'QTRX', 'QTWO', 'QUAD', 'QUIK', 'QUMU', 'QUOT', 'R', 'RACE', 'RAD', 'RADA', 'RAIL', 'RAMP', 'RAPT', 'RARE', 'RAVE', 'RAVN', 'RBA', 'RBBN', 'RBC', 'RBCAA', 'RBCN', 'RCI', 'RCII', 'RCKT', 'RCKY', 'RCL', 'RCM', 'RCUS', 'RDFN', 'RDI', 'RDN', 'RDNT', 'RDUS', 'RDWR', 'RE', 'REAL', 'RECN', 'REGI', 'REGN', 'RELL', 'REPH', 'REPL', 'RES', 'RESN', 'RETA', 'REV', 'REVG', 'REX', 'REZI', 'RF', 'RFIL', 'RFL', 'RFP', 'RGA', 'RGEN', 'RGLD', 'RGNX', 'RGR', 'RGS', 'RH', 'RHI', 'RICK', 'RILY', 'RIOT', 'RJF', 'RL', 'RLGY', 'RLH', 'RLI', 'RM', 'RMAX', 'RMBS', 'RMCF', 'RMD', 'RMNI', 'RMR', 'RMTI', 'RNET', 'RNG', 'RNR', 'RNST', 'RNWK', 'ROCK', 'ROG', 'ROK', 'ROKU', 'ROL', 'ROLL', 'ROP', 'ROST', 'RP', 'RPAY', 'RPD', 'RPM', 'RRC', 'RRD', 'RRGB', 'RRR', 'RRTS', 'RS', 'RSG', 'RST', 'RTIX', 'RTN', 'RTRX', 'RUBI', 'RUBY', 'RUN', 'RUSHA', 'RUTH', 'RVLV', 'RVNC', 'RWLK', 'RXN', 'RY', 'RYI', 'RYTM', 'S', 'SA', 'SABR', 'SAFM', 'SAFT', 'SAGE', 'SAH', 'SAIA', 'SAIC', 'SAIL', 'SAM', 'SANM', 'SANW', 'SASR', 'SATS', 'SAVE', 'SBCF', 'SBGI', 'SBH', 'SBNY', 'SBPH', 'SBSI', 'SBT', 'SBUX', 'SC', 'SCCO', 'SCHL', 'SCHN', 'SCHW', 'SCI', 'SCKT', 'SCL', 'SCOR', 'SCPL', 'SCS', 'SCSC', 'SCU', 'SCVL', 'SCWX', 'SCX', 'SDC', 'SDRL', 'SEAS', 'SEB', 'SEDG', 'SEE', 'SEIC', 'SEM', 'SENEA', 'SERV', 'SF', 'SFBS', 'SFE', 'SFIX', 'SFM', 'SFNC', 'SG', 'SGA', 'SGBX', 'SGC', 'SGEN', 'SGH', 'SGMO', 'SGMS', 'SGRY', 'SGU', 'SHAK', 'SHEN', 'SHLD', 'SHLO', 'SHOO', 'SHOP', 'SHW', 'SIBN', 'SIEN', 'SIF', 'SIG', 'SIGA', 'SIGI', 'SILK', 'SINA', 'SINT', 'SIRI', 'SITE', 'SIVB', 'SIX', 'SJI', 'SJM', 'SJR', 'SJW', 'SKX', 'SKY', 'SKYW', 'SLAB', 'SLB', 'SLCA', 'SLCT', 'SLDB', 'SLF', 'SLGG', 'SLGN', 'SLM', 'SLP', 'SLRX', 'SM', 'SMAR', 'SMBC', 'SMED', 'SMG', 'SMMF', 'SMP', 'SMPL', 'SMTC', 'SMTX', 'SNA', 'SNAP', 'SNBR', 'SNCR', 'SND', 'SNDX', 'SNOA', 'SNPS', 'SNSS', 'SNV', 'SNX', 'SO', 'SOI', 'SON', 'SONM', 'SONO', 'SORL', 'SP', 'SPAR', 'SPB', 'SPCE', 'SPFI', 'SPGI', 'SPKE', 'SPLK', 'SPN', 'SPNE', 'SPNS', 'SPOK', 'SPOT', 'SPPI', 'SPR', 'SPSC', 'SPTN', 'SPWR', 'SPXC', 'SQ', 'SR', 'SRAX', 'SRCE', 'SRCL', 'SRDX', 'SRE', 'SRI', 'SRL', 'SRPT', 'SRRK', 'SSB', 'SSD', 'SSI', 'SSNC', 'SSP', 'SSRM', 'SSTI', 'SSTK', 'SSYS', 'ST', 'STAA', 'STBA', 'STC', 'STE', 'STFC', 'STIM', 'STL', 'STLD', 'STMP', 'STNE', 'STOK', 'STRA', 'STRL', 'STRO', 'STRS', 'STRT', 'STSA', 'STT', 'STX', 'STZ', 'SU', 'SUM', 'SUP', 'SUPN', 'SVMK', 'SWAV', 'SWCH', 'SWIR', 'SWK', 'SWKS', 'SWM', 'SWN', 'SWTX', 'SWX', 'SXC', 'SXI', 'SXT', 'SYBT', 'SYBX', 'SYF', 'SYK', 'SYKE', 'SYNA', 'SYNC', 'SYNH', 'SYRS', 'SYX', 'SYY', 'T', 'TA', 'TACO', 'TACT', 'TALO', 'TAP', 'TARA', 'TARO', 'TAST', 'TBBK', 'TBI', 'TBIO', 'TBK', 'TBNK', 'TBPH', 'TCBI', 'TCBK', 'TCDA', 'TCMD', 'TCON', 'TCRR', 'TCS', 'TCX', 'TD', 'TDC', 'TDG', 'TDOC', 'TDS', 'TDW', 'TDY', 'TEAM', 'TECD', 'TECH', 'TECK', 'TEL', 'TELL', 'TEN', 'TENB', 'TER', 'TERP', 'TESS', 'TEX', 'TFC', 'TFSL', 'TFX', 'TG', 'TGE', 'TGEN', 'TGH', 'TGI', 'TGLS', 'TGNA', 'TGP', 'TGT', 'TGTX', 'THC', 'THFF', 'THG', 'THMO', 'THO', 'THR', 'THRM', 'THS', 'TIF', 'TILE', 'TISI', 'TIVO', 'TJX', 'TKKS', 'TKR', 'TLRA', 'TLRD', 'TLRY', 'TLYS', 'TMHC', 'TMO', 'TMP', 'TMST', 'TMUS', 'TNAV', 'TNC', 'TNDM', 'TNET', 'TNP', 'TOL', 'TORC', 'TOWN', 'TPC', 'TPCO', 'TPH', 'TPIC', 'TPR', 'TPRE', 'TPTX', 'TPX', 'TR', 'TRC', 'TREE', 'TREX', 'TRGP', 'TRHC', 'TRI', 'TRIP', 'TRMB', 'TRMK', 'TRN', 'TROW', 'TROX', 'TRP', 'TRQ', 'TRS', 'TRST', 'TRT', 'TRTN', 'TRU', 'TRUE', 'TRUP', 'TRV', 'TRWH', 'TRXC', 'TSC', 'TSCO', 'TSEM', 'TSG', 'TSLA', 'TSN', 'TSQ', 'TSRI', 'TTC', 'TTD', 'TTEC', 'TTEK', 'TTGT', 'TTMI', 'TTOO', 'TTPH', 'TTWO', 'TUFN', 'TUP', 'TUSK', 'TVTY', 'TW', 'TWIN', 'TWLO', 'TWNK', 'TWOU', 'TWST', 'TWTR', 'TXG', 'TXMD', 'TXN', 'TXRH', 'TXT', 'TYL', 'TZOO', 'UAA', 'UAL', 'UBER', 'UBSI', 'UBX', 'UCBI', 'UCTT', 'UEIC', 'UFCS', 'UFI', 'UFPI', 'UFPT', 'UFS', 'UG', 'UGI', 'UHAL', 'UHS', 'UI', 'UIHC', 'UIS', 'ULBI', 'ULH', 'ULTA', 'UMBF', 'UMPQ', 'UNB', 'UNF', 'UNFI', 'UNH', 'UNM', 'UNP', 'UNT', 'UNVR', 'UPLD', 'UPS', 'UPWK', 'URBN', 'URGN', 'URI', 'UROV', 'USAK', 'USAP', 'USAS', 'USB', 'USCR', 'USFD', 'USLM', 'USM', 'USNA', 'USPH', 'USX', 'UTHR', 'UTI', 'UTL', 'UTMD', 'UTSI', 'UTX', 'UVE', 'UVSP', 'UVV', 'V', 'VAC', 'VAL', 'VALU', 'VAPO', 'VAR', 'VBTX', 'VC', 'VCEL', 'VCNX', 'VCRA', 'VCYT', 'VEC', 'VECO', 'VEEV', 'VERI', 'VERU', 'VFC', 'VGR', 'VHI', 'VIAC', 'VIAV', 'VICR', 'VIE', 'VIR', 'VIRT', 'VIVE', 'VIVO', 'VKTX', 'VLGEA', 'VLO', 'VLY', 'VMC', 'VMI', 'VMW', 'VNCE', 'VNDA', 'VNE', 'VOXX', 'VOYA', 'VPG', 'VRA', 'VRAY', 'VRCA', 'VREX', 'VRNS', 'VRNT', 'VRRM', 'VRS', 'VRSK', 'VRSN', 'VRTS', 'VRTU', 'VRTV', 'VRTX', 'VSAT', 'VSEC', 'VSH', 'VSLR', 'VST', 'VSTO', 'VUZI', 'VVI', 'VVUS', 'VVV', 'VYGR', 'VZ', 'W', 'WAB', 'WABC', 'WAFD', 'WAL', 'WASH', 'WAT', 'WATT', 'WBA', 'WBS', 'WBT', 'WCC', 'WCN', 'WD', 'WDAY', 'WDC', 'WDFC', 'WDR', 'WEC', 'WEN', 'WERN', 'WETF', 'WEX', 'WEYS', 'WFC', 'WGO', 'WH', 'WHD', 'WHG', 'WHR', 'WIFI', 'WINA', 'WING', 'WINS', 'WIRE', 'WIX', 'WK', 'WLDN', 'WLFC', 'WLH', 'WLK', 'WLL', 'WLTW', 'WM', 'WMB', 'WMK', 'WMS', 'WMT', 'WNC', 'WNEB', 'WOR', 'WORK', 'WORX', 'WOW', 'WPM', 'WPRT', 'WPX', 'WRB', 'WRK', 'WRLD', 'WRTC', 'WSBC', 'WSBF', 'WSC', 'WSFS', 'WSM', 'WSO', 'WST', 'WTBA', 'WTFC', 'WTI', 'WTM', 'WTR', 'WTRE', 'WTS', 'WU', 'WVE', 'WVFC', 'WVVI', 'WW', 'WWD', 'WWE', 'WWR', 'WWW', 'WYND', 'WYNN', 'X', 'XAIR', 'XBIT', 'XEC', 'XEL', 'XENE', 'XENT', 'XLNX', 'XLRN', 'XNCR', 'XOG', 'XOM', 'XOMA', 'XON', 'XONE', 'XPEL', 'XPER', 'XPO', 'XRAY', 'XRX', 'XYL', 'Y', 'YELP', 'YETI', 'YEXT', 'YMAB', 'YNDX', 'YORW', 'YRCW', 'YUM', 'YUMA', 'YUMC', 'ZAGG', 'ZBH', 'ZBRA', 'ZEN', 'ZFGN', 'ZG', 'ZGNX', 'ZION', 'ZIOP', 'ZIXI', 'ZM', 'ZNGA', 'ZS', 'ZTS', 'ZUMZ', 'ZUO', 'ZVO', 'ZYME', 'ZYNE']\n"
     ]
    }
   ],
   "source": [
    "ticker_file = open(\"ticker.txt\")\n",
    "data = ticker_file.readlines()\n",
    "ticker_file.close()\n",
    "\n",
    "ticker_list = [i.rstrip('\\n') for i in data]\n",
    "\n",
    "print(len(ticker_list))\n",
    "print(ticker_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.stocktwits.com/api/2/streams/symbol/BBRY.json\n",
      "./data/BBRY_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/AAPL.json\n",
      "./data/AAPL_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/AMZN.json\n",
      "./data/AMZN_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/BABA.json\n",
      "./data/BABA_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/YHOO.json\n",
      "./data/YHOO_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/LQMT.json\n",
      "./data/LQMT_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/FB.json\n",
      "./data/FB_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/GOOG.json\n",
      "./data/GOOG_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/BBBY.json\n",
      "./data/BBBY_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/JNUG.json\n",
      "./data/JNUG_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/SBUX.json\n",
      "./data/SBUX_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/MU.json\n",
      "./data/MU_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/MRVL.json\n",
      "./data/MRVL_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ADMA.json\n",
      "./data/ADMA_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/HIG.json\n",
      "./data/HIG_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/LGND.json\n",
      "./data/LGND_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/DORM.json\n",
      "./data/DORM_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/GIB.json\n",
      "./data/GIB_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/SIEN.json\n",
      "./data/SIEN_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/NFE.json\n",
      "./data/NFE_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/CSFL.json\n",
      "./data/CSFL_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/LM.json\n",
      "./data/LM_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/CERN.json\n",
      "./data/CERN_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/NEE.json\n",
      "./data/NEE_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/FULC.json\n",
      "./data/FULC_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/SGBX.json\n",
      "./data/SGBX_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/TROW.json\n",
      "./data/TROW_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/CWBC.json\n",
      "./data/CWBC_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/TFC.json\n",
      "./data/TFC_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/SBSI.json\n",
      "./data/SBSI_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/DGLY.json\n",
      "./data/DGLY_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/KODK.json\n",
      "./data/KODK_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/BLD.json\n",
      "./data/BLD_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/REZI.json\n",
      "./data/REZI_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/MOS.json\n",
      "./data/MOS_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/SRAX.json\n",
      "./data/SRAX_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/PRA.json\n",
      "./data/PRA_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/CTL.json\n",
      "./data/CTL_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/COLB.json\n",
      "./data/COLB_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/EFC.json\n",
      "./data/EFC_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/BBY.json\n",
      "./data/BBY_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/DK.json\n",
      "./data/DK_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/JILL.json\n",
      "./data/JILL_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/IIIV.json\n",
      "./data/IIIV_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/AJG.json\n",
      "./data/AJG_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/OCGN.json\n",
      "./data/OCGN_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/JBL.json\n",
      "./data/JBL_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ALG.json\n",
      "./data/ALG_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/PSN.json\n",
      "./data/PSN_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/WYND.json\n",
      "./data/WYND_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/MRNA.json\n",
      "./data/MRNA_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ABG.json\n",
      "./data/ABG_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ACIA.json\n",
      "./data/ACIA_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/VRA.json\n",
      "./data/VRA_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/CSS.json\n",
      "./data/CSS_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/SPAR.json\n",
      "./data/SPAR_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/CZNC.json\n",
      "./data/CZNC_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/GWB.json\n",
      "./data/GWB_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/BRKL.json\n",
      "./data/BRKL_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/UTMD.json\n",
      "./data/UTMD_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/MRIN.json\n",
      "./data/MRIN_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/BZH.json\n",
      "./data/BZH_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/WMS.json\n",
      "./data/WMS_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/SNV.json\n",
      "./data/SNV_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/DBD.json\n",
      "./data/DBD_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/LNN.json\n",
      "./data/LNN_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/FBIZ.json\n",
      "./data/FBIZ_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/RBBN.json\n",
      "./data/RBBN_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/SWCH.json\n",
      "./data/SWCH_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/INVA.json\n",
      "./data/INVA_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/AN.json\n",
      "./data/AN_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/FBHS.json\n",
      "./data/FBHS_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/CDAY.json\n",
      "./data/CDAY_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/PNFP.json\n",
      "./data/PNFP_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/EPAM.json\n",
      "./data/EPAM_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/SCI.json\n",
      "./data/SCI_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/VYGR.json\n",
      "./data/VYGR_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/DFS.json\n",
      "./data/DFS_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/FSLY.json\n",
      "./data/FSLY_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/SSRM.json\n",
      "./data/SSRM_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/NEOG.json\n",
      "./data/NEOG_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/QLYS.json\n",
      "./data/QLYS_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/CHMA.json\n",
      "./data/CHMA_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ANF.json\n",
      "./data/ANF_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/CSTL.json\n",
      "./data/CSTL_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/HMHC.json\n",
      "./data/HMHC_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ADES.json\n",
      "./data/ADES_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/CORT.json\n",
      "./data/CORT_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/EYE.json\n",
      "./data/EYE_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/KFY.json\n",
      "./data/KFY_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/RNWK.json\n",
      "./data/RNWK_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/JJSF.json\n",
      "./data/JJSF_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/CRK.json\n",
      "./data/CRK_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/MANT.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/MANT_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/IMMR.json\n",
      "./data/IMMR_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ADUS.json\n",
      "./data/ADUS_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/AR.json\n",
      "./data/AR_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ATO.json\n",
      "./data/ATO_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/NRC.json\n",
      "./data/NRC_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/BCC.json\n",
      "./data/BCC_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/MATX.json\n",
      "./data/MATX_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/CZZ.json\n",
      "./data/CZZ_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ADS.json\n",
      "./data/ADS_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/LFUS.json\n",
      "./data/LFUS_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ENVA.json\n",
      "./data/ENVA_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/WIRE.json\n",
      "./data/WIRE_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/PTEN.json\n",
      "./data/PTEN_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/CDW.json\n",
      "./data/CDW_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/PCTY.json\n",
      "./data/PCTY_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/TOL.json\n",
      "./data/TOL_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ATRC.json\n",
      "./data/ATRC_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/AEE.json\n",
      "./data/AEE_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/TRI.json\n",
      "./data/TRI_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/PINC.json\n",
      "./data/PINC_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/INFN.json\n",
      "./data/INFN_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/TELL.json\n",
      "./data/TELL_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/MTDR.json\n",
      "./data/MTDR_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/SAIC.json\n",
      "./data/SAIC_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/EBIX.json\n",
      "./data/EBIX_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/LCI.json\n",
      "./data/LCI_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/VNE.json\n",
      "./data/VNE_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/YUMA.json\n",
      "./data/YUMA_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/CCS.json\n",
      "./data/CCS_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/SCSC.json\n",
      "./data/SCSC_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/COP.json\n",
      "./data/COP_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/BCOR.json\n",
      "./data/BCOR_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/PLCE.json\n",
      "./data/PLCE_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/CSWI.json\n",
      "./data/CSWI_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ISEE.json\n",
      "./data/ISEE_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/FARM.json\n",
      "./data/FARM_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ECOM.json\n",
      "./data/ECOM_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/CSPI.json\n",
      "./data/CSPI_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/CSIQ.json\n",
      "./data/CSIQ_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/HAE.json\n",
      "./data/HAE_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/CORE.json\n",
      "./data/CORE_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/RMD.json\n",
      "./data/RMD_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/BLBD.json\n",
      "./data/BLBD_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/QTWO.json\n",
      "./data/QTWO_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/XRAY.json\n",
      "./data/XRAY_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ABEO.json\n",
      "./data/ABEO_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/CO.json\n",
      "./data/CO_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/VERI.json\n",
      "./data/VERI_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/PLOW.json\n",
      "./data/PLOW_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/DELL.json\n",
      "./data/DELL_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/TDG.json\n",
      "./data/TDG_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/BNFT.json\n",
      "./data/BNFT_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/GBX.json\n",
      "./data/GBX_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/PRGO.json\n",
      "./data/PRGO_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/MLND.json\n",
      "./data/MLND_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/AVNS.json\n",
      "./data/AVNS_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/OFLX.json\n",
      "./data/OFLX_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/DBI.json\n",
      "./data/DBI_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/FIZZ.json\n",
      "./data/FIZZ_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/DDS.json\n",
      "./data/DDS_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/ALGN.json\n",
      "./data/ALGN_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/LLY.json\n",
      "./data/LLY_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/IO.json\n",
      "./data/IO_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/CWT.json\n",
      "./data/CWT_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/MDT.json\n",
      "./data/MDT_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/CYD.json\n",
      "./data/CYD_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/CYH.json\n",
      "./data/CYH_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/SLDB.json\n",
      "./data/SLDB_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/WLK.json\n",
      "./data/WLK_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/FND.json\n",
      "./data/FND_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/MPWR.json\n",
      "./data/MPWR_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/PTLA.json\n",
      "./data/PTLA_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/XBIT.json\n",
      "./data/XBIT_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/MSCI.json\n",
      "./data/MSCI_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/INBK.json\n",
      "./data/INBK_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/CDK.json\n",
      "./data/CDK_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/AVY.json\n",
      "./data/AVY_20200131_0150.json\n",
      "https://api.stocktwits.com/api/2/streams/symbol/UIHC.json\n"
     ]
    }
   ],
   "source": [
    "symbols = ['BBRY', 'AAPL', 'AMZN', 'BABA', 'YHOO', 'FB', 'GOOG', 'BBBY', 'JNUG', 'SBUX', 'MU']\n",
    "\n",
    "NUM_REQUEST = 200 - len(symbols)\n",
    "\n",
    "#symbols.extend(ticker_list[0:50])\n",
    "random.seed(12345)\n",
    "symbols.extend(random.sample(ticker_list, NUM_REQUEST))\n",
    "\n",
    "args = ['curl', '-X', 'GET', '']\n",
    "URL = \"https://api.stocktwits.com/api/2/streams/symbol/\"\n",
    "\n",
    "FILE_PATH = \"./data/\"\n",
    "\n",
    "start_datetime = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "for symbol in symbols:\n",
    "    try:\n",
    "        args[3] = URL + symbol + \".json\"\n",
    "        print(args[3])\n",
    "        proc = subprocess.run(args,stdout = subprocess.PIPE, stderr = subprocess.PIPE)\n",
    "\n",
    "        path = FILE_PATH + symbol + \"_\" + start_datetime + \".json\"\n",
    "        print(path)\n",
    "        with open(path, mode='w') as f:\n",
    "            f.write(proc.stdout.decode(\"utf8\"))\n",
    "    except:\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正常なレスポンス・ステータスを持っていないファイルを取り除きます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/LQMT_20200131_0150.json\n",
      "data/CSWI_20200131_0150.json\n",
      "data/INBK_20200131_0150.json\n"
     ]
    }
   ],
   "source": [
    "!grep -rlv '{\"response\":{\"status\":200}' data\n",
    "!grep -rlv '{\"response\":{\"status\":200}' data | xargs rm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、保存したファイルを、分散処理環境（クラスター）を使って加工するためにHDFSへコピーします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/BCC_20200131_0150.json\n",
      "./data/QLYS_20200131_0150.json\n",
      "./data/EFC_20200131_0150.json\n",
      "./data/CDW_20200131_0150.json\n",
      "./data/SBUX_20200131_0150.json\n",
      "./data/AJG_20200131_0150.json\n",
      "./data/MPWR_20200131_0150.json\n",
      "./data/AEE_20200131_0150.json\n",
      "./data/PTEN_20200131_0150.json\n",
      "./data/GWB_20200131_0150.json\n",
      "./data/NFE_20200131_0150.json\n",
      "./data/DK_20200131_0150.json\n",
      "./data/CZZ_20200131_0150.json\n",
      "./data/FULC_20200131_0150.json\n",
      "./data/ENVA_20200131_0150.json\n",
      "./data/COP_20200131_0150.json\n",
      "./data/SMED_20200131_0150.json\n",
      "./data/GIB_20200131_0150.json\n",
      "./data/CSS_20200131_0150.json\n",
      "./data/SLDB_20200131_0150.json\n",
      "./data/SNV_20200131_0150.json\n",
      "./data/ADS_20200131_0150.json\n",
      "./data/CORE_20200131_0150.json\n",
      "./data/LM_20200131_0150.json\n",
      "./data/OCGN_20200131_0150.json\n",
      "./data/CFFI_20200131_0150.json\n",
      "./data/CORT_20200131_0150.json\n",
      "./data/SBSI_20200131_0150.json\n",
      "./data/JBL_20200131_0150.json\n",
      "./data/EBS_20200131_0150.json\n",
      "./data/INVA_20200131_0150.json\n",
      "./data/CSPI_20200131_0150.json\n",
      "./data/MRNA_20200131_0150.json\n",
      "./data/ANF_20200131_0150.json\n",
      "./data/PSMT_20200131_0150.json\n",
      "./data/FBIZ_20200131_0150.json\n",
      "./data/NRC_20200131_0150.json\n",
      "./data/MDT_20200131_0150.json\n",
      "./data/GOOG_20200131_0150.json\n",
      "./data/VRA_20200131_0150.json\n",
      "./data/WIRE_20200131_0150.json\n",
      "./data/FND_20200131_0150.json\n",
      "./data/BZH_20200131_0150.json\n",
      "./data/HAE_20200131_0150.json\n",
      "./data/FSLY_20200131_0150.json\n",
      "./data/SSRM_20200131_0150.json\n",
      "./data/RMD_20200131_0150.json\n",
      "./data/NEOG_20200131_0150.json\n",
      "./data/ATH_20200131_0150.json\n",
      "./data/AMZN_20200131_0150.json\n",
      "./data/VNE_20200131_0150.json\n",
      "./data/MOS_20200131_0150.json\n",
      "./data/YUMA_20200131_0150.json\n",
      "./data/MRVL_20200131_0150.json\n",
      "./data/CSFL_20200131_0150.json\n",
      "./data/PRA_20200131_0150.json\n",
      "./data/CDK_20200131_0150.json\n",
      "./data/DGLY_20200131_0150.json\n",
      "./data/AVYA_20200131_0150.json\n",
      "./data/TOL_20200131_0150.json\n",
      "./data/CWBC_20200131_0150.json\n",
      "./data/SRAX_20200131_0150.json\n",
      "./data/RNWK_20200131_0150.json\n",
      "./data/SWCH_20200131_0150.json\n",
      "./data/DBD_20200131_0150.json\n",
      "./data/DORM_20200131_0150.json\n",
      "./data/FB_20200131_0150.json\n",
      "./data/CYH_20200131_0150.json\n",
      "./data/AN_20200131_0150.json\n",
      "./data/ADES_20200131_0150.json\n",
      "./data/CAG_20200131_0150.json\n",
      "./data/BBRY_20200131_0150.json\n",
      "./data/ISEE_20200131_0150.json\n",
      "./data/YHOO_20200131_0150.json\n",
      "./data/IMMR_20200131_0150.json\n",
      "./data/MLND_20200131_0150.json\n",
      "./data/FARM_20200131_0150.json\n",
      "./data/REX_20200131_0150.json\n",
      "./data/ATO_20200131_0150.json\n",
      "./data/COLB_20200131_0150.json\n",
      "./data/XBIT_20200131_0150.json\n",
      "./data/CYD_20200131_0150.json\n",
      "./data/ACIA_20200131_0150.json\n",
      "./data/EBIX_20200131_0150.json\n",
      "./data/LFUS_20200131_0150.json\n",
      "./data/CDAY_20200131_0150.json\n",
      "./data/CO_20200131_0150.json\n",
      "./data/ECOM_20200131_0150.json\n",
      "./data/SGBX_20200131_0150.json\n",
      "./data/FIZZ_20200131_0150.json\n",
      "./data/ABEO_20200131_0150.json\n",
      "./data/KFY_20200131_0150.json\n",
      "./data/PCTY_20200131_0150.json\n",
      "./data/BNFT_20200131_0150.json\n",
      "./data/IIIV_20200131_0150.json\n",
      "./data/LGND_20200131_0150.json\n",
      "./data/DDS_20200131_0150.json\n",
      "./data/CWT_20200131_0150.json\n",
      "./data/VERI_20200131_0150.json\n",
      "./data/EPAM_20200131_0150.json\n",
      "./data/HIG_20200131_0150.json\n",
      "./data/BLD_20200131_0150.json\n",
      "./data/NEE_20200131_0150.json\n",
      "./data/BBY_20200131_0150.json\n",
      "./data/TELL_20200131_0150.json\n",
      "./data/LNN_20200131_0150.json\n",
      "./data/ALGN_20200131_0150.json\n",
      "./data/CERN_20200131_0150.json\n",
      "./data/SAIC_20200131_0150.json\n",
      "./data/UTMD_20200131_0150.json\n",
      "./data/AR_20200131_0150.json\n",
      "./data/MU_20200131_0150.json\n",
      "./data/PRGO_20200131_0150.json\n",
      "./data/BLBD_20200131_0150.json\n",
      "./data/XNCR_20200131_0150.json\n",
      "./data/MANT_20200131_0150.json\n",
      "./data/OFLX_20200131_0150.json\n",
      "./data/IO_20200131_0150.json\n",
      "./data/SPAR_20200131_0150.json\n",
      "./data/DFS_20200131_0150.json\n",
      "./data/JJSF_20200131_0150.json\n",
      "./data/SINT_20200131_0150.json\n",
      "./data/HMHC_20200131_0150.json\n",
      "./data/VYGR_20200131_0150.json\n",
      "./data/ALG_20200131_0150.json\n",
      "./data/TFC_20200131_0150.json\n",
      "./data/BABA_20200131_0150.json\n",
      "./data/FBHS_20200131_0150.json\n",
      "./data/TRU_20200131_0150.json\n",
      "./data/WMS_20200131_0150.json\n",
      "./data/PSN_20200131_0150.json\n",
      "./data/PNFP_20200131_0150.json\n",
      "./data/CTL_20200131_0150.json\n",
      "./data/LLY_20200131_0150.json\n",
      "./data/HCAT_20200131_0150.json\n",
      "./data/WLK_20200131_0150.json\n",
      "./data/RLI_20200131_0150.json\n",
      "./data/AOS_20200131_0150.json\n",
      "./data/ATRC_20200131_0150.json\n",
      "./data/PZZA_20200131_0150.json\n",
      "./data/FBMS_20200131_0150.json\n",
      "./data/WAFD_20200131_0150.json\n",
      "./data/JNUG_20200131_0150.json\n",
      "./data/BRKL_20200131_0150.json\n",
      "./data/SIEN_20200131_0150.json\n",
      "./data/CHMA_20200131_0150.json\n",
      "./data/REZI_20200131_0150.json\n",
      "./data/BBBY_20200131_0150.json\n",
      "./data/MYOK_20200131_0150.json\n",
      "./data/SCSC_20200131_0150.json\n",
      "./data/CZNC_20200131_0150.json\n",
      "./data/ADMA_20200131_0150.json\n",
      "./data/TRI_20200131_0150.json\n",
      "./data/TRQ_20200131_0150.json\n",
      "./data/CCS_20200131_0150.json\n",
      "./data/LCI_20200131_0150.json\n",
      "./data/DELL_20200131_0150.json\n",
      "./data/DRAD_20200131_0150.json\n",
      "./data/BCOR_20200131_0150.json\n",
      "./data/CSIQ_20200131_0150.json\n",
      "./data/PLCE_20200131_0150.json\n",
      "./data/SLCA_20200131_0150.json\n",
      "./data/MTDR_20200131_0150.json\n",
      "./data/LBY_20200131_0150.json\n",
      "./data/RBBN_20200131_0150.json\n",
      "./data/DBI_20200131_0150.json\n",
      "./data/TDG_20200131_0150.json\n",
      "./data/UIHC_20200131_0150.json\n",
      "./data/KODK_20200131_0150.json\n",
      "./data/CSTL_20200131_0150.json\n",
      "./data/QTWO_20200131_0150.json\n",
      "./data/GBX_20200131_0150.json\n",
      "./data/PINC_20200131_0150.json\n",
      "./data/ABG_20200131_0150.json\n",
      "./data/PTLA_20200131_0150.json\n",
      "./data/SCI_20200131_0150.json\n",
      "./data/TNDM_20200131_0150.json\n",
      "./data/AVNS_20200131_0150.json\n",
      "./data/AVY_20200131_0150.json\n",
      "./data/AAPL_20200131_0150.json\n",
      "./data/CRK_20200131_0150.json\n",
      "./data/MSCI_20200131_0150.json\n",
      "./data/TROW_20200131_0150.json\n",
      "./data/WYND_20200131_0150.json\n",
      "./data/INVE_20200131_0150.json\n",
      "./data/MATX_20200131_0150.json\n",
      "./data/OFIX_20200131_0150.json\n",
      "./data/RNET_20200131_0150.json\n",
      "./data/PLOW_20200131_0150.json\n",
      "./data/CLDX_20200131_0150.json\n",
      "./data/JILL_20200131_0150.json\n",
      "./data/INFN_20200131_0150.json\n",
      "./data/MRIN_20200131_0150.json\n",
      "./data/FTR_20200131_0150.json\n",
      "./data/XRAY_20200131_0150.json\n",
      "./data/EYE_20200131_0150.json\n",
      "./data/ADUS_20200131_0150.json\n"
     ]
    }
   ],
   "source": [
    "os.environ['HADOOP_CONF_DIR'] = \"/etc/spark/conf/yarn-conf\"\n",
    "\n",
    "HDFS_PATH_DIR = './twits/'\n",
    "\n",
    "args = ['hdfs', 'dfs', '-put', '', HDFS_PATH_DIR]\n",
    "\n",
    "\n",
    "try:\n",
    "    args_mkdir = ['hdfs', 'dfs', '-mkdir', HDFS_PATH_DIR]\n",
    "    proc = subprocess.run(args_mkdir,stdout = subprocess.PIPE, stderr = subprocess.PIPE)\n",
    "except:\n",
    "    traceback.print_exc()\n",
    "\n",
    "file_list = glob.glob(\"./data/*\")\n",
    "\n",
    "\n",
    "for file in file_list:\n",
    "    try:\n",
    "        args[3] = file\n",
    "        print(file)\n",
    "\n",
    "        proc = subprocess.run(args,stdout = subprocess.PIPE, stderr = subprocess.PIPE)\n",
    "\n",
    "    except:\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. データ変換\n",
    "\n",
    "クラスターでデータを変換します。CDSW上では、ユーザーごとに別のプロジェクトを使っていましたが、クラスター環境では、自分が利用しているユーザーとデータを意識して取り扱う必要があります。\n",
    "\n",
    "\n",
    "あなたの（HADOOPクラスターへアクセスする）ユーザ名は以下で確認できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user4\r\n"
     ]
    }
   ],
   "source": [
    "!echo $HADOOP_USER_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#$ beeline -u 'jdbc:hive2://10.0.0.55:10000' -f tables.hql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データベースの準備\n",
    "\n",
    "\n",
    "\n",
    "**下記のセルの中を適切なユーザ名とURL（Hiveサーバー）に置換してください。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Engine(hive://user2@master.ykono.work:10000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlalchemy.create_engine('hive://user4@master.ykono.work:10000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**下記のセルの中を適切なユーザ名とURL（Hiveサーバー）に置換してください。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Connected: user4@None'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql hive://user4@master.ykono.work:10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**あなたのユーザ名でデータベースを作成・利用してください**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hive://master.ykono.work:10000\n",
      " * hive://user4@master.ykono.work:10000\n",
      "(pyhive.exc.OperationalError) TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=['*org.apache.hive.service.cli.HiveSQLException:Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Database user4 already exists:28:27', 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:329', 'org.apache.hive.service.cli.operation.SQLOperation:runQuery:SQLOperation.java:258', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:293', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:260', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:505', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatement:HiveSessionImpl.java:480', 'sun.reflect.GeneratedMethodAccessor31:invoke::-1', 'sun.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:498', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:-2', 'javax.security.auth.Subject:doAs:Subject.java:422', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1875', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'com.sun.proxy.$Proxy37:executeStatement::-1', 'org.apache.hive.service.cli.CLIService:executeStatement:CLIService.java:270', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:508', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1437', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1422', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:39', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:39', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:286', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1149', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:624', 'java.lang.Thread:run:Thread.java:748', '*org.apache.hadoop.hive.ql.metadata.HiveException:Database user4 already exists:36:9', 'org.apache.hadoop.hive.ql.exec.DDLTask:createDatabase:DDLTask.java:4198', 'org.apache.hadoop.hive.ql.exec.DDLTask:execute:DDLTask.java:308', 'org.apache.hadoop.hive.ql.exec.Task:executeTask:Task.java:199', 'org.apache.hadoop.hive.ql.exec.TaskRunner:runSequential:TaskRunner.java:97', 'org.apache.hadoop.hive.ql.Driver:launchTask:Driver.java:2200', 'org.apache.hadoop.hive.ql.Driver:execute:Driver.java:1843', 'org.apache.hadoop.hive.ql.Driver:runInternal:Driver.java:1563', 'org.apache.hadoop.hive.ql.Driver:run:Driver.java:1339', 'org.apache.hadoop.hive.ql.Driver:run:Driver.java:1334', 'org.apache.hive.service.cli.operation.SQLOperation:runQuery:SQLOperation.java:256', '*org.apache.hadoop.hive.metastore.api.AlreadyExistsException:Database user4 already exists:56:20', 'org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$create_database_result$create_database_resultStandardScheme:read:ThriftHiveMetastore.java:26211', 'org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$create_database_result$create_database_resultStandardScheme:read:ThriftHiveMetastore.java:26197', 'org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$create_database_result:read:ThriftHiveMetastore.java:26131', 'org.apache.thrift.TServiceClient:receiveBase:TServiceClient.java:86', 'org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client:recv_create_database:ThriftHiveMetastore.java:741', 'org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client:create_database:ThriftHiveMetastore.java:728', 'org.apache.hadoop.hive.metastore.HiveMetaStoreClient:createDatabase:HiveMetaStoreClient.java:800', 'sun.reflect.NativeMethodAccessorImpl:invoke0:NativeMethodAccessorImpl.java:-2', 'sun.reflect.NativeMethodAccessorImpl:invoke:NativeMethodAccessorImpl.java:62', 'sun.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:498', 'org.apache.hadoop.hive.metastore.RetryingMetaStoreClient:invoke:RetryingMetaStoreClient.java:154', 'com.sun.proxy.$Proxy36:createDatabase::-1', 'sun.reflect.NativeMethodAccessorImpl:invoke0:NativeMethodAccessorImpl.java:-2', 'sun.reflect.NativeMethodAccessorImpl:invoke:NativeMethodAccessorImpl.java:62', 'sun.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:498', 'org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler:invoke:HiveMetaStoreClient.java:2562', 'com.sun.proxy.$Proxy36:createDatabase::-1', 'org.apache.hadoop.hive.ql.metadata.Hive:createDatabase:Hive.java:433', 'org.apache.hadoop.hive.ql.exec.DDLTask:createDatabase:DDLTask.java:4194'], sqlState='42000', errorCode=1, errorMessage='Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Database user4 already exists'), operationHandle=None)\n",
      "[SQL: CREATE DATABASE user4]\n",
      "(Background on this error at: http://sqlalche.me/e/e3q8)\n",
      "   hive://master.ykono.work:10000\n",
      " * hive://user4@master.ykono.work:10000\n",
      "Done.\n",
      "   hive://master.ykono.work:10000\n",
      " * hive://user4@master.ykono.work:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>tab_name</th>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql CREATE DATABASE user4\n",
    "%sql USE user4\n",
    "%sql SHOW TABLES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ライブラリファイルのコピー・登録\n",
    "\n",
    "Hiveクエリの中でjsonファイルを扱えるようにするためのライブラリを登録します。\n",
    "ライブラリファイルはGithubリポジトリに含まれています（ライブラリの詳細は`/lib/README.jar`を参照ください）。\n",
    "はじめにCDSWからHDFSにコピーし、HDFS上のファイルをHiveへ登録します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hdfs', 'dfs', '-put', './lib/json-1.3.7.3.jar', './']\n",
      "['hdfs', 'dfs', '-put', './lib/brickhouse-0.7.1-SNAPSHOT.jar', './']\n",
      "['hdfs', 'dfs', '-put', './lib/json-serde-cdh5-shim-1.3.7.3.jar', './']\n",
      "['hdfs', 'dfs', '-put', './lib/json-serde-1.3.7.3.jar', './']\n"
     ]
    }
   ],
   "source": [
    "#HDFS_PATH_DIR = '/tmp/'\n",
    "HDFS_PATH_DIR = './'\n",
    "\n",
    "args = ['hdfs', 'dfs', '-put', '', HDFS_PATH_DIR]\n",
    "\n",
    "file_list = glob.glob(\"./lib/*.jar\")\n",
    "\n",
    "for file in file_list:\n",
    "    try:\n",
    "        args[3] = file\n",
    "        print(args)\n",
    "\n",
    "        proc = subprocess.run(args,stdout = subprocess.PIPE, stderr = subprocess.PIPE)\n",
    "  \n",
    "    except:\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put: `brickhouse-0.7.1-SNAPSHOT.jar': File exists\n",
      "put: `json-1.3.7.3.jar': File exists\n",
      "put: `json-serde-1.3.7.3.jar': File exists\n",
      "put: `json-serde-cdh5-shim-1.3.7.3.jar': File exists\n",
      "Found 7 items\n",
      "drwx------   - user4 supergroup          0 2020-01-31 03:53 .Trash\n",
      "-rw-r--r--   3 user4 supergroup     308146 2020-01-31 03:56 brickhouse-0.7.1-SNAPSHOT.jar\n",
      "drwxr-xr-x   - user4 supergroup          0 2020-01-31 03:53 cdsw\n",
      "-rw-r--r--   3 user4 supergroup      44477 2020-01-31 03:56 json-1.3.7.3.jar\n",
      "-rw-r--r--   3 user4 supergroup      36653 2020-01-31 03:56 json-serde-1.3.7.3.jar\n",
      "-rw-r--r--   3 user4 supergroup       5110 2020-01-31 03:56 json-serde-cdh5-shim-1.3.7.3.jar\n",
      "drwxr-xr-x   - user4 supergroup          0 2020-01-31 02:49 twits\n"
     ]
    }
   ],
   "source": [
    "!export HADOOP_CONF_DIR=/etc/hadoop/conf; hdfs dfs -put `ls -1 ./lib/*.jar` .; hdfs dfs -ls ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hdfs', 'dfs', '-ls']\n",
      "['hdfs', 'dfs', '-put', './lib/json-serde-1.3.7.3.jar', './']\n"
     ]
    }
   ],
   "source": [
    "str_command = \"hdfs dfs -ls\"\n",
    "print(str_command.split())\n",
    "try:\n",
    "\n",
    "    proc = subprocess.run(str_command.split(),stdout = subprocess.PIPE, stderr = subprocess.PIPE)\n",
    "\n",
    "except:\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export HADOOP_CONF_DIR=/etc/hadoop/conf; hdfs dfs -put `ls -1 ./lib/*.jar` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hive://master.ykono.work:10000\n",
      " * hive://user4@master.ykono.work:10000\n",
      "Done.\n",
      "   hive://master.ykono.work:10000\n",
      " * hive://user4@master.ykono.work:10000\n",
      "Done.\n",
      "   hive://master.ykono.work:10000\n",
      " * hive://user4@master.ykono.work:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql add jar hdfs:/user/user4/json-1.3.7.3.jar\n",
    "%sql add jar hdfs:/user/user4/json-serde-1.3.7.3.jar\n",
    "%sql add jar hdfs:/user/user4/json-serde-cdh5-shim-1.3.7.3.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hive://master.ykono.work:10000\n",
      " * hive://user4@master.ykono.work:10000\n",
      "Done.\n",
      "   hive://master.ykono.work:10000\n",
      " * hive://user4@master.ykono.work:10000\n",
      "Done.\n",
      "   hive://master.ykono.work:10000\n",
      " * hive://user4@master.ykono.work:10000\n",
      "Done.\n",
      "   hive://master.ykono.work:10000\n",
      " * hive://user4@master.ykono.work:10000\n",
      "Done.\n",
      "   hive://master.ykono.work:10000\n",
      " * hive://user4@master.ykono.work:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql DROP TABLE IF EXISTS twits\n",
    "%sql DROP TABLE IF EXISTS message_extracted\n",
    "%sql DROP TABLE IF EXISTS message_filtered\n",
    "%sql DROP TABLE IF EXISTS message_exploded\n",
    "%sql DROP TABLE IF EXISTS sentiment_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SNSメッセージファイルを格納した場所を指定して、テーブルを作成します。\n",
    "\n",
    "**`LOCATION`指定にあなたがファイルをアップロードしたパスを指定してください**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hive://master.ykono.work:10000\n",
      " * hive://user4@master.ykono.work:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "CREATE EXTERNAL TABLE twits (\n",
    "\tmessages \n",
    "\tARRAY<\n",
    "\t    STRUCT<body: STRING,\n",
    "\t        symbols:ARRAY<STRUCT<symbol:STRING>>,\n",
    "\t        entities:STRUCT<sentiment:STRUCT<basic:STRING>>\n",
    "\t    >\n",
    "\t>\n",
    ")\n",
    "ROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe' \n",
    "STORED AS TEXTFILE\n",
    "LOCATION '/user/user4/twits'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hive://master.ykono.work:10000\n",
      " * hive://user4@master.ykono.work:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>messages</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>[{&quot;body&quot;:&quot;$AAPL heng is about to go negative get ready for tomorrow.&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bearish&quot;}}},{&quot;body&quot;:&quot;$AMRN $AAPL $STML $SCYX If you watch only one documentary, it should be this one:\\n\\nhttps://www.netflix.com/title/81026143\\n\\nGod bless these doctors and researchers.&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;AMRN&quot;},{&quot;symbol&quot;:&quot;STML&quot;},{&quot;symbol&quot;:&quot;SCYX&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$AMZN of course not just tesla.. $AAPL  &amp;amp; $MSFT basically did nothing on the cash open after ER&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;AMZN&quot;},{&quot;symbol&quot;:&quot;MSFT&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;Ripping $AAPL $MSFT $SPCE $SPY&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;MSFT&quot;},{&quot;symbol&quot;:&quot;SPY&quot;},{&quot;symbol&quot;:&quot;SPCE&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$AAPL $BABA virus bears at this point are gonna get steam rolled tomorrow....Asia way up&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;BABA&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$AAPL This should easily gap up to 325 tomorrow&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$SPY $TVIX $AAPL \\n\\nSPY current behavior has shown that -1% - 1.5% drop is just a consolidating respectable pullback. Investors that bought in at the beginning of 2019 only bought the dip of fear to compound over &amp;amp; over...Can easily be a 2-3 year rally with bears calling a crash at 410.&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;SPY&quot;},{&quot;symbol&quot;:&quot;TVIX&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$AAPL\\n\\n$330 tomorrow!&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$AAPL futures up&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$AAPL gap up incoming&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$AAPL Apple PT Raised to $375.00 \\n\\nhttps://newsfilter.io/a/c44f08aed41cfc4109cfd90ca575dc5e&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$AAPL Apple Upgraded to Hold by Maxim Group \\n\\nhttps://newsfilter.io/a/3cc9600276d07833855007b0b68aeba3&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$AAPL Hey kids- remember when the bitch was a big deal? 😂😂😂😂&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$AMZN literally amazon is the one us company doing things masterfully. They employs tons of US workers unlike $AAPL and contribute greatly wherever they go. Tons of\\nBezos haters look like dumbasses when they post today. He is self made entrepreneur i remember in 97 people put them down then for selling books online $SPY . Permabears never learn&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;AMZN&quot;},{&quot;symbol&quot;:&quot;SPY&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$AMZN Asia market recovering expect big day tomorrow $AAPL $MSFT $TSLA  ✌️&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;AMZN&quot;},{&quot;symbol&quot;:&quot;MSFT&quot;},{&quot;symbol&quot;:&quot;TSLA&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$SPY ALL TIME HIGHS BABY!!! BEEN CALLING THE DIP SINCE MONDAY!!! LETS GO BULLS!!! SUPER BOWL WEEK!!! MONEY REPO TEAM BACK ONCE AGAIN SIPPIN HENN MIXED WITH JUICE AND GIN! JOIN THEM!!! $$$$$ $BA $AAPL $AMZN $TSLA&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;AMZN&quot;},{&quot;symbol&quot;:&quot;BA&quot;},{&quot;symbol&quot;:&quot;SPY&quot;},{&quot;symbol&quot;:&quot;TSLA&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;Peak profit for the last 6 expired option alerts for $AAPL -18.08  | 529.17  | 3.02  | 354.20  | 402.11  | 294.90  |&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$YM_F if you want to win come trade with me if you want to lose go somewhere else $AAPL $SPY&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;YM_F&quot;},{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;SPY&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$AAPL $TSLA $AMZN $MSFT are earnings winners. Investors let your winners run. Pizza anyone?&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;AMZN&quot;},{&quot;symbol&quot;:&quot;MSFT&quot;},{&quot;symbol&quot;:&quot;TSLA&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$AAPL just leaving this here as future turn green https://www.cnbc.com/2020/01/31/china-economy-beijing-announces-official-manufacturing-pmi-for-january.html&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$AAPL $BABA so futures are up, and Asia, so maybe those numbers aren’t that bad like it said.....&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;BABA&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$NFLX $SPY $AAPL $FB \\n\\n After several successful trades in Netflix both ways it’s shown more obvious bullish strength then bearish. I have been trading Netflix swings above 340 primarily taking the outlook 80% Bull 20% bear which is a healthy practice I do personally let me not get ahead myself to be a perma-bear. Currently Netflix is 4% away from it’s all time high and has struggled several times to breakout above it, but as long as Netflix is trading well above 340 and vegans to build a base above 344 within the next couple of weeks Netflix should be kissing 362.12 - 373.40.&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;NFLX&quot;},{&quot;symbol&quot;:&quot;SPY&quot;},{&quot;symbol&quot;:&quot;FB&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$AA Alcoa’s fundamentals horrible but it may have a bounce as I explained earlier. Moreover Alcoa Day Candle Daily Chart is a Dragonfly Doji     👍\\nwhich is bullish \\nwhen it occurs in a downtrend \\nDaily technicals of RSI being extremely oversold for days and it is END OF MONTH\\n $AAPL $AMZN $WDC $SPY&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;AMZN&quot;},{&quot;symbol&quot;:&quot;AA&quot;},{&quot;symbol&quot;:&quot;SPY&quot;},{&quot;symbol&quot;:&quot;WDC&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$SPY we ripping again, so let’s make sure we stick to chicken 🐔 if we gonna be making any soup and leave them 🦇 alone, #corona $AAPL $AMZN $TSLA&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;AMZN&quot;},{&quot;symbol&quot;:&quot;SPY&quot;},{&quot;symbol&quot;:&quot;TSLA&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$AAPL Watching $317 support.  Daily chart.&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;AA Alcoa’s fundamentals horrible but it may have a bounce as I explained earlier. Moreover Alcoa Day Candle Daily Chart is a Dragonfly Doji     👍\\nwhich bullish \\nwhen it occurs in a downtrend \\nDaily technicals of RSI being extremely oversold for days and it is END OF MONTH\\n $AAPL $AMZN $WDC $SPY&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;AMZN&quot;},{&quot;symbol&quot;:&quot;SPY&quot;},{&quot;symbol&quot;:&quot;WDC&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$AAPL JPMorgan Chase &amp;amp; Boosts Apple Price Target to $350.00 \\n\\nhttps://newsfilter.io/a/9fdc74006956fbe2cb5c99471f5bf803&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$AAPL coronavirus goes to India and Philippines google closed all offices Tesla closed https://m.youtube.com/watch?v=6DBFwIlT4fg&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$AMZN $SPY $AAPL $MCD&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;AMZN&quot;},{&quot;symbol&quot;:&quot;MCD&quot;},{&quot;symbol&quot;:&quot;SPY&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$AAPL never short a market when you have  trump has president this guy won’t let the market go down&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;AAPL&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>[{&quot;body&quot;:&quot;$ABEO Stochastic is turning. Tomorrow will likely bring the stochastic buy signal, as the slow crosses above the fast, and that Olivergarden dimwit can go pound sand&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABEO&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$ABEO 2.30s ouch... can I get 2.10s?&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABEO&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bearish&quot;}}},{&quot;body&quot;:&quot;$ABEO added bigly here...insider owns $500k for a reason here&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABEO&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$ABEO $CLSN two biggest positions and two biggest drawdowns right now&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;CLSN&quot;},{&quot;symbol&quot;:&quot;ABEO&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$ABEO if it doesn’t hold here, it’s going to get really cheap.&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABEO&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ABEO Their CSO stepped down at the beginning of the year. 🤦Big news into cell-therapy, reported earnings date of 3-21-20.👍 Look to get in around $2.20 or lower. 💵🤞&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABEO&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$ABEO Daily short volume at a whopping 75%, same as yesterday. That can&amp;#39;t be sustainable. MMs have two days to make good on all the non-existent shares they sold today, and tomorrow is the settlement date for the assloads of non-existent shares they sold yesterday. They can&amp;#39;t settle counterfeit shares using more non-existent shares. That doesn&amp;#39;t work. Let&amp;#39;s see what the short-bag-holding dimwits do tomorrow. The clock is ticking. I&amp;#39;d like to see the fraudsters stick around and get torched&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABEO&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$ABEO ⏫✔ Wait on the right price point to get in.  Looking around $2.20 🤘🐂&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABEO&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$ABEO will go to 2 soon&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABEO&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bearish&quot;}}},{&quot;body&quot;:&quot;I wish I shorted $ABEO a year ago; the -61.46% change sure looks sweet now https://wallmine.com/nasdaq/abeo?utm_source=stocktwits&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABEO&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ABEO vol just not there , I’m out GL all&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABEO&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ABEO here we go, bids stacking at 2.49, 2.5 mini wall down&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABEO&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$ABEO long hold 👍🏻&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABEO&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$ABEO need to knock down the 2.5 mini wall and we will move&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABEO&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$ABEO ↗ Charts say buy✔ I&amp;#39;m holding off for another drop before conformation on positive price movement. 🤘&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABEO&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$ABEO back to 3 please. any news in the near future?!&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABEO&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$ABEO Daily short volume was 75%, and those parasites still lost ground&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABEO&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$ABEO another head and shoulders setup on the intraday pattern. @Reformed_Trader&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABEO&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$ABEO what a tin stock I tried to add 15,000 share and pop like 5 cent -- come on&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABEO&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ABEO really, no cheerleading or pumping? for shame&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABEO&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$ABEO looking good&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABEO&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$ABEO pretty clear there’s good support here, if your short why wouldn’t you lock in profits and ride it back to $3 at this point ?&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABEO&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$ABEO some volume coming now&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABEO&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$ABEO lol these two posts juxtaposed 😂  @jsp4423&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABEO&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$ABEO add again.&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABEO&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$ABEO i am betting a dilution very soon...&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABEO&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ABEO insiders added recently. Dilution Done&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABEO&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$ABEO adding more, Wellington adding as well as other institutions is all I need to see, close to 4K shares now&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABEO&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;Wellington Management Group LLP has filed an amended 13G/A, reporting 8.04% ownership in $ABEO - https://fintel.io/so/us/abeo?utm_source=stocktwits.com&amp;amp;utm_medium=social&amp;amp;utm_campaign=owner&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABEO&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ABEO bot more down here, Wellington increase&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABEO&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>[{&quot;body&quot;:&quot;Departure of Directors or Certain  http://www.conferencecalltranscripts.org/8/summary2/?id=7351264 $ABG&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABG&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;Thinking about investing in $ABG #AsburyAutomotive? The 8-K filing touching on departure of directors or certain officers among other topics might be what you&amp;#39;re looking for https://wallmine.com/filing/redirect/12084309?utm_source=stocktwits&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABG&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ABG just filed a Event for Officers https://last10k.com/sec-filings/abg/0001144980-20-000008.htm?utm_source=stocktwits&amp;amp;utm_medium=forum&amp;amp;utm_campaign=8K&amp;amp;utm_term=abg&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABG&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ABG / Asbury Automotive Group files form 8-K - Departure of Directors or Certain Officers; Election of Directors; Appointment of Certain Officers; Compensatory Arrangements of Certain Officers https://fintel.io/s/us/abg?utm_source=stocktwits.com&amp;amp;utm_medium=Social&amp;amp;utm_campaign=filing&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABG&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ABG filed form 8-K on January 30, 17:31:01: Item5.02: Departure of Election 0f Officers or Compensatory Arrangements https://s.flashalert.me/oUEIw&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABG&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ABG Form 8-K: Departure of Directors or Certain Officers; Election of Directors; Appointment of Certain Officers; Compensatory Arrangements of Certain Officers.Asbury Automotive Group’s has announced.. \\n\\nhttps://newsfilter.io/a/db75603965c2c7ba2de12f8e4613ad92&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABG&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;Victory Capital Management Inc has filed an amended 13G/A, reporting 3.17% ownership in $ABG - https://fintel.io/so/us/abg?utm_source=stocktwits.com&amp;amp;utm_medium=social&amp;amp;utm_campaign=owner&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABG&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ABG filed form SC 13G/A on January 30, 10:08:33 https://s.flashalert.me/ctDqz5&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABG&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;Victory Capital Management Inc. just provided an update on share ownership of Asbury Automotive http://www.conferencecalltranscripts.org/13G/summary/?id=7348696 $ABG&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABG&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ABG Form SC 13G/A (statement of acquisition of beneficial ownership by individuals) filed with the SEC \\n\\nhttps://newsfilter.io/a/5596c4ca5f31b12c5d5994aa21748dfe&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABG&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;BULLISH NEWS FOR $ABG\\n\\nhttps://www.nasdaq.com/articles/asbury-automotive-group-abg-earnings-expected-to-grow%3A-what-to-know-ahead-of-next-weeks&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABG&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$ABG $2.34 Earnings Per Share Expected for Asbury Automotive Group This Quarter \\n\\nhttps://newsfilter.io/a/6362dd8c82a409e658cd91c1f1f571a7&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABG&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ABG Asbury Automotive Group Price Target Cut to $104.00 \\n\\nhttps://newsfilter.io/a/9539ffb509fcedb246b9bf036f5b143c&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABG&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;Asbury Automotive Group&amp;#39;s PT cut by Morgan Stanley to $104.00. equal weight rating. https://www.marketbeat.com/r/1333835 $ABG&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABG&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ABG Morgan Stanley Maintains to Equal-Weight : PT $104.00 https://stockhoot.com/ExtSymbol.aspx?from=AnalystRatingTweet&amp;amp;symbol=ABG&amp;amp;t=593&amp;amp;Social=StockTwits&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABG&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;Asbury (ABG) to release earnings before the market closes on Monday, February 3. Expected EPS: 2.34. $ABG https://www.tipranks.com/stocks/ABG/earnings-calendar?ref=TREarnings&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABG&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ABG Asbury Automotive Group Schedules Release of Fourth Quarter and Full Year 2019 Financial Results \\n\\nhttps://newsfilter.io/a/f62d14cb5a8c54a2a4c53a2891ff572f&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABG&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;Undervalued Signal Alert: $ABG. More insights: https://stockinvest.us/technical-analysis/ABG?utm_source=stocktwits&amp;amp;utm_medium=autopost&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABG&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;How does this make you feel? $ABG RSI Indicator left the oversold zone. View odds of uptrend. https://tickeron.com/go/1133340&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABG&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;AsburyAutomotiveGroup $ABG BidaskScore is Downgraded to Held https://bidaskclub.com/news/company-news/company-news-company-news/2020/01/asbury-automotive-group-abg-bidaskscore-is-downgraded-to-held/&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABG&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$GPI - Hold Group 1 and Asbury Automotive ($ABG) in the same… http://dlvr.it/RMsv0z #portfolio_prospective #better_portfolio #diversify&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABG&quot;},{&quot;symbol&quot;:&quot;GPI&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;Departure of Directors or Certain  http://www.conferencecalltranscripts.org/8/summary/?id=7272462 $ABG&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABG&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ABG / Asbury Automotive Group files form 8-K - Departure of Directors or Certain Officers; Election of Directors; Appointment of Certain Officers; Compensatory Arrangements of Certain Officers https://fintel.io/s/us/abg?utm_source=stocktwits.com&amp;amp;utm_medium=Social&amp;amp;utm_campaign=filing&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABG&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ABG just filed a Event for Officers https://last10k.com/sec-filings/abg/0001144980-20-000005.htm?utm_source=stocktwits&amp;amp;utm_medium=forum&amp;amp;utm_campaign=8K&amp;amp;utm_term=abg&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABG&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;Fresh - #AsburyAutomotive released a current report talking about election of directors and other topics. See what others think $ABG https://wallmine.com/filing/redirect/12031426?utm_source=stocktwits&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABG&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ABG filed form 8-K on January 09, 17:29:18: Item5.02: Departure of Election 0f Officers or Compensatory Arrangements https://s.flashalert.me/6seZQ&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABG&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ABG  Form 8-K: Departure of Directors or Certain Officers; Election of Directors; Appointment of Certain Officers; Compensatory Arrangements of Certain Officers.As previously reported by Asbury Automo.. https://newsfilter.io/a/38a217f41dadf750c697717dc4a83b50&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABG&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;$ABG  Should Value Investors Pick Asbury Automotive Stock?: Value investing is easily one of the most popular ways to find great stocks in any market environment. After all, who wouldn’t want to find s.. https://newsfilter.io/a/2523652df6f32201b94582f805d556f8&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABG&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}},{&quot;body&quot;:&quot;BULLISH NEWS FOR $ABG\\n\\nhttps://simplywall.st/stocks/us/retail/nyse-abg/asbury-automotive-group/news/heres-why-i-think-asbury-automotive-group-nyseabg-is-an-interesting-stock/&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;ABG&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:{&quot;basic&quot;:&quot;Bullish&quot;}}},{&quot;body&quot;:&quot;$LAD $ABG $GOLF $MRC $CMC  5 Growth Stocks to Buy as Middle-East Tensions Subside: When the markets were expecting Middle-East tensions to intensify following Tehran’s retaliatory attack, President Don.. https://newsfilter.io/a/bd29d2fae3108f0c0ea80b2e20467ba7&quot;,&quot;symbols&quot;:[{&quot;symbol&quot;:&quot;GOLF&quot;},{&quot;symbol&quot;:&quot;ABG&quot;},{&quot;symbol&quot;:&quot;CMC&quot;},{&quot;symbol&quot;:&quot;LAD&quot;},{&quot;symbol&quot;:&quot;MRC&quot;}],&quot;entities&quot;:{&quot;sentiment&quot;:null}}]</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[('[{\"body\":\"$AAPL heng is about to go negative get ready for tomorrow.\",\"symbols\":[{\"symbol\":\"AAPL\"}],\"entities\":{\"sentiment\":{\"basic\":\"Bearish\"}}},{\"b ... (6923 characters truncated) ... hen you have  trump has president this guy won’t let the market go down\",\"symbols\":[{\"symbol\":\"AAPL\"}],\"entities\":{\"sentiment\":{\"basic\":\"Bullish\"}}}]',),\n",
       " ('[{\"body\":\"$ABEO Stochastic is turning. Tomorrow will likely bring the stochastic buy signal, as the slow crosses above the fast, and that Olivergarde ... (4839 characters truncated) ... entiment\":null}},{\"body\":\"$ABEO bot more down here, Wellington increase\",\"symbols\":[{\"symbol\":\"ABEO\"}],\"entities\":{\"sentiment\":{\"basic\":\"Bullish\"}}}]',),\n",
       " ('[{\"body\":\"Departure of Directors or Certain  http://www.conferencecalltranscripts.org/8/summary2/?id=7351264 $ABG\",\"symbols\":[{\"symbol\":\"ABG\"}],\"enti ... (7325 characters truncated) ... 0c0ea80b2e20467ba7\",\"symbols\":[{\"symbol\":\"GOLF\"},{\"symbol\":\"ABG\"},{\"symbol\":\"CMC\"},{\"symbol\":\"LAD\"},{\"symbol\":\"MRC\"}],\"entities\":{\"sentiment\":null}}]',)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "select * from twits limit 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データ変換のためのテーブルを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hive://master.ykono.work:10000\n",
      " * hive://user4@master.ykono.work:10000\n",
      "Done.\n",
      "   hive://master.ykono.work:10000\n",
      " * hive://user4@master.ykono.work:10000\n",
      "Done.\n",
      "   hive://master.ykono.work:10000\n",
      " * hive://user4@master.ykono.work:10000\n",
      "Done.\n",
      "   hive://master.ykono.work:10000\n",
      " * hive://user4@master.ykono.work:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql create table message_extracted (symbols array<struct<symbol:string>>, sentiment STRING, body STRING) STORED AS TEXTFILE\n",
    "%sql create table message_filtered (symbols array<struct<symbol:string>>, sentiment STRING, body STRING) STORED AS TEXTFILE\n",
    "%sql create table message_exploded (symbol string, sentiment STRING, body STRING) STORED AS TEXTFILE\n",
    "%sql create table sentiment_data (sentiment int, body STRING) STORED AS TEXTFILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "元のデータから必要なデータのみを抽出します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hive://master.ykono.work:10000\n",
      " * hive://user4@master.ykono.work:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "insert overwrite table message_extracted \n",
    "select message.symbols, message.entities.sentiment, message.body from twits \n",
    "lateral view explode(messages) messages as message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hive://master.ykono.work:10000\n",
      " * hive://user4@master.ykono.work:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>symbols</th>\n",
       "        <th>sentiment</th>\n",
       "        <th>body</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>[{&quot;symbol&quot;:&quot;AAPL&quot;}]</td>\n",
       "        <td>Bearish</td>\n",
       "        <td>$AAPL heng is about to go negative get ready for tomorrow.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;AMRN&quot;},{&quot;symbol&quot;:&quot;STML&quot;},{&quot;symbol&quot;:&quot;SCYX&quot;}]</td>\n",
       "        <td>Bullish</td>\n",
       "        <td>$AMRN $AAPL $STML $SCYX If you watch only one documentary, it should be this one:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>[]</td>\n",
       "        <td>None</td>\n",
       "        <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>[{&quot;symbol&quot;:&quot;https://www.netflix.com/title/81026143&quot;}]</td>\n",
       "        <td>None</td>\n",
       "        <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>[]</td>\n",
       "        <td>None</td>\n",
       "        <td>None</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[('[{\"symbol\":\"AAPL\"}]', 'Bearish', '$AAPL heng is about to go negative get ready for tomorrow.'),\n",
       " ('[{\"symbol\":\"AAPL\"},{\"symbol\":\"AMRN\"},{\"symbol\":\"STML\"},{\"symbol\":\"SCYX\"}]', 'Bullish', '$AMRN $AAPL $STML $SCYX If you watch only one documentary, it should be this one:'),\n",
       " ('[]', None, None),\n",
       " ('[{\"symbol\":\"https://www.netflix.com/title/81026143\"}]', None, None),\n",
       " ('[]', None, None)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "select * from message_extracted limit 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データから、メッセージ・ボディが含まれていないデータ以外のデータを取り出します。同時に、銘柄に対するセンチメントを文字列からを数値に置換します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hive://master.ykono.work:10000\n",
      " * hive://user4@master.ykono.work:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "insert overwrite table message_filtered \n",
    "select symbols, \n",
    "    case sentiment when 'Bearish' then -2 when 'Bullish' then 2 ELSE 0 END as sentiment, \n",
    "    body from message_extracted \n",
    "    where body is not null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hive://master.ykono.work:10000\n",
      " * hive://user4@master.ykono.work:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>symbols</th>\n",
       "        <th>sentiment</th>\n",
       "        <th>body</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>[{&quot;symbol&quot;:&quot;AAPL&quot;}]</td>\n",
       "        <td>-2</td>\n",
       "        <td>$AAPL heng is about to go negative get ready for tomorrow.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;AMRN&quot;},{&quot;symbol&quot;:&quot;STML&quot;},{&quot;symbol&quot;:&quot;SCYX&quot;}]</td>\n",
       "        <td>2</td>\n",
       "        <td>$AMRN $AAPL $STML $SCYX If you watch only one documentary, it should be this one:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>[{&quot;symbol&quot;:&quot;AAPL&quot;},{&quot;symbol&quot;:&quot;AMZN&quot;},{&quot;symbol&quot;:&quot;MSFT&quot;}]</td>\n",
       "        <td>0</td>\n",
       "        <td>$AMZN of course not just tesla.. $AAPL  &amp;amp; $MSFT basically did nothing on the cash open after ER</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[('[{\"symbol\":\"AAPL\"}]', '-2', '$AAPL heng is about to go negative get ready for tomorrow.'),\n",
       " ('[{\"symbol\":\"AAPL\"},{\"symbol\":\"AMRN\"},{\"symbol\":\"STML\"},{\"symbol\":\"SCYX\"}]', '2', '$AMRN $AAPL $STML $SCYX If you watch only one documentary, it should be this one:'),\n",
       " ('[{\"symbol\":\"AAPL\"},{\"symbol\":\"AMZN\"},{\"symbol\":\"MSFT\"}]', '0', '$AMZN of course not just tesla.. $AAPL  &amp; $MSFT basically did nothing on the cash open after ER')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "select * from message_filtered limit 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一つのメッセージに複数の銘柄が紐づけられています。データ正規化のため、データ１行につき、一つの銘柄を持つようにデータを変換します（同じメッセージを持つ行が複数作られます）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hive://master.ykono.work:10000\n",
      " * hive://user4@master.ykono.work:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "insert overwrite table message_exploded \n",
    "select symbol.symbol, sentiment, body from message_filtered lateral view explode(symbols) symbols as symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hive://master.ykono.work:10000\n",
      " * hive://user4@master.ykono.work:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>symbol</th>\n",
       "        <th>sentiment</th>\n",
       "        <th>body</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>AAPL</td>\n",
       "        <td>-2</td>\n",
       "        <td>$AAPL heng is about to go negative get ready for tomorrow.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>AAPL</td>\n",
       "        <td>2</td>\n",
       "        <td>$AMRN $AAPL $STML $SCYX If you watch only one documentary, it should be this one:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>AMRN</td>\n",
       "        <td>2</td>\n",
       "        <td>$AMRN $AAPL $STML $SCYX If you watch only one documentary, it should be this one:</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[('AAPL', '-2', '$AAPL heng is about to go negative get ready for tomorrow.'),\n",
       " ('AAPL', '2', '$AMRN $AAPL $STML $SCYX If you watch only one documentary, it should be this one:'),\n",
       " ('AMRN', '2', '$AMRN $AAPL $STML $SCYX If you watch only one documentary, it should be this one:')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "select * from message_exploded limit 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここまでの操作で、元の複雑な構造のデータから、１レコードにつき、銘柄、センチメント、メッセージ本文を持つフォーマットに変換されました。\n",
    "銘柄毎のセンチメントの件数などの分析を行うには、このテーブルを利用します。\n",
    "\n",
    "この後の感情分析では、メッセージ本文の文字列から、センチメントを判定する予測モデルを構築します。そのため銘柄情報は利用しないため、センチメントとメッセージ本文のみを取り出します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hive://master.ykono.work:10000\n",
      " * hive://user4@master.ykono.work:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "insert overwrite table sentiment_data \n",
    "select sentiment, body from message_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hive://master.ykono.work:10000\n",
      " * hive://user4@master.ykono.work:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>sentiment</th>\n",
       "        <th>body</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>-2</td>\n",
       "        <td>$AAPL heng is about to go negative get ready for tomorrow.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2</td>\n",
       "        <td>$AMRN $AAPL $STML $SCYX If you watch only one documentary, it should be this one:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>0</td>\n",
       "        <td>$AMZN of course not just tesla.. $AAPL  &amp;amp; $MSFT basically did nothing on the cash open after ER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2</td>\n",
       "        <td>Ripping $AAPL $MSFT $SPCE $SPY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>0</td>\n",
       "        <td>$AAPL $BABA virus bears at this point are gonna get steam rolled tomorrow....Asia way up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2</td>\n",
       "        <td>$AAPL This should easily gap up to 325 tomorrow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2</td>\n",
       "        <td>$SPY $TVIX $AAPL </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2</td>\n",
       "        <td>$AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2</td>\n",
       "        <td>$AAPL futures up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>0</td>\n",
       "        <td>$AAPL gap up incoming</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(-2, '$AAPL heng is about to go negative get ready for tomorrow.'),\n",
       " (2, '$AMRN $AAPL $STML $SCYX If you watch only one documentary, it should be this one:'),\n",
       " (0, '$AMZN of course not just tesla.. $AAPL  &amp; $MSFT basically did nothing on the cash open after ER'),\n",
       " (2, 'Ripping $AAPL $MSFT $SPCE $SPY'),\n",
       " (0, '$AAPL $BABA virus bears at this point are gonna get steam rolled tomorrow....Asia way up'),\n",
       " (2, '$AAPL This should easily gap up to 325 tomorrow'),\n",
       " (2, '$SPY $TVIX $AAPL '),\n",
       " (2, '$AAPL'),\n",
       " (2, '$AAPL futures up'),\n",
       " (0, '$AAPL gap up incoming')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "select * from sentiment_data limit 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSONファイルの作成\n",
    "\n",
    "加工したデータをJSONファイルとして出力します。\n",
    "\n",
    "感情分析を担当するデータサイエンティスト・機械学習エンジニアは、このJSONファイルを使います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add jar hdfs:/tmp/brickhouse-0.7.1-SNAPSHOT.jar;\n",
    "CREATE TEMPORARY FUNCTION to_json AS 'brickhouse.udf.json.ToJsonUDF';\n",
    "\n",
    "create table json_message (message STRING) STORED AS TEXTFILE;\n",
    "\n",
    "insert overwrite table json_message\n",
    "select to_json(named_struct('message_body', body, 'sentiment', sentiment)) from sentiment_data;\n",
    "\n",
    "select * from json_message;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hive://master.ykono.work:10000\n",
      " * hive://user4@master.ykono.work:10000\n",
      "Done.\n",
      "   hive://master.ykono.work:10000\n",
      " * hive://user4@master.ykono.work:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql add jar hdfs:/tmp/brickhouse-0.7.1-SNAPSHOT.jar\n",
    "%sql CREATE TEMPORARY FUNCTION to_json AS 'brickhouse.udf.json.ToJsonUDF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hive://master.ykono.work:10000\n",
      " * hive://user4@master.ykono.work:10000\n",
      "Done.\n",
      "   hive://master.ykono.work:10000\n",
      " * hive://user4@master.ykono.work:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql DROP TABLE IF EXISTS json_message\n",
    "%sql create table json_message (message STRING) STORED AS TEXTFILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hive://master.ykono.work:10000\n",
      " * hive://user4@master.ykono.work:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "insert overwrite table json_message\n",
    "select to_json(named_struct('message_body', body, 'sentiment', sentiment)) from sentiment_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hive://master.ykono.work:10000\n",
      " * hive://user4@master.ykono.work:10000\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>message</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>{&quot;message_body&quot;:&quot;$AAPL heng is about to go negative get ready for tomorrow.&quot;,&quot;sentiment&quot;:-2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>{&quot;message_body&quot;:&quot;$AMRN $AAPL $STML $SCYX If you watch only one documentary, it should be this one:&quot;,&quot;sentiment&quot;:2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>{&quot;message_body&quot;:&quot;$AMZN of course not just tesla.. $AAPL  &amp;amp; $MSFT basically did nothing on the cash open after ER&quot;,&quot;sentiment&quot;:0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>{&quot;message_body&quot;:&quot;Ripping $AAPL $MSFT $SPCE $SPY&quot;,&quot;sentiment&quot;:2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>{&quot;message_body&quot;:&quot;$AAPL $BABA virus bears at this point are gonna get steam rolled tomorrow....Asia way up&quot;,&quot;sentiment&quot;:0}</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[('{\"message_body\":\"$AAPL heng is about to go negative get ready for tomorrow.\",\"sentiment\":-2}',),\n",
       " ('{\"message_body\":\"$AMRN $AAPL $STML $SCYX If you watch only one documentary, it should be this one:\",\"sentiment\":2}',),\n",
       " ('{\"message_body\":\"$AMZN of course not just tesla.. $AAPL  &amp; $MSFT basically did nothing on the cash open after ER\",\"sentiment\":0}',),\n",
       " ('{\"message_body\":\"Ripping $AAPL $MSFT $SPCE $SPY\",\"sentiment\":2}',),\n",
       " ('{\"message_body\":\"$AAPL $BABA virus bears at this point are gonna get steam rolled tomorrow....Asia way up\",\"sentiment\":0}',)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "select * from json_message limit 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`HQL_SELECT_MESSAGE`をあなたが作成したデータベースを指定してください**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from __future__ import print_function\n",
    "\n",
    "HQL_SELECT_MESSAGE = \"select * from user4.json_message\"\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"JsonGen\")\\\n",
    "    .getOrCreate()\n",
    "    \n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "json_list = spark.sql(HQL_SELECT_MESSAGE)\n",
    "\n",
    "path = \"./output.json\"\n",
    "\n",
    "with open(path, mode='w') as f:\n",
    "    f.write('{\"data\":[')\n",
    "    bool_first_line = True\n",
    "    for row in json_list.rdd.collect():\n",
    "        if bool_first_line:\n",
    "            bool_first_line = False\n",
    "            f.write(row.message)\n",
    "        else:\n",
    "            #print(row.message)\n",
    "            #f.write(row.message.encode(\"utf-8\"))\n",
    "            for i in range(100): # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "                f.write(\",\\n\")\n",
    "                f.write(row.message)\n",
    "    \n",
    "    f.write(\"]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 感情分析\n",
    "\n",
    "投資判断のために、企業の価値を考慮する際のアプローチとして、従来の枠組みにとらわれない様々な情報（オルタナティブ・データ）を用いることを考えます。\n",
    "\n",
    "投資家の判断を左右し得る様々な情報を入力とし、投資判断のための定量的なシグナルに変換する予測モデルを構築します。\n",
    "入力となるデータには様々なものがあります。以下はその例です。\n",
    "\n",
    "- ニュース（製品のリコール、自然災害など）\n",
    "\n",
    "ニューラルネットワークを使ったDeep Learningによって、入力データの形式を問わず、予測モデルを構築することができます。\n",
    "\n",
    "ここでは、ソーシャルメディアサイトStockTwitsの投稿を使用します。\n",
    "StockTwitsのコミュニティは、投資家、トレーダー、起業家により利用されています。\n",
    "\n",
    "感情のスコアを生成するこれらのtwitを中心にモデルを構築します。\n",
    "\n",
    "モデルの訓練のためには、入力に対応するラベルが必要になります。ラベルの精度は、モデルの訓練に当たって大変重要な要素です。\n",
    "\n",
    "センチメントの度合いを把握するために、非常にネガティブ、ネガティブ、ニュートラル、ポジティブ、非常にポジティブという5段階のスケールを使用します。それぞれ、-2から2までの数値に対応しています。\n",
    "\n",
    "このラベル付きデータによって訓練されたモデルを使用して、自然言語を入力として、その文章の背後にある感情を予測するモデルを構築します。\n",
    "\n",
    "\n",
    "### データの確認\n",
    "データがどのように見えるかを確認します。\n",
    "\n",
    "各フィールドの意味:\n",
    "\n",
    "* `'message_body'`: メッセージ本文テキスト\n",
    "* `'sentiment'`: センチメントスコア。-2から2までの５段階。0は中立。\n",
    "\n",
    "下記のような内容になっているはずです。\n",
    "```\n",
    "{'data':\n",
    "  {'message_body': '............................',\n",
    "   'sentiment': 2},\n",
    "  {'message_body': '............................',\n",
    "   'sentiment': -2},\n",
    "   ...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"data\":[{\"message_body\":\"$AAPL heng is about to go negative get ready for tomorrow.\",\"sentiment\":-2},\n",
      "{\"message_body\":\"$AMRN $AAPL $STML $SCYX If you watch only one documentary, it should be this one:\",\"sentiment\":2},\n",
      "{\"message_body\":\"$AMRN $AAPL $STML $SCYX If you watch only one documentary, it should be this one:\",\"sentiment\":2},\n",
      "{\"message_body\":\"$AMRN $AAPL $STML $SCYX If you watch only one documentary, it should be this one:\",\"sentiment\":2},\n",
      "{\"message_body\":\"$AMRN $AAPL $STML $SCYX If you watch only one documentary, it should be this one:\",\"sentiment\":2},\n",
      "{\"message_body\":\"$AMRN $AAPL $STML $SCYX If you watch only one documentary, it should be this one:\",\"sentiment\":2},\n",
      "{\"message_body\":\"$AMRN $AAPL $STML $SCYX If you watch only one documentary, it should be this one:\",\"sentiment\":2},\n",
      "{\"message_body\":\"$AMRN $AAPL $STML $SCYX If you watch only one documentary, it should be this one:\",\"sentiment\":2},\n",
      "{\"message_body\":\"$AMRN $AAPL $STML $SCYX If you watch only one documentary, it should be this one:\",\"sentiment\":2},\n",
      "{\"message_body\":\"$AMRN $AAPL $STML $SCYX If you watch only one documentary, it should be this one:\",\"sentiment\":2},\n",
      "{\"message_body\":\"$CEI $SAEX on fire in PM could fuel shares of $BIMI and $YUMA\",\"sentiment\":2},\n",
      "{\"message_body\":\"$CEI $SAEX on fire in PM could fuel shares of $BIMI and $YUMA\",\"sentiment\":2},\n",
      "{\"message_body\":\"$CEI $SAEX on fire in PM could fuel shares of $BIMI and $YUMA\",\"sentiment\":2},\n",
      "{\"message_body\":\"$CEI $SAEX on fire in PM could fuel shares of $BIMI and $YUMA\",\"sentiment\":2},\n",
      "{\"message_body\":\"$CEI $SAEX on fire in PM could fuel shares of $BIMI and $YUMA\",\"sentiment\":2},\n",
      "{\"message_body\":\"$CEI $SAEX on fire in PM could fuel shares of $BIMI and $YUMA\",\"sentiment\":2},\n",
      "{\"message_body\":\"$CEI $SAEX on fire in PM could fuel shares of $BIMI and $YUMA\",\"sentiment\":2},\n",
      "{\"message_body\":\"$CEI $SAEX on fire in PM could fuel shares of $BIMI and $YUMA\",\"sentiment\":2},\n",
      "{\"message_body\":\"$CEI $SAEX on fire in PM could fuel shares of $BIMI and $YUMA\",\"sentiment\":2},\n",
      "{\"message_body\":\"$CEI $SAEX on fire in PM could fuel shares of $BIMI and $YUMA\",\"sentiment\":2}]}"
     ]
    }
   ],
   "source": [
    "!head output.json\n",
    "!tail output.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'message_body': '$AAPL heng is about to go negative get ready for tomorrow.', 'sentiment': -2}, {'message_body': '$AMRN $AAPL $STML $SCYX If you watch only one documentary, it should be this one:', 'sentiment': 2}, {'message_body': '$AMRN $AAPL $STML $SCYX If you watch only one documentary, it should be this one:', 'sentiment': 2}, {'message_body': '$AMRN $AAPL $STML $SCYX If you watch only one documentary, it should be this one:', 'sentiment': 2}, {'message_body': '$AMRN $AAPL $STML $SCYX If you watch only one documentary, it should be this one:', 'sentiment': 2}, {'message_body': '$AMRN $AAPL $STML $SCYX If you watch only one documentary, it should be this one:', 'sentiment': 2}, {'message_body': '$AMRN $AAPL $STML $SCYX If you watch only one documentary, it should be this one:', 'sentiment': 2}, {'message_body': '$AMRN $AAPL $STML $SCYX If you watch only one documentary, it should be this one:', 'sentiment': 2}, {'message_body': '$AMRN $AAPL $STML $SCYX If you watch only one documentary, it should be this one:', 'sentiment': 2}, {'message_body': '$AMRN $AAPL $STML $SCYX If you watch only one documentary, it should be this one:', 'sentiment': 2}]\n"
     ]
    }
   ],
   "source": [
    "with open('./output.json', 'r') as f:\n",
    "    twits = json.load(f)\n",
    "\n",
    "print(twits['data'][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データ件数の確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "590901\n"
     ]
    }
   ],
   "source": [
    "print(len(twits['data']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データの前処理\n",
    "データを入手したら、テキストを前処理する必要があります。これらのtwitは、twit自体でリーダー$シンボルで示されるティッカーシンボルでフィルタリングすることにより収集されます。例えば、\n",
    "\n",
    "{'message_body': 'RT @google Our annual look at the year in Google blogging (and beyond) http://t.co/sptHOAh8 $GOOG',\n",
    " 'sentiment': 0}\n",
    "\n",
    "ティッカーシンボルはセンチメントに関する情報を提供せず、すべてのツイットに含まれているため、削除する必要があります。このtwitには@googleユーザー名もあり、ここでもセンチメント情報は提供されないため、削除する必要があります。URLも表示されますhttp://t.co/sptHOAh8。これらも削除しましょう。\n",
    "\n",
    "特定の単語やフレーズを削除する最も簡単な方法は、reモジュールを使用して正規表現を使用することです。スペースを使用して特定のパターンをサブアウトできます。\n",
    "\n",
    "re.sub(pattern, ' ', text)\n",
    "これにより、テキスト内のパターンが一致する場所でスペースが置換されます。後でテキストをトークン化するときに、それらのスペースで適切に分割します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Message Body and Sentiment Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [twit['message_body'] for twit in twits['data']]\n",
    "# Since the sentiment scores are discrete, we'll scale the sentiments to 0 to 4 for use in our network\n",
    "sentiments = [twit['sentiment'] + 2 for twit in twits['data']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/cdsw/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "\n",
    "def preprocess(message):\n",
    "    \"\"\"\n",
    "    入力として文字列を受け取り、次の操作を実行する: \n",
    "        - 全てのアルファベットを小文字に変換\n",
    "        - URLを削除\n",
    "        - ティッカーシンボルを削除 \n",
    "        - 句読点を削除\n",
    "        - 文字列をスペースで分割しトークン化する\n",
    "        - シングル・キャラクターのトークンを削除\n",
    "    \n",
    "    パラメータ\n",
    "    ----------\n",
    "        message : 前処理の対象テキストメッセージ\n",
    "        \n",
    "    戻り値\n",
    "    -------\n",
    "        tokens: 前処理後のトークン配列\n",
    "    \"\"\" \n",
    "    #TODO: Implement \n",
    "    \n",
    "    # Lowercase the twit message\n",
    "    text = message.lower()\n",
    "    \n",
    "    # Replace URLs with a space in the message\n",
    "    text = re.sub(\"http(s)?://([\\w\\-]+\\.)+[\\w-]+(/[\\w\\- ./?%&=]*)?\",' ', text)\n",
    "    \n",
    "    # Replace ticker symbols with a space. The ticker symbols are any stock symbol that starts with $.\n",
    "    text = re.sub(\"\\$[^ \\t\\n\\r\\f]+\", ' ', text)\n",
    "    \n",
    "    # Replace StockTwits usernames with a space. The usernames are any word that starts with @.\n",
    "    text = re.sub(\"@[^ \\t\\n\\r\\f]+\", ' ', text)\n",
    "\n",
    "    # Replace everything not a letter with a space\n",
    "    text = re.sub(\"[^a-z]\", ' ', text)\n",
    "    \n",
    "    \n",
    "    # Tokenize by splitting the string on whitespace into a list of words\n",
    "    tokens = text.split()\n",
    "\n",
    "    # Lemmatize words using the WordNetLemmatizer. You can ignore any word that is not longer than one character.\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    tokens = [wnl.lemmatize(w, pos='v') for w in tokens if len(w) > 1]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitsメッセージ前処理\n",
    "Now we can preprocess each of the twits in our dataset. Apply the function `preprocess` to all the twit messages.\n",
    "\n",
    "※この処理には、データのサイズに応じて多少時間がかかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['heng', 'be', 'about', 'to', 'go', 'negative', 'get', 'ready', 'for', 'tomorrow'], ['if', 'you', 'watch', 'only', 'one', 'documentary', 'it', 'should', 'be', 'this', 'one'], ['if', 'you', 'watch', 'only', 'one', 'documentary', 'it', 'should', 'be', 'this', 'one']]\n",
      "590901\n"
     ]
    }
   ],
   "source": [
    "tokenized = list(map(preprocess, messages))\n",
    "\n",
    "print(tokenized[:3])\n",
    "print(len(tokenized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words\n",
    "\n",
    "すべてのメッセージがトークン化されたので、語彙を作成し、コーパス全体で各単語が出現する頻度をカウントします。Counter関数を使用して、すべてのトークンをカウントアップします。\n",
    "[`Counter`](https://docs.python.org/3.1/library/collections.html#collections.Counter)\n",
    "\n",
    "※この処理には、データのサイズに応じて多少時間がかかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['heng', 'be', 'about', 'to', 'go', 'negative', 'get', 'ready', 'for', 'tomorrow', 'if', 'you', 'watch']\n",
      "7680210\n",
      "590901\n",
      "[[5949, 3, 83, 2, 37, 702, 51, 674, 9, 151], [100, 56, 208, 204, 106, 3350, 30, 216, 3, 17, 106], [100, 56, 208, 204, 106, 3350, 30, 216, 3, 17, 106]]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "#words = []\n",
    "#for tokens in tokenized:\n",
    "#    for token in tokens:\n",
    "#        words.append(token)\n",
    "out_list = tokenized\n",
    "words = [element for in_list in out_list for element in in_list]\n",
    "\n",
    "print(words[:13])\n",
    "print(len(words))\n",
    "\n",
    "\"\"\"\n",
    "Create a vocabulary by using Bag of words\n",
    "\"\"\"\n",
    "\n",
    "# TODO: Implement \n",
    "\n",
    "word_counts = Counter(words)\n",
    "sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "int_to_vocab = {ii: word for ii, word in enumerate(sorted_vocab)}\n",
    "vocab_to_int = {word:ii for ii, word in int_to_vocab.items()}\n",
    "\n",
    "bow = []\n",
    "for tokens in tokenized:\n",
    "    bow.append([vocab_to_int[token] for token in tokens])\n",
    "\n",
    "print(len(bow))\n",
    "print(bow[:3])\n",
    "\n",
    "# This BOW will not be used because it is not filtered to eliminate common words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### メッセージに現れる単語の頻度\n",
    "\n",
    "ボキャブラリーを使用して、「the」、「and」、「it」などの最も一般的な単語の一部を削除します。\n",
    "これらの単語は感情を特定するのに寄与せず、非常に一般的であるため、ニューラルネットワークの入力のノイズとなります。これらを除外することで、ネットワークの学習時間を短縮することができます。\n",
    "\n",
    "また、ほんの数回しか使われていない、非常にまれな単語も削除します。ここでは、各単語のカウントをメッセージの数で除算する必要があります。次に、メッセージのごく一部にしか表示されない単語を削除します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(sorted_vocab): 5950\n",
      "sorted_vocab - top: ['the', 'of', 'to']\n",
      "sorted_vocab - least: ['desktop', 'verge', 'catalysts', 'nyseamerican', 'driver', 'astronomically', 'edit', 'theme', 'ignore', 'particular', 'explode', 'autoscalp', 'yaawn', 'salamis', 'heng']\n",
      "freqs[the]: 0.027121654225600603\n",
      "high_cutoff: 20\n",
      "low_cutoff: 2e-06\n",
      "K_most_common: ['the', 'of', 'to', 'be', 'amp', 'utm', 'and', 'on', 'in', 'for', 'file', 'form', 'share', 'stock', 'by', 'sec', 'report', 'this', 'earn', 'at']\n",
      "len(filtered_words): 5929\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Set the following variables:\n",
    "    freqs\n",
    "    low_cutoff\n",
    "    high_cutoff\n",
    "    K_most_common\n",
    "\"\"\"\n",
    "\n",
    "# TODO Implement \n",
    "\n",
    "print(\"len(sorted_vocab):\",len(sorted_vocab))\n",
    "print(\"sorted_vocab - top:\", sorted_vocab[:3])\n",
    "print(\"sorted_vocab - least:\", sorted_vocab[-15:])\n",
    "\n",
    "# Dictionart that contains the Frequency of words appearing in messages.\n",
    "# The key is the token and the value is the frequency of that word in the corpus.\n",
    "total_count = len(words)\n",
    "freqs = {word: count/total_count for word, count in word_counts.items()}\n",
    "\n",
    "#print(\"freqs[supplication]:\",freqs[\"supplication\"] )\n",
    "print(\"freqs[the]:\",freqs[\"the\"] )\n",
    "\n",
    "\"\"\"\n",
    "This was the post by Ricardo:\n",
    "\n",
    "there's no exact value for low_cutoff and high_cutoff, \n",
    "however I'd recommend you to use \n",
    "a low_cutoff that's around 0.000002 and 0.000007 \n",
    "(This depends on the values you get from your freqs calculations) and \n",
    "a high_cutofffrom 5 to 20 (this depends on the most_common values from the bow).\n",
    "\"\"\"\n",
    "\n",
    "# Float that is the frequency cutoff. Drop words with a frequency that is lower or equal to this number.\n",
    "low_cutoff = 0.000002\n",
    "\n",
    "# Integer that is the cut off for most common words. Drop words that are the `high_cutoff` most common words.\n",
    "\"\"\"\n",
    "example_count = []\n",
    "example_count.append(sorted_vocab.index(\"the\"))\n",
    "example_count.append(sorted_vocab.index(\"for\"))\n",
    "example_count.append(sorted_vocab.index(\"of\"))\n",
    "print(example_count)\n",
    "high_cutoff = min(example_count)\n",
    "\"\"\"\n",
    "high_cutoff = 20\n",
    "print(\"high_cutoff:\",high_cutoff)\n",
    "print(\"low_cutoff:\",low_cutoff)\n",
    "\n",
    "# The k most common words in the corpus. Use `high_cutoff` as the k.\n",
    "#K_most_common = [word for word in sorted_vocab[:high_cutoff]]\n",
    "K_most_common = sorted_vocab[:high_cutoff]\n",
    "\n",
    "print(\"K_most_common:\",K_most_common)\n",
    "\n",
    "\n",
    "##  END of TODO Implement\n",
    "\n",
    "filtered_words = [word for word in freqs if (freqs[word] > low_cutoff and word not in K_most_common)]\n",
    "\n",
    "print(\"len(filtered_words):\",len(filtered_words)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### フィルターされた単語を削除して語彙を更新する¶\n",
    "ボキャブラリーに役立つ3つの変数を作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(tokenized): 590901\n",
      "len(filtered): 590901\n",
      "tokenized[:1] [['heng', 'be', 'about', 'to', 'go', 'negative', 'get', 'ready', 'for', 'tomorrow']]\n",
      "filtered[:1] [['about', 'go', 'negative', 'get', 'ready', 'tomorrow']]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Set the following variables:\n",
    "    vocab\n",
    "    id2vocab\n",
    "    filtered\n",
    "\"\"\"\n",
    "\n",
    "#TODO Implement\n",
    "\n",
    "# A dictionary for the `filtered_words`. The key is the word and value is an id that represents the word. \n",
    "vocab =  {word:ii for ii, word in enumerate(filtered_words)}\n",
    "# Reverse of the `vocab` dictionary. The key is word id and value is the word. \n",
    "id2vocab = {ii:word for word, ii in vocab.items()}\n",
    "# tokenized with the words not in `filtered_words` removed.\n",
    "\n",
    "print(\"len(tokenized):\", len(tokenized))\n",
    "\n",
    "filtered = [[token for token in tokens if token in vocab] for tokens in tokenized]\n",
    "print(\"len(filtered):\", len(filtered))\n",
    "print(\"tokenized[:1]\", tokenized[:1])\n",
    "print(\"filtered[:1]\",filtered[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### クラスのバランス\n",
    "最後の前処理ステップをいくつか行いましょう。twitのラベル付けを見ると、twitの50％がニュートラルであることがわかります。これは、毎回0を推測するだけで、ネットワークの精度が50％になることを意味します。ネットワークが適切に学習できるように、クラスのバランスを取る必要があります。つまり、それぞれのセンチメントスコアがデータにほぼ同じ頻度で表示されることを確認します。\n",
    "\n",
    "ここでできることは、それぞれの例に目を通し、中立的な感情を持つtwitsをランダムにドロップすることです。50％のニュートラルから20％のニュートラルtwitを取得したい場合、これらのtwitをドロップする確率はどうなりますか？この機会に、長さ0のメッセージを削除する必要もあります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced = {'messages': [], 'sentiments':[]}\n",
    "\n",
    "n_neutral = sum(1 for each in sentiments if each == 2)\n",
    "N_examples = len(sentiments)\n",
    "keep_prob = (N_examples - n_neutral)/4/n_neutral\n",
    "\n",
    "for idx, sentiment in enumerate(sentiments):\n",
    "    message = filtered[idx]\n",
    "    if len(message) == 0:\n",
    "        # skip this message because it has length zero\n",
    "        continue\n",
    "    elif sentiment != 2 or random.random() < keep_prob:\n",
    "        balanced['messages'].append(message)\n",
    "        balanced['sentiments'].append(sentiment) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you did it correctly, you should see the following result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2127790374604797"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_neutral = sum(1 for each in balanced['sentiments'] if each == 2)\n",
    "N_examples = len(balanced['sentiments'])\n",
    "n_neutral/N_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's convert our tokens into integer ids which we can pass to the network.\n",
    "\n",
    "メッセージをID（数値）に変換します。この処理は、ニューラルネットワークの入力として用いるために必要です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = [[vocab[word] for word in message] for message in balanced['messages']]\n",
    "sentiments = balanced['sentiments']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ボキャブラリ・ファイルを保存します。このファイルは、予測の際に、入力を変換するために必要になります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('vocab.pickle', 'wb') as f:\n",
    "    pickle.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ニューラルネットワーク\n",
    "これでボキャブラリーができたので、トークンをIDに変換し、それをネットワークに渡すことができます。ネットワークを定義します\n",
    "\n",
    "下記は、ネットワークの概要です：\n",
    "\n",
    "#### Embed -> RNN -> Dense -> Softmax\n",
    "### Text classifier (テキスト分類器)実装\n",
    "テキスト分類器を作成する前に、「RNNを使用したセンチメント分析」演習で作成した他のネットワーク（ここでは「SentimentRNN」と呼ばれるネットワーク、ここでは「TextClassifer」と呼びます）を覚えている場合、3つの主要な部分で構成されています：: 1) init function `__init__` 2) forward pass `forward`  3) hidden state `init_hidden`. \n",
    "\n",
    "このネットワークは、forwardパスで期待して構築したネットワークに非常に似ています 。シグモイドの代わりにsoftmaxを使用します。シグモイドを使用しないのは、NNの出力がバイナリではないためです。このネットワークでは、センチメントスコアには5つの結果があります。最も高い確率の結果を探しているため、softmaxの方が適しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, lstm_size, output_size, lstm_layers=1, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "            vocab_size : The vocabulary size.\n",
    "            embed_size : The embedding layer size.\n",
    "            lstm_size : The LSTM layer size.\n",
    "            output_size : The output size.\n",
    "            lstm_layers : The number of LSTM layers.\n",
    "            dropout : The dropout probability.\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.lstm_size = lstm_size\n",
    "        self.output_size = output_size\n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # TODO Implement\n",
    "\n",
    "        # Setup embedding layer\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embed_size)\n",
    "        \n",
    "        # Setup additional layers\n",
    "        self.lstm = nn.LSTM(self.embed_size, self.lstm_size, self.lstm_layers, dropout=self.dropout)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(lstm_size, output_size)\n",
    "        \n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\" \n",
    "        Initializes hidden state\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "            batch_size : The size of batches.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "            hidden_state\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO Implement \n",
    "        \n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        \n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        hidden = (weight.new(self.lstm_layers, batch_size,self.lstm_size).zero_(),\n",
    "                         weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_())\n",
    "        return hidden\n",
    "\n",
    "\n",
    "    def forward(self, nn_input, hidden_state):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on nn_input.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "            nn_input : The batch of input to the NN.\n",
    "            hidden_state : The LSTM hidden state.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            logps: log softmax output\n",
    "            hidden_state: The new hidden state.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO Implement \n",
    "        batch_size = nn_input.size(0)\n",
    "        \n",
    "        embeds = self.embedding(nn_input)\n",
    "        lstm_out, hidden_state = self.lstm(embeds, hidden_state)\n",
    "        \n",
    "        #lstm_out = lstm_out.contiguous().view(-1, self.lstm_size)    \n",
    "        \"\"\"\n",
    "        remember here you do not have batch_first=True, \n",
    "        so accordingly shape your input. \n",
    "        Moreover, since now input is seq_length x batch you just need to transform lstm_out = lstm_out[-1,:,:].\n",
    "        you don't have to use batch_first=True in this case, \n",
    "        nor reshape the outputs with .view just transform your lstm_out as advised and you should be good to go.\n",
    "        \"\"\"\n",
    "        lstm_out = lstm_out[-1,:,:]\n",
    "        \n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        logps = self.softmax(out)\n",
    "        \n",
    "        \n",
    "        return logps, hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルの確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.3052, -1.6370, -1.7643, -1.7601, -1.6555],\n",
      "        [-1.3033, -1.6333, -1.7503, -1.7675, -1.6681],\n",
      "        [-1.2493, -1.6195, -1.7996, -1.7675, -1.7195],\n",
      "        [-1.3039, -1.6398, -1.7491, -1.7688, -1.6605]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "model = SentimentClassifier(len(vocab), 10, 6, 5, dropout=0.1, lstm_layers=2)\n",
    "model.embedding.weight.data.uniform_(-1, 1)\n",
    "input = torch.randint(0, 1000, (5, 4), dtype=torch.int64)\n",
    "hidden = model.init_hidden(4)\n",
    "\n",
    "logps, _ = model.forward(input, hidden)\n",
    "print(logps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### トレーニング\n",
    "### DataLoaderとバッチ処理\n",
    "ここで、データをループするために使用できるジェネレーターを構築する必要があります。シーケンスをバッチとして渡すことができれば、より効率的です。入力テンソルは次のようになり(sequence_length, batch_size)ます。したがって、シーケンスが40トークンで、25シーケンスを渡す場合、入力サイズはになり(40, 25)ます。\n",
    "\n",
    "シーケンスの長さを40に設定した場合、40トークンより多いまたは少ないメッセージをどう処理しますか？40トークン未満のメッセージの場合、空のスポットにゼロを埋め込みます。データを処理する前にRNNが何も開始しないように、必ずパッドを残しておく必要があります。メッセージに20個のトークンがある場合、40個の長いシーケンスの最初の20個のスポットは0になります。メッセージに40個を超えるトークンがある場合、最初の40個のトークンを保持します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def dataloader(messages, labels, sequence_length=30, batch_size=32, shuffle=False):\n",
    "def dataloader(messages, labels, sequence_length=20, batch_size=32, shuffle=False):\n",
    "    \"\"\" \n",
    "    Build a dataloader.\n",
    "    \"\"\"\n",
    "    if shuffle:\n",
    "        indices = list(range(len(messages)))\n",
    "        random.shuffle(indices)\n",
    "        messages = [messages[idx] for idx in indices]\n",
    "        labels = [labels[idx] for idx in indices]\n",
    "\n",
    "    total_sequences = len(messages)\n",
    "\n",
    "    for ii in range(0, total_sequences, batch_size):\n",
    "        batch_messages = messages[ii: ii+batch_size]\n",
    "        \n",
    "        # First initialize a tensor of all zeros\n",
    "        batch = torch.zeros((sequence_length, len(batch_messages)), dtype=torch.int64)\n",
    "        for batch_num, tokens in enumerate(batch_messages):\n",
    "            token_tensor = torch.tensor(tokens)\n",
    "            # Left pad!\n",
    "            start_idx = max(sequence_length - len(token_tensor), 0)\n",
    "            batch[start_idx:, batch_num] = token_tensor[:sequence_length]\n",
    "        \n",
    "        label_tensor = torch.tensor(labels[ii: ii+len(batch_messages)])\n",
    "        \n",
    "        yield batch, label_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and  Validation\n",
    "With our data in nice shape, we'll split it into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Split data into training and validation datasets. Use an appropriate split size.\n",
    "The features are the `token_ids` and the labels are the `sentiments`.\n",
    "\"\"\"   \n",
    "\n",
    "# TODO Implement \n",
    "\n",
    "split_frac = 0.98 # for small data\n",
    "#split_frac = 0.8 # for big data\n",
    "\n",
    "## split data into training, validation, and test data (features and labels, x and y)\n",
    "\n",
    "split_idx = int(len(token_ids)*split_frac)\n",
    "train_features, remaining_features = token_ids[:split_idx], token_ids[split_idx:]\n",
    "train_labels, remaining_labels = sentiments[:split_idx], sentiments[split_idx:]\n",
    "\n",
    "test_idx = int(len(remaining_features)*0.5)\n",
    "valid_features, test_features = remaining_features[:test_idx], remaining_features[test_idx:]\n",
    "valid_labels, test_labels = remaining_labels[:test_idx], remaining_labels[test_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_batch, labels = next(iter(dataloader(train_features, train_labels, sequence_length=20, batch_size=64)))\n",
    "model = SentimentClassifier(len(vocab)+1, 200, 128, 5, dropout=0.)\n",
    "hidden = model.init_hidden(64)\n",
    "logps, hidden = model.forward(text_batch, hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "It's time to train the neural network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentClassifier(\n",
       "  (embedding): Embedding(5930, 1024)\n",
       "  (lstm): LSTM(1024, 512, num_layers=2, dropout=0.2)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (fc): Linear(in_features=512, out_features=5, bias=True)\n",
       "  (softmax): LogSoftmax()\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = SentimentClassifier(len(vocab)+1, 1024, 512, 5, lstm_layers=2, dropout=0.2)\n",
    "model.embedding.weight.data.uniform_(-1, 1)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### トレーニング実施\n",
    "\n",
    "※この処理には、データのサイズに応じて、十分な時間が必要です。\n",
    "\n",
    "GPUを備えた環境で実行する場合、ターミナルで以下のコマンドを実行することで、GPUが利用されていることを確認することができます（ GPU実行中、コマンド実行により表示されるテーブルの右上のVolatile GPU-Utilのパーセンテージ値が増えます）\n",
    "```\n",
    "$ watch nvidia-smi\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-11cdf2ff3eab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0meach\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cdsw/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-8a871cb61b09>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, nn_input, hidden_state)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0membeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mlstm_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;31m#lstm_out = lstm_out.contiguous().view(-1, self.lstm_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cdsw/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cdsw/.local/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 559\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    560\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Train your model with dropout. Make sure to clip your gradients.\n",
    "Print the training loss, validation loss, and validation accuracy for every 100 steps.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "epochs = 4 #pass\n",
    "batch_size =  64#pass\n",
    "batch_size =  512#pass\n",
    "learning_rate = 0.001 #pass\n",
    "\n",
    "print_every = 100\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "model.train()\n",
    "\n",
    "val_losses = []\n",
    "accuracy = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Starting epoch {}'.format(epoch + 1))\n",
    "    \n",
    "    steps = 0\n",
    "    for text_batch, labels in dataloader(\n",
    "            train_features, train_labels, batch_size=batch_size, sequence_length=20, shuffle=True):\n",
    "        steps += 1\n",
    "        hidden = model.init_hidden(labels.shape[0]) #pass\n",
    "        \n",
    "        # Set Device\n",
    "        text_batch, labels = text_batch.to(device), labels.to(device)\n",
    "        for each in hidden:\n",
    "            each.to(device)\n",
    "        \n",
    "        # TODO Implement: Train Model\n",
    "        hidden = tuple([each.data for each in hidden])\n",
    "        model.zero_grad()\n",
    "        output, hidden = model(text_batch, hidden)\n",
    "        loss = criterion(output.squeeze(), labels)\n",
    "        loss.backward()\n",
    "        clip = 5\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate loss\n",
    "        val_losses.append(loss.item())\n",
    "        \n",
    "        correct_count = 0.0\n",
    "        if steps % print_every == 0:\n",
    "            model.eval()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            ps = torch.exp(output)\n",
    "            top_p, top_class = ps.topk(1, dim=1)\n",
    "            #?top_class = top_class.to(device)\n",
    "            #?labels = labels.to(device)\n",
    "\n",
    "            correct_count += torch.sum(top_class.squeeze()== labels)\n",
    "            accuracy.append(100*correct_count/len(labels))\n",
    "            \n",
    "            # TODO Implement: Print metrics\n",
    "            print(\"Epoch: {}/{}...\".format(epoch+1, epochs),\n",
    "                 \"Step: {}...\".format(steps),\n",
    "                 \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                 \"Val Loss: {:.6f}\".format(np.mean(val_losses)),\n",
    "                 #\"Collect Count: {}\".format(correct_count),\n",
    "                 #\"Accuracy: {:.2f}\".format((100*correct_count/len(labels))),\n",
    "                 # AttributeError: 'torch.dtype' object has no attribute 'type'\n",
    "                 #\"Accuracy Avg: {:.2f}\".format(np.mean(accuracy))\n",
    "                 )\n",
    "            \n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'state_dict': model.state_dict()}, 'checkpoint.pth.tar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 予測（Prediction）関数の作成\n",
    "\n",
    "訓練されたモデルを使って、テキスト入力から予測結果ベクトル（）を生成するpredict関数を実装します。\n",
    "\n",
    "入力されたテキストは、ネットワークに渡される前に前処理される必要があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/cdsw/misc', '/home/cdsw/requirements.txt', '/home/cdsw/sentiment', '/home/cdsw/cdsw-build.sh', '/home/cdsw/checkpoint.pth.tar', '/home/cdsw/nlp_handson.ipynb', '/home/cdsw/json_get.py', '/home/cdsw/model_api.py', '/home/cdsw/output1.json', '/home/cdsw/train_model.py', '/home/cdsw/reqnew.txt', '/home/cdsw/data', '/home/cdsw/lib', '/home/cdsw/nlp_solution.ipynb', '/home/cdsw/README.md', '/home/cdsw/nltk_data', '/home/cdsw/ticker.txt', '/home/cdsw/vocab.pickle', '/home/cdsw/output.json']\n",
      "/home/cdsw\n",
      "[ 0.01212226  0.00776675  0.91733634  0.0069572   0.05581738]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/cdsw/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "print(glob.glob(\"/home/cdsw/*\"))\n",
    "\n",
    "import pickle\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import torch\n",
    "\n",
    "import os\n",
    "import sys\n",
    "cur_dir = os.path.dirname(os.path.abspath('__file__'))\n",
    "print(cur_dir)\n",
    "sys.path.append(cur_dir)\n",
    "\n",
    "vocab_filename = 'vocab.pickle'\n",
    "vocab_path = cur_dir + \"/\" + vocab_filename\n",
    "vocab_l = pickle.load(open(vocab_path, 'rb'))\n",
    "\n",
    "#model_path = cur_dir + \"/\" + \"model.torch\"\n",
    "#model_l = torch.load(model_path, map_location='cpu')\n",
    "\n",
    "model_l = TextClassifier(len(vocab_l)+1, 1024, 512, 5, lstm_layers=2, dropout=0.2)\n",
    "checkpoint = torch.load('./checkpoint.pth.tar')\n",
    "model_l.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "class UnknownWordsError(Exception):\n",
    "  \"Only unknown words are included in text\"\n",
    "\n",
    "\n",
    "def preprocess(message):\n",
    "    \"\"\"\n",
    "    This function takes a string as input, then performs these operations: \n",
    "        - lowercase\n",
    "        - remove URLs\n",
    "        - remove ticker symbols \n",
    "        - removes punctuation\n",
    "        - tokenize by splitting the string on whitespace \n",
    "        - removes any single character tokens\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        message : The text message to be preprocessed.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "        tokens: The preprocessed text into tokens.\n",
    "    \"\"\" \n",
    "    \n",
    "    # Lowercase the twit message\n",
    "    text = message.lower()\n",
    "    \n",
    "    # Replace URLs with a space in the message\n",
    "    text = re.sub(\"http(s)?://([\\w\\-]+\\.)+[\\w-]+(/[\\w\\- ./?%&=]*)?\",' ', text)\n",
    "    \n",
    "    # Replace ticker symbols with a space. The ticker symbols are any stock symbol that starts with $.\n",
    "    text = re.sub(\"\\$[^ \\t\\n\\r\\f]+\", ' ', text)\n",
    "    \n",
    "    # Replace StockTwits usernames with a space. The usernames are any word that starts with @.\n",
    "    text = re.sub(\"@[^ \\t\\n\\r\\f]+\", ' ', text)\n",
    "\n",
    "    # Replace everything not a letter with a space\n",
    "    text = re.sub(\"[^a-z]\", ' ', text)\n",
    "    \n",
    "    \n",
    "    # Tokenize by splitting the string on whitespace into a list of words\n",
    "    tokens = text.split()\n",
    "\n",
    "    # Lemmatize words using the WordNetLemmatizer. You can ignore any word that is not longer than one character.\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    tokens = [wnl.lemmatize(w, pos='v') for w in tokens if len(w) > 1]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "\n",
    "def predict_func(text, model, vocab):\n",
    "    \"\"\" \n",
    "    Make a prediction on a single sentence.\n",
    "    Parameters\n",
    "    ----------\n",
    "        text : The string to make a prediction on.\n",
    "        model : The model to use for making the prediction.\n",
    "        vocab : Dictionary for word to word ids. The key is the word and the value is the word id.\n",
    "    Returns\n",
    "    -------\n",
    "        pred : Prediction vector\n",
    "    \"\"\"\n",
    "\n",
    "    tokens = preprocess(text)    \n",
    "\n",
    "    # Filter non-vocab words\n",
    "    tokens = [token for token in tokens if token in vocab] #pass\n",
    "    # Convert words to ids\n",
    "    tokens = [vocab[token] for token in tokens] #pass\n",
    "\n",
    "    if len(tokens) == 0:\n",
    "      raise UnknownWordsError\n",
    "\n",
    "    # Adding a batch dimension\n",
    "    text_input = torch.from_numpy(np.asarray(torch.LongTensor(tokens).view(-1, 1)))\n",
    "\n",
    "    # Get the NN output       \n",
    "    batch_size = 1\n",
    "    hidden = model.init_hidden(batch_size) #pass\n",
    "    \n",
    "    logps, _ = model(text_input, hidden) #pass\n",
    "    # Take the exponent of the NN output to get a range of 0 to 1 for each label.\n",
    "    pred = torch.round(logps.squeeze())#pass\n",
    "    pred = torch.exp(logps) \n",
    "    \n",
    "    return pred\n",
    "\n",
    "\n",
    "def predict_api(args):\n",
    "    text = args.get('text')\n",
    "    try:\n",
    "        result = predict_func(text, model_l, vocab_l)\n",
    "        return result.detach().numpy()[0]\n",
    "    except UnknownWordsError:\n",
    "        return [0,0,1,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.39283948e-04   1.18818964e-04   9.96657729e-01   1.55341288e-04\n",
      "   2.92887469e-03]\n"
     ]
    }
   ],
   "source": [
    "args = {\"text\": \"I'll strongly recommend buying $goog\"}\n",
    "\n",
    "#args = {\"text\": \"elyoq baoq pquq $goog\"}\n",
    "result = predict_api(args)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.1813184   0.29552919  0.12654278  0.31264037  0.0839693 ]\n"
     ]
    }
   ],
   "source": [
    "args = {\"text\": \"I'm bearish on $goog\"}\n",
    "result = predict_api(args)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "args = {\"text\": \"kono yoshiyuki\"}\n",
    "result = predict_api(args)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データベースを削除する場合は、**データベース名を適切に変更した後で**下記を実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql DROP DATABASE IF EXISTS user3 CASCADE;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
